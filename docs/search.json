[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesse Onland",
    "section": "",
    "text": "I’m a data scientist and business intelligence analyst living in Kitchener, Ontario, Canada.\nI’m interested in inference broadly construed. That includes symbolic logic, mathematical proof, statistical inference, machine learning, and other topics related to knowing what we ought to conclude from the facts available to us.\nMy motto is, “If mathematics were cheesecake, I wouldn’t be a piano tuner.” Ask me about it some time!"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Data Science Links",
    "section": "",
    "text": "Why Business Data Science Irritates Me\nOn Moving from Statistics to Machine Learning, the Final Stage of Grief\nMost Data Work Seems Fundamentally Worthless\nData Science Is Different Now\nData Scientist: The Sexiest Job of the 21st Century\nIs Data Scientist Still the Sexiest Job of the 21st Century?\nYour Organization Probably Doesn’t Want To Improve Things"
  },
  {
    "objectID": "links.html#careers-industry",
    "href": "links.html#careers-industry",
    "title": "Data Science Links",
    "section": "",
    "text": "Why Business Data Science Irritates Me\nOn Moving from Statistics to Machine Learning, the Final Stage of Grief\nMost Data Work Seems Fundamentally Worthless\nData Science Is Different Now\nData Scientist: The Sexiest Job of the 21st Century\nIs Data Scientist Still the Sexiest Job of the 21st Century?\nYour Organization Probably Doesn’t Want To Improve Things"
  },
  {
    "objectID": "links.html#methodology-practice",
    "href": "links.html#methodology-practice",
    "title": "Data Science Links",
    "section": "Methodology & Practice",
    "text": "Methodology & Practice\n\nCommon Statistical Tests Are Linear Models (or: How To Teach Stats)\nNever Test For Normality\nStats Can’t Make Modeling Decisions\nThe Shortcomings of Standardized Regression Coefficients\nDirected Acyclic Graphs\nSimulating Confounders, Colliders and Mediators\nGo Get the Data\nThree Advantages of Non-AI Models\nLog Transforms, Geometric Means and Estimating Population Totals\nStatistician’s Time Series Hack\nThere Are No Magic Outcome Variables\nDo We Have To Tune the Number of Trees in a Random Forest?\nSolving for the Hidden Data\nFirst Time Seeing a Rare Event\nTell Me What You Really Want: How to Identify the Real Business Question\nThe Misunderstood Kelly Criterion\nWhat Size Is That Correlation?\nBayes Rule in Odds Form\nSensitivity Counts Against You\nA/B Tests for Engineers\nThe Medical Test Paradox, and Redesigning Bayes’ Rule\nUsing Dimensional Analysis To Check Probability Calculations"
  },
  {
    "objectID": "links.html#philosophy",
    "href": "links.html#philosophy",
    "title": "Data Science Links",
    "section": "Philosophy",
    "text": "Philosophy\n\nReview of Probability Theory: The Logic of Science\nProbability That a Number Is Prime\nPhilosophy of Probability\nPhilosophy of Statistics\nWhat Is Probability?\nCan You Have Confidence in a Confidence Interval?\nPrior Probabilities\nThe Well-Posed Problem\nMonkeys, Kangaroos, and N\nLevels of Uncertainty"
  },
  {
    "objectID": "links.html#puzzles-paradoxes",
    "href": "links.html#puzzles-paradoxes",
    "title": "Data Science Links",
    "section": "Puzzles & Paradoxes",
    "text": "Puzzles & Paradoxes\n\nThe Weirdest Paradox in Statistics (and Machine Learning)\nSimpson’s Paradox All the Way Down\nBertrand’s Paradox\nThe Exchange Paradox\nMore On Bertrand’s Paradox"
  },
  {
    "objectID": "links.html#references-resources",
    "href": "links.html#references-resources",
    "title": "Data Science Links",
    "section": "References & Resources",
    "text": "References & Resources\n\nThe Good Research Code Handbook\nAwesome Public Datasets\nModern Data Science with R\nStatistical Rethinking with brms, ggplot2, and the tidyverse\nData Creators Club\nRegression Modeling Strategies\nBayes Rules! An Introduction to Applied Bayesian Modeling\nPast, Present, and Future of Software for Bayesian Inference\nCausal Inference: The Mixtape\nHappy Git and GitHub for the useR\nIntroduction to Modern Statistics (2nd Ed)"
  },
  {
    "objectID": "links.html#visualization",
    "href": "links.html#visualization",
    "title": "Data Science Links",
    "section": "Visualization",
    "text": "Visualization\n\n1 Dataset 100 Visualizations\nHow People Actually Lie With Charts\nFrom Data to Viz\nDataviz Inspiration\nReview of The Visual Display of Quantitative Information\nWhat Ordinary People Need Most from Information Visualization Today\nMarket Cafe Magazine"
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html",
    "title": "Books No One Else Has",
    "section": "",
    "text": "LibraryThing is a site for storing and sharing book catalogues. It currently has about 3 million users and 183 million books catalogued.\nThese are the books on my shelves which appear in no other user’s catalogue."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#赤峰幸生の暮しっく-by-eisuke-yamashita",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#赤峰幸生の暮しっく-by-eisuke-yamashita",
    "title": "Books No One Else Has",
    "section": "赤峰幸生の暮しっく by Eisuke Yamashita",
    "text": "赤峰幸生の暮しっく by Eisuke Yamashita\nThis is a coffee table book of photos of the world’s most stylish man, Yukio Akamine. Derek Guy interviewed the author for Put This On here.\nThis is a boutique, foreign-language publication on a niche subject, which I expect explains why it doesn’t appear in anyone else’s catalogue."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#dicionario-de-la-10.000-radiki-di-la-linguo-universala-ido-by-marcel-peach",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#dicionario-de-la-10.000-radiki-di-la-linguo-universala-ido-by-marcel-peach",
    "title": "Books No One Else Has",
    "section": "Dicionario de la 10.000 Radiki di la Linguo Universala Ido by Marcel Peach",
    "text": "Dicionario de la 10.000 Radiki di la Linguo Universala Ido by Marcel Peach\nA self-published dictionary of the would-be successor to Esperanto. It’s somewhat dated (e.g. “di la” is usually “dil” today) and Ido is only spoken by a very small community of enthusiasts. I’m surprised none of the collections of any of the various Esperanto organizations, some of which are very extensive, contain it."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#on-the-choice-of-a-common-language-by-h.-jacob",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#on-the-choice-of-a-common-language-by-h.-jacob",
    "title": "Books No One Else Has",
    "section": "On the Choice of a Common Language by H. Jacob",
    "text": "On the Choice of a Common Language by H. Jacob\nAnother artifact from the era of peak public interest in auxiliary languages. The author followed up this essay the next year with A Planned Auxiliary Language, which was briefly reviewed in Nature."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#logical-methods-by-greg-restall-and-shawn-standefer",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#logical-methods-by-greg-restall-and-shawn-standefer",
    "title": "Books No One Else Has",
    "section": "Logical Methods by Greg Restall and Shawn Standefer",
    "text": "Logical Methods by Greg Restall and Shawn Standefer\nIn his review, Peter Smith calls this a “an introduction to some aspects of formal logic that are of particular philosophical interest”. Unusually, it covers propositional modal logic before developing first-order logic.\nThis is a newly-released book and I’m sure it will fall off this list once a few more university lecturers start using it in their courses."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#tidy-modeling-with-r-a-framework-for-modeling-in-the-tidyverse-by-max-kuhn-and-julia-silge",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#tidy-modeling-with-r-a-framework-for-modeling-in-the-tidyverse-by-max-kuhn-and-julia-silge",
    "title": "Books No One Else Has",
    "section": "Tidy Modeling with R: A Framework for Modeling in the Tidyverse by Max Kuhn and Julia Silge",
    "text": "Tidy Modeling with R: A Framework for Modeling in the Tidyverse by Max Kuhn and Julia Silge\nThis is an O’Reilly book about the tidymodels framework for statistical inference and machine learning in R. I’m a bit surprised that no other users have this yet. Maybe Python is just too popular."
  },
  {
    "objectID": "posts/books_no_one_else_has/books_no_one_else_has.html#coffee-table-books-from-iceland",
    "href": "posts/books_no_one_else_has/books_no_one_else_has.html#coffee-table-books-from-iceland",
    "title": "Books No One Else Has",
    "section": "Coffee Table Books from Iceland",
    "text": "Coffee Table Books from Iceland\n\nMyndlistardeild Listaháskóla Íslands: útskriftarverkefni 2022 by Bjarki Bragason\nThis is the book for the 2022 graduation exhibition of the Iceland University of the Arts. I’m not surprised no one else on the site has this.\n\n\nErró: The Power of Images\nAnother exhibition book from Iceland, this one from an Erró retrospective at the Reykjavík Art Museum.\n\n\nBirgir Andrésson: In Icelandic Colours by Robert Hobbs\nA retrospective of Birgir Andrésson’s artworks.\n\n\nSkáklist - 32 Pieces: The Art of Chess by Mark Sanders\nThe Art of Chess is a travelling exhibition of interesting chess sets. Although I wasn’t able to see it, the book from its stop in Iceland makes a nice addition to my collection of books about chess sets.\n\n\nStakkurinn - The Coat\nA pictorial history of (and fairly compelling advertisement for) the outerwear company 66°N.\n\n\nEf ég hefði verdið … : Reykjavík 1950-1970 by Nina Zurier\nA fictional autobiography in which the editor imagines what it would have been like to grow up in Reykjavík instead of the American Midwest by compiling material from the Reykjavík Museum of Photography.\n\n\nSaga Spilanna by Guðbrandur Magnússon\nA history of playing cards in Iceland."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html",
    "href": "posts/city_temperatures/city_temperatures.html",
    "title": "Cities with Nice Weather",
    "section": "",
    "text": "I live near Toronto. It’s springtime, and currently about 30 °C. In my opinion, Toronto is too hot in the summer and too cold in the winter. I’d like to know which cities have the least deviation from a tolerable average temperature."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#introduction",
    "href": "posts/city_temperatures/city_temperatures.html#introduction",
    "title": "Cities with Nice Weather",
    "section": "",
    "text": "I live near Toronto. It’s springtime, and currently about 30 °C. In my opinion, Toronto is too hot in the summer and too cold in the winter. I’d like to know which cities have the least deviation from a tolerable average temperature."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#tools",
    "href": "posts/city_temperatures/city_temperatures.html#tools",
    "title": "Cities with Nice Weather",
    "section": "Tools",
    "text": "Tools\nThis page was created using Quarto. I’m using the tidyverse for data wrangling, ggplot2 and ggExtra to plot."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#data",
    "href": "posts/city_temperatures/city_temperatures.html#data",
    "title": "Cities with Nice Weather",
    "section": "Data",
    "text": "Data\nFirst, I created a CSV file comprising all the information in the Wikpedia article List of cities by average temperature.\n\nread_csv(\"temps.csv\", show_col_types = FALSE) -&gt;\n  city_temps\n\nhead(city_temps)\n\n# A tibble: 6 × 15\n  Country City         Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct\n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Algeria Algiers     11.2  11.9  12.8  14.7  17.7  21.3  24.6  25.2  23.2  19.4\n2 Algeria Tamanrass…  12.8  15    18.1  22.2  26.1  28.9  28.7  28.2  26.5  22.4\n3 Algeria Reggane     16    18.2  23.1  27.9  32.2  36.4  39.8  38.4  35.5  29.2\n4 Angola  Luanda      26.7  28.5  28.6  28.2  27    23.9  22.1  22.1  23.5  25.2\n5 Benin   Cotonou     27.3  28.5  28.9  28.6  27.8  26.5  25.8  25.6  26    26.7\n6 Benin   Parakou     26.5  28.7  29.6  29    27.5  26.1  25.1  24.7  25    26.1\n# ℹ 3 more variables: Nov &lt;dbl&gt;, Dec &lt;dbl&gt;, Year &lt;dbl&gt;\n\n\nEach row corresponds to a distinct city. There are two text columns containing each city’s name and country, twelve numeric columns indicating the “averages of the daily highs and lows”1 for each month, and one additional numeric column containing the same figure for the entire year. The units are degrees Celsius. 455 cities are included in the data.1 This is a bit ambiguous, but no matter. The article also points out that “the actual daytime temperature in a given month will be 3 to 10 °C higher than the temperature listed here, depending on how large the difference between daily highs and lows is.”\nWe’ll define the “deviation” mentioned above as the difference between the value recorded for the coldest and hottest months, and the “average” as the value recorded for the whole year overall.\nWe’ll ignore any other weather characteristics like humidity, rain, wind, diurnal temperature difference, etc.22 This may or many not be reasonable depending on your personal preferences about weather.\n\ncity_temps |&gt;\n  rename_with(tolower) |&gt;\n  rowwise() |&gt;\n  transmute(city,\n            avg = year,\n            range = max(c_across(jan:dec)) - min(c_across(jan:dec))) -&gt;\n  city_temps\n\nhead(city_temps)\n\n# A tibble: 6 × 3\n# Rowwise: \n  city          avg range\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Algiers      17.4 14   \n2 Tamanrasset  21.7 16.1 \n3 Reggane      28.3 23.8 \n4 Luanda       25.8  6.5 \n5 Cotonou      27.2  3.30\n6 Parakou      26.8  4.9"
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#summary-statistics",
    "href": "posts/city_temperatures/city_temperatures.html#summary-statistics",
    "title": "Cities with Nice Weather",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nNow we can investigate the distribution of each of our two variables.\nHere are the default summaries:\n\ncity_temps |&gt; \n  select(avg, range) |&gt;\n  summarize()\n\n          N    Mean   SD     Min    Q1 Median    Q3  Max\n1   avg 455   18.00 8.12   -14.4 12.45   18.6 25.65 30.5\n2 range 455   13.75 9.53     0.7  5.65   12.1 21.00 58.1\n\n\nWhich cities correspond to the extremes for each variable?\n\ncity_temps |&gt;\n  filter(\n    avg   %in% (city_temps |&gt; pull(avg)   |&gt; range())  || \n    range %in% (city_temps |&gt; pull(range) |&gt; range())) |&gt;\n  arrange(avg)\n\n# A tibble: 4 × 3\n# Rowwise: \n  city         avg  range\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Gjoa Haven -14.4 42    \n2 Yakutsk     -8.8 58.1  \n3 Honiara     26.5  0.700\n4 Assab       30.5  8.7  \n\n\nLet’s see the values for Toronto as a baseline, and save them for later:\n\ncity_temps |&gt;\n  filter(city == \"Toronto\")\n\n# A tibble: 1 × 3\n# Rowwise: \n  city      avg range\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Toronto   9.4    26\n\ncity_temps |&gt;\n  filter(city == \"Toronto\") |&gt;\n  pull(avg) -&gt;\n  toronto_avg\n\ncity_temps |&gt;\n  filter(city == \"Toronto\") |&gt;\n  pull(range) -&gt;\n  toronto_range\n\nBy global standards, Toronto is cool on average, but in keeping with my subjective perception, the deviation from that average over the year is quite large."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#plots",
    "href": "posts/city_temperatures/city_temperatures.html#plots",
    "title": "Cities with Nice Weather",
    "section": "Plots",
    "text": "Plots\nLet’s look at a scatter plot with marginal histograms:\n\ncity_temps |&gt;\n  ggplot(aes(x = avg, y = range)) +\n  geom_point(alpha = 0.33, colour = \"#dc2828\") +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  geom_hline(yintercept = toronto_range,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  labs(title = \"Average Temperature vs Range by City\",\n       x = \"Average Temperature (°C)\",\n       y = \"Difference Between Hottest and Coldest Months (°C)\") +\n  theme_bw() -&gt;\n  plot\n\nplot |&gt;\n  ggMarginal(type = \"histogram\", fill = \"#b4b4b4\", size = 10) -&gt;\n  plot\n\nplot\n\n\n\n\nHere Toronto is indicated by the dashed lines.\nWe can see there’s a negative association between a city’s average temperature and the range of temperatures experienced there. In particular, there’s a big cluster of very hot cities which have little difference between their hottest and coldest months.\nTen tropical cities fall into both the hottest decile and the least varying decile:\n\ncity_temps |&gt; \n  filter(range &lt; quantile(city_temps$range, 0.1),\n         avg   &gt; quantile(city_temps$avg,   0.9)) |&gt;\n  select(city)\n\n# A tibble: 10 × 1\n# Rowwise: \n   city         \n   &lt;chr&gt;        \n 1 Lodwar       \n 2 Palembang    \n 3 Pontianak    \n 4 Kuala Lumpur \n 5 Malé         \n 6 Lanka Colombo\n 7 Oranjestad   \n 8 Willemstad   \n 9 Panama City  \n10 Barranquilla \n\n\nWhile these cities see very little temperature variation throughout the year, they are much too hot."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#zooming-in",
    "href": "posts/city_temperatures/city_temperatures.html#zooming-in",
    "title": "Cities with Nice Weather",
    "section": "Zooming In",
    "text": "Zooming In\nThe area of this plot I’m most interested in is the vertical slice around Toronto. Let’s see the same plot, including only the cities within one degree of Toronto’s average temperature.3 We’ll exclude the marginal histograms but add labels to the cities.3 I haven’t defined an ideal average temperature, but any city with a similar average and smaller range than Toronto is a clear improvement.\n\ncity_temps |&gt;\n  filter(abs(avg - toronto_avg)&lt;=1) |&gt;\n  ggplot(aes(x = avg, y = range, label = city)) +\n  geom_point(colour = \"#dc2828\") +\n  geom_text(size = 4, nudge_x = 0.01, hjust = \"left\") +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  geom_hline(yintercept = toronto_range,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  labs(title = \"Average Temperature vs Range by City (Detail 1)\",\n       x = \"Average Temperature (°C)\",\n       y = \"Difference Between Hottest and Coldest Months (°C)\") +\n  theme_bw()\n\n\n\n\nSo it seems that La Paz, Edinburgh, or Dublin might be good options.\nBut which cities are the best? These would be the ones with the smallest range for a given maximum average. Let’s find them."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#finding-the-cities-with-the-nicest-weather",
    "href": "posts/city_temperatures/city_temperatures.html#finding-the-cities-with-the-nicest-weather",
    "title": "Cities with Nice Weather",
    "section": "Finding the Cities with the Nicest Weather",
    "text": "Finding the Cities with the Nicest Weather\nWe want to know, for each maximum average temperature, the city that has the minimum range of temperatures. These are the cities that form the “bottom-left edge” of our first plot.\nNine cities fit this criterion:\n\ncity_temps |&gt;\n  arrange(avg) |&gt;\n  cbind(city_temps |&gt; arrange(avg) |&gt; pull(range) |&gt; cummin()) |&gt;\n  rename(running_min = 4) |&gt;\n  filter(range == running_min) |&gt;\n  select(city)\n\n        city\n1 Gjoa Haven\n2     Dikson\n3       Nuuk\n4  Reykjavík\n5    Stanley\n6     La Paz\n7      Cusco\n8     Bogotá\n9    Honiara\n\n\nOf these, the first two have temperatures which are more variable than Toronto, so we can remove them from consideration.\nLet’s plot the final seven candidates:\n\ncity_temps |&gt;\n  arrange(avg) |&gt;\n  cbind(city_temps |&gt; arrange(avg) |&gt; pull(range) |&gt; cummin()) |&gt;\n  rename(running_min = 4) |&gt;\n  filter(range == running_min) |&gt;\n  select(-running_min) |&gt;\n  filter(range &lt;= toronto_range) |&gt;\n  ggplot(aes(x = avg, y = range, label = city)) +\n  geom_point(colour = \"#dc2828\") +\n  geom_text(size = 4, nudge_x = 0.5, hjust = \"left\") +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  scale_x_continuous(expand = expansion(mult = 0.15)) +\n    labs(title = \"Average Temperature vs Range by City (Detail 2)\",\n         x = \"Average Temperature (°C)\",\n         y = \"Difference Between Hottest and Coldest Months (°C)\") +\n  theme_bw()\n\n\n\n\nAgain we see that La Paz has a similar overall average temperature to Toronto, but much less annual variability. Cusco and Bogotá are warmer but even less variable.\nReykjavík and Stanley are colder than Toronto, and while they represent a smaller decrease in variability compared to La Paz, Cusco, and Bogotá, they have the benefit (for me) of being 98%+ English-speaking.\nNuuk and Honiara are right out."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#next-steps",
    "href": "posts/city_temperatures/city_temperatures.html#next-steps",
    "title": "Cities with Nice Weather",
    "section": "Next Steps",
    "text": "Next Steps\nIt would be interesting to use detailed time series for each city and a utility function on temperatures (perhaps including wind chill and humidex) to determine which cities are truly mean-variance optimal.\nOf course, one should probably not choose a place to live based solely on the weather."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html",
    "href": "posts/coryat_scores/coryat_scores.html",
    "title": "Coryat Scores",
    "section": "",
    "text": "The Coryat score is a way of measuring one’s performance when playing along with Jeopardy! at home. It is named after musician, philosopher of physics, and two-day Jeopardy! champion Karl Coryat.\nA player’s Coryat score is the total value of clues answered correctly, minus that of those answered incorrectly, counting correctly-answered Daily Doubles according to their board position and ignoring Final Jeopardy! and any incorrectly-answered Daily Doubles.\nThus the Coryat score is a measure of one’s knowledge of the trivia material used on the show, ignoring other strategic elements like wagering.\n\n\n\nJ! Scorer is a convenient way to record games and determine one’s Coryat score. The site was created by two-time TV game show contestant Steve McClellan and has a public GitHub repository.\nJ! Scorer users can download a JSON file of their games."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#introduction",
    "href": "posts/coryat_scores/coryat_scores.html#introduction",
    "title": "Coryat Scores",
    "section": "",
    "text": "The Coryat score is a way of measuring one’s performance when playing along with Jeopardy! at home. It is named after musician, philosopher of physics, and two-day Jeopardy! champion Karl Coryat.\nA player’s Coryat score is the total value of clues answered correctly, minus that of those answered incorrectly, counting correctly-answered Daily Doubles according to their board position and ignoring Final Jeopardy! and any incorrectly-answered Daily Doubles.\nThus the Coryat score is a measure of one’s knowledge of the trivia material used on the show, ignoring other strategic elements like wagering.\n\n\n\nJ! Scorer is a convenient way to record games and determine one’s Coryat score. The site was created by two-time TV game show contestant Steve McClellan and has a public GitHub repository.\nJ! Scorer users can download a JSON file of their games."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#data",
    "href": "posts/coryat_scores/coryat_scores.html#data",
    "title": "Coryat Scores",
    "section": "Data",
    "text": "Data\nIt took a little effort to reverse-engineer the format of the files produced by J! Scorer. (I did this before finding the above-mentioned GitHub repo.) I used jsonlite::read_json plus a little trial and error. Then it’s just a matter of straightforward data transformations using dplyr.\n\n# read the JSON file, convert to data frame, and unnest some columns\nread_json(list.files()[grep(\".jscor\", list.files())], simplifyVector = TRUE) |&gt;\n  as.data.frame() |&gt;\n  unnest(cols = c(games_attributes.sixths_attributes,\n                  games_attributes.final_attributes),\n         names_repair = \"universal\") |&gt;\n  # keep only regular difficulty games\n  filter(!games_attributes.play_type %in% c(\"toc\", \"masters\")) |&gt;\n  # keep only the columns we need, add a round indicator, pivot to one row per\n  # clue instead of per category, and calculate each clue's contribution to the\n  # Coryat score\n  select(2, 10:14) |&gt;\n  mutate(round = rep(c(rep(1, 6), rep(2, 6)), n()/12)) |&gt;\n  pivot_longer(cols = 2:6, names_to = \"clue\", names_prefix = \"result\") |&gt;\n  rename(result = value) |&gt;\n  mutate(result = recode(result, `1` = -1, `3` = 1, `7` = 1, .default = 0),\n         score = round * as.numeric(clue) * 200 * result) |&gt;\n  # group by game, add up the total score, sort by game order, and throw away\n  # the time stamps\n  group_by(games_attributes.date_played) |&gt;\n  summarize(sum(score)) |&gt;\n  arrange(1) |&gt;\n  mutate(game = row_number()) |&gt;\n  select(-1) |&gt;\n  rename(score = `sum(score)`) -&gt;\n  # store the results\n  coryat_scores\n\n\nTable\n\ncoryat_scores |&gt;\n  kable(caption = \"My Jeopardy! Coryat Scores\") |&gt;\n  scroll_box(height = \"5in\")\n\n\n\nMy Jeopardy! Coryat Scores\n\n\nscore\ngame\n\n\n\n\n17800\n1\n\n\n10200\n2\n\n\n16000\n3\n\n\n12400\n4\n\n\n16400\n5\n\n\n16800\n6\n\n\n18200\n7\n\n\n16000\n8\n\n\n16000\n9\n\n\n18400\n10\n\n\n18600\n11\n\n\n22600\n12\n\n\n25000\n13\n\n\n17600\n14\n\n\n19600\n15\n\n\n24000\n16\n\n\n19600\n17\n\n\n18600\n18\n\n\n20200\n19\n\n\n17800\n20\n\n\n26800\n21\n\n\n21000\n22\n\n\n20800\n23\n\n\n19600\n24\n\n\n17800\n25\n\n\n18200\n26\n\n\n16600\n27\n\n\n25400\n28\n\n\n4000\n29\n\n\n27800\n30\n\n\n18600\n31\n\n\n27000\n32\n\n\n24200\n33\n\n\n21600\n34\n\n\n22600\n35\n\n\n21400\n36\n\n\n18800\n37\n\n\n11800\n38\n\n\n23200\n39\n\n\n28000\n40\n\n\n24400\n41\n\n\n14800\n42\n\n\n17000\n43\n\n\n16200\n44\n\n\n27800\n45\n\n\n18000\n46\n\n\n18200\n47\n\n\n28600\n48\n\n\n22600\n49\n\n\n13600\n50\n\n\n18000\n51\n\n\n16200\n52\n\n\n18400\n53\n\n\n17200\n54\n\n\n24200\n55\n\n\n19400\n56\n\n\n20400\n57\n\n\n21000\n58\n\n\n16800\n59\n\n\n19800\n60\n\n\n24800\n61\n\n\n19600\n62\n\n\n20400\n63\n\n\n24400\n64\n\n\n18600\n65\n\n\n17200\n66\n\n\n23400\n67\n\n\n26600\n68\n\n\n24800\n69\n\n\n21600\n70\n\n\n24600\n71\n\n\n21600\n72\n\n\n17600\n73\n\n\n12000\n74\n\n\n21600\n75\n\n\n17800\n76\n\n\n23600\n77\n\n\n21800\n78\n\n\n22400\n79\n\n\n18800\n80\n\n\n28400\n81\n\n\n16800\n82\n\n\n21400\n83\n\n\n5600\n84\n\n\n15000\n85\n\n\n19400\n86\n\n\n14200\n87\n\n\n16400\n88\n\n\n23200\n89\n\n\n20400\n90\n\n\n22600\n91\n\n\n18200\n92\n\n\n14000\n93\n\n\n17000\n94\n\n\n19000\n95\n\n\n18400\n96\n\n\n18400\n97\n\n\n26200\n98\n\n\n26400\n99\n\n\n22800\n100\n\n\n33200\n101\n\n\n19800\n102\n\n\n26400\n103\n\n\n18600\n104\n\n\n13600\n105\n\n\n22200\n106\n\n\n24800\n107\n\n\n27400\n108\n\n\n13400\n109\n\n\n14000\n110\n\n\n13600\n111\n\n\n15200\n112\n\n\n12000\n113\n\n\n20800\n114\n\n\n26200\n115\n\n\n16800\n116\n\n\n20400\n117\n\n\n28800\n118\n\n\n32400\n119\n\n\n10400\n120\n\n\n20000\n121\n\n\n12400\n122\n\n\n11400\n123\n\n\n20600\n124\n\n\n19800\n125\n\n\n18200\n126\n\n\n20200\n127\n\n\n20000\n128\n\n\n15800\n129\n\n\n16400\n130\n\n\n17000\n131\n\n\n22400\n132\n\n\n15600\n133\n\n\n20200\n134\n\n\n20600\n135\n\n\n18600\n136\n\n\n19400\n137\n\n\n22800\n138\n\n\n13400\n139\n\n\n18800\n140\n\n\n15200\n141\n\n\n20000\n142\n\n\n14000\n143\n\n\n25200\n144\n\n\n25800\n145\n\n\n20000\n146\n\n\n28600\n147\n\n\n16200\n148\n\n\n16400\n149\n\n\n18000\n150\n\n\n20200\n151\n\n\n18600\n152\n\n\n19800\n153\n\n\n13000\n154\n\n\n10200\n155\n\n\n15600\n156\n\n\n14400\n157\n\n\n25200\n158\n\n\n5800\n159\n\n\n12800\n160\n\n\n15600\n161\n\n\n10200\n162\n\n\n15400\n163\n\n\n26800\n164\n\n\n14800\n165\n\n\n21400\n166\n\n\n16800\n167\n\n\n19200\n168\n\n\n17600\n169\n\n\n20800\n170\n\n\n25000\n171\n\n\n27400\n172\n\n\n20800\n173\n\n\n\n\n\n\n\n\n\nAverage Score\nAn average score of around $25,000 is considered appropriate for prospective contestants.\n\n# calculate my mean score\ncoryat_scores |&gt; pull(score) |&gt; mean() |&gt; round()\n\n[1] 19405\n\n\nClearly, I have some studying to do before I consider trying to compete on the show."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#visualization",
    "href": "posts/coryat_scores/coryat_scores.html#visualization",
    "title": "Coryat Scores",
    "section": "Visualization",
    "text": "Visualization\nWe can create a histogram showing the distribution of my scores and a line chart showing the evolution of my scores over time using ggplot2.\n\nLine Chart and HistogramCumulative Distribution\n\n\n\n\nCode\n# main plot\n(coryat_scores |&gt;\n  ggplot() +\n  # aesthetic mapping\n  aes(x = game, y = score) +\n  # visual elements representing the data\n  geom_line(colour = \"#b4b4b4\") +\n  geom_smooth(se = FALSE, colour = \"black\") +\n  geom_hline(yintercept = 25000, linetype = \"dashed\") +\n  geom_point(colour = \"#dc2828\") +\n  # scales\n  scale_y_continuous(labels = scales::label_dollar(), \n                     n.breaks = 6, \n                     limits = c(0, 30000)) +\n  scale_x_continuous(expand = c(0, 0), breaks = NULL) +\n  # labels\n  labs(title = \"My Jeopardy! Coryat Scores\",\n       subtitle = \"Trend and distribution\", \n       x = \"\", \n       y = \"Score\",\n       caption = \"Dashed black line indicates target average of $25,000.\") +\n  # theming\n  theme_bw()) |&gt;\n  # add the marginal histogram\n  ggMarginal(type = \"histogram\", margins = \"y\", fill = \"#b4b4b4\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoryat_scores |&gt;\n  arrange(score) |&gt;\n  ggplot(aes(x = score, y = cumsum(score)/sum(score))) +\n  geom_line(size = 2, colour = \"#dc2828\") +\n  scale_x_continuous(labels = scales::label_dollar(), \n                     n.breaks = 6, \n                     expand = c(0, 0)) +\n  scale_y_continuous(labels = scales::label_percent(), expand = c(0, 0)) +\n  labs(title = \"My Jeopardy! Coryat Scores\",\n       subtitle = \"Cumulative distribution\", \n       x = \"Score\", \n       y = \"\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#references",
    "href": "posts/coryat_scores/coryat_scores.html#references",
    "title": "Coryat Scores",
    "section": "References",
    "text": "References\n\nJ! Scorer\nJ! Archive\nJ!ometry"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html",
    "href": "posts/crossword_times/crossword_times.html",
    "title": "Crossword Times",
    "section": "",
    "text": "Down For Across is a website which allows you to solve user-uploaded crosswords solo or with friends.\nThe crosswords published in major American newspapers often differ in difficulty according to the day of the week. For example, the New York Times’ guide explains that their Monday puzzles are the easiest and their Saturday puzzles the hardest. Meanwhile, the Sunday puzzles have easy clues but a large grid, and the Thursday puzzles have some unique gimmick. By contrast, the New Yorker’s crosswords decrease in difficulty over the week.\nI’ve started keeping track of my solo completion times for the New York Times, Los Angeles Times, New Yorker, and Wall Street Journal crosswords."
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#introduction",
    "href": "posts/crossword_times/crossword_times.html#introduction",
    "title": "Crossword Times",
    "section": "",
    "text": "Down For Across is a website which allows you to solve user-uploaded crosswords solo or with friends.\nThe crosswords published in major American newspapers often differ in difficulty according to the day of the week. For example, the New York Times’ guide explains that their Monday puzzles are the easiest and their Saturday puzzles the hardest. Meanwhile, the Sunday puzzles have easy clues but a large grid, and the Thursday puzzles have some unique gimmick. By contrast, the New Yorker’s crosswords decrease in difficulty over the week.\nI’ve started keeping track of my solo completion times for the New York Times, Los Angeles Times, New Yorker, and Wall Street Journal crosswords."
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#reading-the-data",
    "href": "posts/crossword_times/crossword_times.html#reading-the-data",
    "title": "Crossword Times",
    "section": "Reading the Data",
    "text": "Reading the Data\nI’m accustomed to thinking of the week as beginning with Sunday, but for the purposes of crossword difficulty analysis, Monday is better. We’ll convert the solve times from 00:00-style to minutes.\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |&gt;\n  transmute(Publisher,\n            day = factor(wday(Date, week_start = 1)),\n            minutes = period_to_seconds(ms(Time))/60) |&gt;\n  rename(publisher = Publisher) -&gt;\n  crossword_times\n\n\nTableSummary\n\n\n\n\nCode\ncrossword_times |&gt;\n  mutate(minutes = round(minutes)) |&gt;\n  kable(caption = \"My Crossword Solve Times\") |&gt;\n  scroll_box(height = \"5in\")\n\n\n\n\nMy Crossword Solve Times\n\n\npublisher\nday\nminutes\n\n\n\n\nNew York Times\n2\n11\n\n\nLos Angeles Times\n2\n6\n\n\nLos Angeles Times\n3\n9\n\n\nNew York Times\n3\n22\n\n\nNew York Times\n4\n14\n\n\nLos Angeles Times\n4\n16\n\n\nLos Angeles Times\n5\n19\n\n\nNew York Times\n5\n22\n\n\nNew York Times\n1\n9\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n10\n\n\nNew York Times\n3\n12\n\n\nLos Angeles Times\n3\n7\n\n\nNew York Times\n4\n18\n\n\nLos Angeles Times\n4\n21\n\n\nLos Angeles Times\n5\n8\n\n\nNew York Times\n7\n29\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n6\n17\n\n\nNew York Times\n6\n37\n\n\nLos Angeles Times\n6\n14\n\n\nLos Angeles Times\n2\n10\n\n\nNew Yorker\n2\n13\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n2\n5\n\n\nNew Yorker\n2\n16\n\n\nNew Yorker\n1\n19\n\n\nNew Yorker\n4\n8\n\n\nNew Yorker\n5\n8\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n4\n5\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n12\n\n\nNew York Times\n7\n36\n\n\nNew Yorker\n3\n7\n\n\nNew Yorker\n1\n33\n\n\nLos Angeles Times\n6\n15\n\n\nNew York Times\n4\n38\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n26\n\n\nNew Yorker\n5\n22\n\n\nLos Angeles Times\n5\n17\n\n\nLos Angeles Times\n6\n17\n\n\nNew York Times\n6\n18\n\n\nNew York Times\n7\n61\n\n\nNew Yorker\n1\n65\n\n\nLos Angeles Times\n7\n38\n\n\nNew Yorker\n3\n8\n\n\nNew Yorker\n2\n17\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n1\n21\n\n\nLos Angeles Times\n7\n23\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n15\n\n\nNew Yorker\n1\n22\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n27\n\n\nLos Angeles Times\n4\n18\n\n\nNew York Times\n4\n14\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n48\n\n\nLos Angeles Times\n5\n22\n\n\nLos Angeles Times\n6\n18\n\n\nNew York Times\n7\n41\n\n\nLos Angeles Times\n7\n27\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n7\n\n\nNew York Times\n3\n7\n\n\nNew Yorker\n1\n39\n\n\nNew Yorker\n2\n12\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n7\n\n\nNew Yorker\n3\n13\n\n\nLos Angeles Times\n4\n18\n\n\nNew York Times\n4\n19\n\n\nNew Yorker\n4\n6\n\n\nNew Yorker\n5\n10\n\n\nLos Angeles Times\n5\n18\n\n\nNew York Times\n5\n13\n\n\nLos Angeles Times\n6\n32\n\n\nNew York Times\n6\n35\n\n\nNew York Times\n7\n47\n\n\nLos Angeles Times\n7\n24\n\n\nLos Angeles Times\n1\n9\n\n\nNew York Times\n1\n8\n\n\nNew Yorker\n1\n37\n\n\nLos Angeles Times\n2\n12\n\n\nNew York Times\n2\n7\n\n\nNew York Times\n5\n55\n\n\nNew Yorker\n2\n16\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n13\n\n\nNew Yorker\n3\n14\n\n\nLos Angeles Times\n4\n13\n\n\nNew Yorker\n4\n9\n\n\nNew York Times\n4\n22\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n5\n12\n\n\nLos Angeles Times\n3\n8\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n3\n18\n\n\nLos Angeles Times\n4\n11\n\n\nNew York Times\n6\n22\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n5\n17\n\n\nNew York Times\n3\n14\n\n\nLos Angeles Times\n3\n10\n\n\nNew York Times\n4\n17\n\n\nLos Angeles Times\n4\n17\n\n\nNew Yorker\n4\n8\n\n\nNew Yorker\n3\n21\n\n\nLos Angeles Times\n5\n29\n\n\nNew York Times\n5\n29\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n10\n\n\nNew Yorker\n1\n50\n\n\nNew Yorker\n2\n14\n\n\nNew York Times\n2\n6\n\n\nLos Angeles Times\n3\n10\n\n\nNew Yorker\n3\n12\n\n\nNew York Times\n3\n12\n\n\nWall Street Journal\n3\n19\n\n\nNew York Times\n4\n24\n\n\nWall Street Journal\n4\n19\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n7\n\n\nLos Angeles Times\n5\n24\n\n\nNew Yorker\n5\n10\n\n\nWall Street Journal\n6\n47\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n20\n\n\nNew Yorker\n2\n16\n\n\nNew York Times\n2\n10\n\n\nLos Angeles Times\n2\n7\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n7\n\n\nWall Street Journal\n3\n11\n\n\nNew Yorker\n3\n11\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n4\n20\n\n\nWall Street Journal\n4\n15\n\n\nNew York Times\n5\n19\n\n\nNew Yorker\n5\n7\n\n\nWall Street Journal\n5\n13\n\n\nNew York Times\n6\n56\n\n\nLos Angeles Times\n6\n21\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nWall Street Journal\n6\n34\n\n\nNew Yorker\n1\n39\n\n\nLos Angeles Times\n2\n7\n\n\nNew Yorker\n2\n21\n\n\nNew York Times\n3\n10\n\n\nNew Yorker\n3\n8\n\n\nLos Angeles Times\n4\n8\n\n\nNew York Times\n4\n23\n\n\nNew Yorker\n4\n8\n\n\nLos Angeles Times\n6\n22\n\n\nNew York Times\n6\n34\n\n\nNew York Times\n5\n39\n\n\nNew Yorker\n5\n28\n\n\nLos Angeles Times\n7\n41\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n37\n\n\nNew York Times\n1\n8\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n3\n14\n\n\nWall Street Journal\n2\n11\n\n\nNew York Times\n2\n9\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n18\n\n\nNew York Times\n3\n13\n\n\nLos Angeles Times\n3\n11\n\n\nNew Yorker\n3\n7\n\n\nNew York Times\n4\n21\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n6\n\n\nLos Angeles Times\n5\n12\n\n\nNew York Times\n5\n21\n\n\nNew Yorker\n5\n11\n\n\nLos Angeles Times\n6\n45\n\n\nNew York Times\n6\n41\n\n\nLos Angeles Times\n7\n27\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n11\n\n\nNew York Times\n3\n16\n\n\nLos Angeles Times\n3\n10\n\n\nNew Yorker\n3\n12\n\n\nWall Street Journal\n3\n16\n\n\nNew Yorker\n5\n11\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n2\n31\n\n\nWall Street Journal\n1\n9\n\n\nLos Angeles Times\n4\n36\n\n\nNew York Times\n5\n18\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n10\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n6\n\n\nNew York Times\n3\n19\n\n\nNew York Times\n4\n28\n\n\nNew Yorker\n4\n5\n\n\nLos Angeles Times\n4\n6\n\n\nNew Yorker\n5\n15\n\n\nWall Street Journal\n5\n19\n\n\nNew York Times\n5\n16\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n9\n\n\nNew York Times\n3\n8\n\n\nWall Street Journal\n1\n5\n\n\nNew York Times\n6\n31\n\n\nWall Street Journal\n2\n10\n\n\nLos Angeles Times\n4\n11\n\n\nNew York Times\n4\n26\n\n\nNew Yorker\n4\n4\n\n\nLos Angeles Times\n5\n13\n\n\nNew York Times\n5\n39\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nLos Angeles Times\n2\n7\n\n\nWall Street Journal\n2\n10\n\n\nWall Street Journal\n6\n34\n\n\nWall Street Journal\n6\n43\n\n\nNew York Times\n2\n7\n\n\nNew York Times\n3\n12\n\n\nLos Angeles Times\n3\n20\n\n\nNew Yorker\n3\n10\n\n\nWall Street Journal\n1\n10\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n26\n\n\nLos Angeles Times\n5\n9\n\n\nWall Street Journal\n5\n28\n\n\nNew Yorker\n5\n16\n\n\nNew York Times\n7\n23\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n5\n\n\nWall Street Journal\n1\n7\n\n\nLos Angeles Times\n2\n7\n\n\nNew Yorker\n2\n22\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n14\n\n\nLos Angeles Times\n3\n11\n\n\nNew Yorker\n3\n14\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n4\n28\n\n\nWall Street Journal\n4\n19\n\n\nLos Angeles Times\n5\n18\n\n\nNew York Times\n5\n20\n\n\nNew Yorker\n5\n16\n\n\nWall Street Journal\n5\n15\n\n\nLos Angeles Times\n6\n33\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n3\n11\n\n\nLos Angeles Times\n3\n11\n\n\nNew York Times\n5\n11\n\n\nLos Angeles Times\n5\n12\n\n\nNew Yorker\n4\n9\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n5\n10\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n2\n27\n\n\nNew Yorker\n1\n39\n\n\nNew York Times\n2\n8\n\n\nNew York Times\n3\n13\n\n\nNew Yorker\n3\n5\n\n\nLos Angeles Times\n4\n12\n\n\nNew Yorker\n4\n6\n\n\nWall Street Journal\n2\n11\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n4\n12\n\n\nWall Street Journal\n3\n9\n\n\nWall Street Journal\n5\n14\n\n\nNew Yorker\n5\n11\n\n\nNew York Times\n6\n66\n\n\nNew York Times\n7\n44\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n10\n\n\nLos Angeles Times\n3\n13\n\n\nNew York Times\n4\n40\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n9\n\n\nNew York Times\n5\n25\n\n\nNew York Times\n2\n7\n\n\nNew Yorker\n5\n16\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n2\n8\n\n\nLos Angeles Times\n2\n6\n\n\nNew Yorker\n1\n58\n\n\nNew Yorker\n2\n14\n\n\nNew York Times\n3\n8\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n4\n20\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n5\n22\n\n\nLos Angeles Times\n5\n18\n\n\nNew Yorker\n5\n16\n\n\nLos Angeles Times\n6\n41\n\n\nNew York Times\n7\n35\n\n\nNew York Times\n1\n5\n\n\nNew Yorker\n1\n25\n\n\nLos Angeles Times\n1\n6\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n13\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n5\n9\n\n\nNew York Times\n3\n18\n\n\nLos Angeles Times\n3\n8\n\n\nNew Yorker\n3\n7\n\n\nWall Street Journal\n6\n23\n\n\nWall Street Journal\n6\n28\n\n\nNew York Times\n4\n28\n\n\nLos Angeles Times\n4\n7\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n21\n\n\nLos Angeles Times\n5\n10\n\n\nNew Yorker\n5\n26\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n11\n\n\nLos Angeles Times\n6\n24\n\n\nWall Street Journal\n1\n7\n\n\nNew Yorker\n2\n37\n\n\nNew York Times\n2\n6\n\n\nLos Angeles Times\n2\n8\n\n\nWall Street Journal\n2\n13\n\n\nLos Angeles Times\n3\n12\n\n\nNew York Times\n3\n9\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n19\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n6\n\n\nNew Yorker\n5\n21\n\n\nNew York Times\n1\n9\n\n\nNew York Times\n7\n41\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n25\n\n\nWall Street Journal\n3\n22\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n6\n\n\nLos Angeles Times\n5\n22\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n6\n\n\nWall Street Journal\n1\n5\n\n\nNew Yorker\n5\n8\n\n\nNew York Times\n3\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n2\n6\n\n\nNew Yorker\n2\n16\n\n\nWall Street Journal\n2\n11\n\n\nNew York Times\n3\n10\n\n\nLos Angeles Times\n3\n6\n\n\nNew York Times\n4\n14\n\n\nLos Angeles Times\n4\n12\n\n\nNew Yorker\n4\n4\n\n\nLos Angeles Times\n5\n13\n\n\nNew Yorker\n5\n6\n\n\nNew York Times\n6\n29\n\n\nWall Street Journal\n6\n38\n\n\nLos Angeles Times\n1\n9\n\n\nNew York Times\n1\n7\n\n\nNew Yorker\n4\n7\n\n\nLos Angeles Times\n4\n14\n\n\nNew Yorker\n5\n23\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n2\n10\n\n\nNew York Times\n3\n11\n\n\nNew Yorker\n5\n14\n\n\nNew York Times\n3\n5\n\n\nNew Yorker\n3\n10\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n1\n4\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n2\n5\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n4\n11\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n13\n\n\nNew Yorker\n5\n14\n\n\nNew York Times\n6\n17\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n8\n\n\nNew York Times\n2\n9\n\n\nNew York Times\n4\n16\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n8\n\n\nNew York Times\n4\n15\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n7\n25\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n7\n\n\nWall Street Journal\n2\n8\n\n\nNew York Times\n3\n8\n\n\nLos Angeles Times\n3\n7\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n4\n4\n\n\nNew York Times\n5\n19\n\n\nNew York Times\n1\n4\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n6\n\n\nNew Yorker\n2\n15\n\n\nNew York Times\n3\n10\n\n\nWall Street Journal\n3\n19\n\n\nNew York Times\n4\n30\n\n\nLos Angeles Times\n4\n13\n\n\nWall Street Journal\n4\n27\n\n\nNew York Times\n5\n12\n\n\nLos Angeles Times\n5\n7\n\n\nNew Yorker\n5\n15\n\n\nNew York Times\n6\n11\n\n\nWall Street Journal\n5\n18\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n5\n\n\nWall Street Journal\n1\n9\n\n\nNew Yorker\n1\n31\n\n\nNew York Times\n2\n5\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n2\n19\n\n\nNew York Times\n3\n12\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n23\n\n\nNew Yorker\n4\n6\n\n\nWall Street Journal\n4\n20\n\n\nNew York Times\n5\n19\n\n\nNew Yorker\n3\n8\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n14\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n2\n8\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n12\n\n\nNew Yorker\n3\n6\n\n\nNew York Times\n4\n16\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n9\n\n\nNew Yorker\n5\n8\n\n\nNew York Times\n6\n38\n\n\nLos Angeles Times\n6\n15\n\n\nLos Angeles Times\n5\n13\n\n\nNew York Times\n7\n26\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n24\n\n\nNew York Times\n2\n12\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n9\n\n\nNew York Times\n3\n8\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n4\n17\n\n\nLos Angeles Times\n4\n9\n\n\nNew Yorker\n4\n4\n\n\nNew Yorker\n5\n12\n\n\nNew York Times\n7\n51\n\n\nLos Angeles Times\n7\n25\n\n\nNew Yorker\n1\n30\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n7\n68\n\n\nNew York Times\n3\n9\n\n\nLos Angeles Times\n3\n5\n\n\nNew Yorker\n5\n8\n\n\n\n\n\n\n\n\n\n\n\nCode\ncrossword_times |&gt;\n  group_by(publisher, day) |&gt;\n  summarize(n = n(), mean = mean(minutes), sd = sd(minutes)) |&gt;\n  mutate(across(c(\"mean\", \"sd\"), \\(x) round(x))) |&gt;\n  pivot_wider(names_from = publisher, \n              values_from = c(\"n\", \"mean\", \"sd\"),\n              names_sort = TRUE) |&gt;\n  relocate(c(1, 2, 6, 10, 3, 7, 11, 4, 8, 12, 5, 9, 13)) |&gt;\n  gt() |&gt;\n  tab_header(title = \"My Crossword Solve Times: Summary\") |&gt;\n  tab_spanner(label = \"Los Angeles Times\", columns = 2:4) |&gt;\n  tab_spanner(label = \"New York Times\", columns = 5:7) |&gt;\n  tab_spanner(label = \"New Yorker\", columns = 8:10) |&gt;\n  tab_spanner(label = \"Wall Street Journal\", columns = 11:13) |&gt;\n  cols_label(starts_with(\"n\") ~ \"n\", \n             starts_with(\"mean\") ~ \"mean\", \n             starts_with(\"sd\") ~ \"sd\")\n\n\n\n\n\n\n  \n    \n      My Crossword Solve Times: Summary\n    \n    \n    \n      day\n      \n        Los Angeles Times\n      \n      \n        New York Times\n      \n      \n        New Yorker\n      \n      \n        Wall Street Journal\n      \n    \n    \n      n\n      mean\n      sd\n      n\n      mean\n      sd\n      n\n      mean\n      sd\n      n\n      mean\n      sd\n    \n  \n  \n    1\n31\n7\n1\n37\n6\n1\n17\n35\n13\n10\n7\n2\n    2\n27\n8\n1\n33\n8\n2\n20\n18\n7\n7\n11\n1\n    3\n23\n9\n3\n35\n12\n5\n24\n11\n4\n7\n16\n5\n    4\n23\n13\n6\n27\n22\n7\n36\n6\n1\n8\n18\n6\n    5\n18\n16\n6\n23\n23\n12\n30\n13\n6\n7\n16\n6\n    6\n12\n25\n11\n14\n32\n15\nNA\nNA\nNA\n7\n35\n8\n    7\n7\n29\n7\n13\n41\n14\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#visualizing-solve-times",
    "href": "posts/crossword_times/crossword_times.html#visualizing-solve-times",
    "title": "Crossword Times",
    "section": "Visualizing Solve Times",
    "text": "Visualizing Solve Times\nIt’s a matter of some debate among data visualization practitioners about when to use density plots, such as violin plots, instead of histograms. Or indeed whether density plots should ever be used. Or even whether histograms should ever be used!\nI disagree with several of the points Angela Collier makes in her video “violin plots should not exist”, but one that I find compelling is that drawing density plots requires making modelling assumptions which may not accurately reflect the actual process from which the data result.\nIn most situations, ggplot uses locally estimated scatterplot smoothing (LOESS) by default, which involves evaluating a fitting a separate polynomial regression model on a weighted neighbourhood around each data point and evaluating it there. It makes nice looking violin plots, but you wouldn’t expect it to reflect that “actual” theoretical distribution of the data. In particular you run into edge cases like a non-zero violin width outside the range in which the data can actually fall. LOESS doesn’t know that it can’t take -1.5 minutes for the first Northern cardinal to appear at your bird feeder after you fill it.\nIt seems to me that this sort of thing is a symptom of a general desire to avoid having to actually specify models by pretending that there’s some bright-line distinction between descriptive statistics and statistical inference.\nSince we were willing to actually specify a model, we can make density plots that show something meaningful: the posterior predictive distributions corresponding to our model. We’ll use add_predicted_draws() from tidybayes to generate draws from those models and use these to draw violins that reflect the data we expect to observe in the future based on those we have observed so far. (We also enlist the help of data_grid from modelr.)\n\nViolin Plot\n\n\nCode\ncrossword_times |&gt;\n  data_grid(publisher, day) |&gt;\n  add_predicted_draws(solve_time_model) |&gt;\n  # filter out predictions for groups that do not actually occur\n  filter(!(publisher == \"New Yorker\" & day %in% c(6, 7)),\n         !(publisher == \"Wall Street Journal\" & day == 7)) |&gt;\n  ggplot() +\n  # plot structure\n  facet_grid(cols = vars(publisher)) +\n  # aesthetic mapping for the violins\n  aes(y = .prediction, x = day) +\n  # visual elements representing the posterior predictive model\n  stat_eye(colour = NA, fill = \"grey\") +\n  # visual elements representing the observed data\n  geom_jitter(width = 0.25, \n              data = crossword_times, \n              aes(y = minutes, fill = publisher),\n              alpha = 0.5,\n              pch = 21) +\n  # scales\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  scale_y_continuous(breaks = seq(0, 70, 5), \n                     limits = c(0, 70), \n                     expand = c(0, 0)) +\n  scale_fill_viridis(discrete = TRUE) +\n  # labels\n  labs(title = \"Crossword Solve Times\",\n       subtitle = \"With posterior predictive log-normal distributions\",\n       x = \"Day of the Week\",\n       y = \"Solve Time (Minutes)\") +\n  guides(fill = \"none\") +\n  # theming\n  theme_bw() +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        strip.text = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nViolins vs Log-Normals\nWhat’s the difference between plotting LOESS-based densities and posterior predictive distributions? I think this image illustrates the shortcomings of the former approach well:\n\n\n\ngeom_violin (black) vs our posterior predictive distributions (grey)."
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#references",
    "href": "posts/crossword_times/crossword_times.html#references",
    "title": "Crossword Times",
    "section": "References",
    "text": "References\n\nHow to Solve The New York Times Crossword\nAnna Shechtman, the new queen of crosswords"
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html",
    "href": "posts/homeworlds_openings/homeworlds_openings.html",
    "title": "Homeworlds Openings",
    "section": "",
    "text": "Homeworlds is an abstract space battle game designed by John Cooper and published by Looney Labs. It’s played with multipurpose game pieces called Looney pyramids, three of each of combination of three sizes (small, medium, and large) and four colours (green, yellow, blue, and red) for 36 in total. A pyramid lying down is a space ship and a pyramid standing up is a star. Each colour is associated with an ability.\nAt the beginning of the game, each player chooses two stars to be their homeworld. Star systems are connected if they have no sizes in common.\n\n\n\nHomeworlds star connection rule (from Looney Labs)\n\n\nExpert players actively debate Homeworlds’ opening theory. One question is which combination of star sizes should be selected by the first player, and how the second player should respond.\n\n\n\nSome homeworld options (from Looney Labs)\n\n\nWhen players’ homeworlds are one step away from one another (i.e. connected), this is known as a “microverse”. If they are two steps away, that’s a “small universe”, and if they are three steps away, that’s a “large universe”.\nIn addition to their homeworld stars, each player selects a pyramid to be their first ship. The rest of the pieces become the bank from which new stars and ships are taken and to which destroyed stars and ships are returned."
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#introduction",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#introduction",
    "title": "Homeworlds Openings",
    "section": "",
    "text": "Homeworlds is an abstract space battle game designed by John Cooper and published by Looney Labs. It’s played with multipurpose game pieces called Looney pyramids, three of each of combination of three sizes (small, medium, and large) and four colours (green, yellow, blue, and red) for 36 in total. A pyramid lying down is a space ship and a pyramid standing up is a star. Each colour is associated with an ability.\nAt the beginning of the game, each player chooses two stars to be their homeworld. Star systems are connected if they have no sizes in common.\n\n\n\nHomeworlds star connection rule (from Looney Labs)\n\n\nExpert players actively debate Homeworlds’ opening theory. One question is which combination of star sizes should be selected by the first player, and how the second player should respond.\n\n\n\nSome homeworld options (from Looney Labs)\n\n\nWhen players’ homeworlds are one step away from one another (i.e. connected), this is known as a “microverse”. If they are two steps away, that’s a “small universe”, and if they are three steps away, that’s a “large universe”.\nIn addition to their homeworld stars, each player selects a pyramid to be their first ship. The rest of the pieces become the bank from which new stars and ships are taken and to which destroyed stars and ships are returned."
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#data",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#data",
    "title": "Homeworlds Openings",
    "section": "Data",
    "text": "Data\nBabamots compiled information from 4928 games of Homeworlds played on BoardGameArena and provided a first player win rate and a count of observations for each combination of first and second player homeworld sizes. Draws were counted as half a win for each player. Only games that lasted at least eight turns for each player were included.\n\nread_csv(\"homeworlds_openings.csv\") -&gt; \n  homeworlds_openings\n\nhomeworlds_openings |&gt;\n  gt() |&gt; \n  cols_label(p1_wr = \"First Player Win Rate\",\n             n = \"Number of Games\",\n             p1_hw = \"First Player\",\n             p2_hw = \"Second Player\") |&gt;\n  tab_spanner(label = md(\"**Homeworld Sizes**\"), columns = 3:4) |&gt;\n  cols_align(align = \"center\", columns = everything()) |&gt;\n  tab_header(title = \"Homeworlds Openings Data\",\n             subtitle = \"Source: Scraped from BGA by Babamots\") |&gt;\n  tab_options(container.height = \"500px\") |&gt;\n  opt_row_striping()\n\n\n\n\n\n  \n    \n      Homeworlds Openings Data\n    \n    \n      Source: Scraped from BGA by Babamots\n    \n    \n      First Player Win Rate\n      Number of Games\n      \n        Homeworld Sizes\n      \n    \n    \n      First Player\n      Second Player\n    \n  \n  \n    0.175\n20\nSS\nSM\n    0.200\n10\nSS\nML\n    0.250\n4\nSS\nLL\n    0.292\n12\nSS\nSL\n    0.385\n13\nSL\nMM\n    0.393\n28\nML\nMM\n    0.408\n71\nSM\nMM\n    0.409\n22\nML\nSS\n    0.441\n34\nML\nLL\n    0.449\n476\nSL\nSM\n    0.472\n724\nML\nSM\n    0.481\n27\nSL\nLL\n    0.486\n590\nSM\nSL\n    0.491\n53\nLL\nSL\n    0.495\n92\nLL\nLL\n    0.495\n813\nSM\nML\n    0.500\n3\nSS\nMM\n    0.502\n430\nSL\nML\n    0.510\n52\nSM\nLL\n    0.538\n398\nML\nSL\n    0.550\n30\nMM\nMM\n    0.571\n7\nSS\nSS\n    0.599\n292\nML\nML\n    0.604\n130\nSL\nSL\n    0.625\n8\nMM\nLL\n    0.631\n84\nLL\nML\n    0.635\n52\nMM\nSM\n    0.643\n300\nSM\nSM\n    0.667\n3\nSM\nSS\n    0.667\n6\nSL\nSS\n    0.667\n6\nMM\nSS\n    0.679\n56\nMM\nML\n    0.723\n47\nLL\nSM\n    0.750\n20\nMM\nSL\n    0.833\n6\nLL\nSS\n    0.889\n9\nLL\nMM\n  \n  \n  \n\n\n\n\nWe’ll also create some helper vectors and functions that will be convenient later.\n\n# helper vectors\n#c(\"SS\", \"SM\", \"SL\", \"MM\", \"ML\", \"LL\") -&gt; hw_sizes\nc(\"SM\", \"SL\", \"ML\", \"SS\", \"MM\", \"LL\") -&gt; hw_sizes\nc(\"micro\", \"small\", \"large\") -&gt; universe_sizes\n\n# list the stars in a homeworld\n(function(hw) {unlist(strsplit(as.character(hw), \"\"))}) -&gt; stars\n\n# determine the universe size\n(function(hw1, hw2) {\n  case_when(length(intersect(stars(hw1), stars(hw2))) == 0 ~ \"micro\",\n            length(unique(c(stars(hw1), stars(hw2)))) &lt;= 2 ~ \"small\",\n            T ~ \"large\")}) -&gt; universe_size"
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#modelling-first-player-advantage",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#modelling-first-player-advantage",
    "title": "Homeworlds Openings",
    "section": "Modelling First Player Advantage",
    "text": "Modelling First Player Advantage\nWe can model first player advantage by imagining that a game of Homeworlds is like a biased coin flip, where each opening is associated with an independent bias. For example, if the bias parameter for some opening is 0.6, then we expect the first player to win 60% of the time that opening is played.11 Assuming “average play”, anyway.\nIf this parameter is distributed according to a beta distribution Beta(a, b), then its posterior distribution accounting for observing n wins and m losses for the first player is Beta(a + n, b + m).2 All that is left is to decide on a prior distribution representing our beliefs before observing any data. This is an interesting subtlety3, but it seems reasonable to choose a uniform prior, Beta(1, 1), under which all possibilities are equally credible.2 See these slides for a quick proof of this fact.3 See this StackExchange answer for more on uninformative Beta priors.\nFrom the posterior distribution, we can construct a 90% credible interval using qbeta. Unlike the confidence intervals used in frequentist statistical inference, which are the source of much confusion, Bayesian credible intervals have a straightforward interpretation: we’re 90% sure the true value of the parameter is contained in a 90% credible interval.44 Frequentists view the bounds of a confidence interval as random, and the parameter value as fixed. Bayesians view the bounds of a credible interval as fixed, and the parameter value as random.\nWe can also calculate the probability that the first player is favoured (i.e. is expected to win at least half the time) in each opening using pbeta. In this case we don’t consider the extent of the advantage, just whether there is one.\n\nhomeworlds_openings |&gt;\n  mutate(across(c(p1_hw, p2_hw), \n                \\(x) factor(x, ordered = T, levels = hw_sizes))) |&gt;\n  rowwise() |&gt;\n  mutate(p1_wins = n*p1_wr,\n         p2_wins = n*(1 - p1_wr),\n         # calculate a 90% credible interval from the posterior distribution\n         ci_lower = qbeta(c(0.05, 0.95), 1 + p1_wins, 1 + p2_wins)[1],\n         ci_upper = qbeta(c(0.05, 0.95), 1 + p1_wins, 1 + p2_wins)[2],\n         # format the interval as text\n         formatted_ci = paste0(round(100 * ci_lower),\n                               \"-\",\n                               round(100 * ci_upper),\n                               \"%\"),\n         # bold intervals that do not contain 50%\n         ci_fontface = case_when(!between(0.5, ci_lower, ci_upper) ~ \"bold\",\n                                 T ~ \"plain\"),\n         universe_size = universe_size(p1_hw, p2_hw),\n         universe_size = factor(universe_size, \n                                ordered = TRUE, \n                                levels = universe_sizes),\n         prob_fav = 1 - pbeta(0.5, 1 + p1_wins, 1 + p2_wins),\n         formatted_prob_fav = case_when(\n           prob_fav &lt; 0.005 ~ \"&lt;1%\",\n           prob_fav &lt; 0.995 ~ paste0(round(100 * prob_fav), \"%\"),\n           T ~ \"&gt;99%\"),\n         prob_fav_fontface = case_when(abs(prob_fav - 0.5) &gt; 0.45 ~ \"bold\", \n                                       T ~ \"plain\")) |&gt;\n  arrange(p1_hw, p2_hw) -&gt;\n  visualization_data\n\nvisualization_data |&gt;\n  select(p1_hw, p2_hw, formatted_ci, prob_fav) |&gt;\n  gt() |&gt;\n  cols_label(p1_hw = \"First Player\",\n             p2_hw = \"Second Player\",\n             formatted_ci = html(\"First Player Advantage&lt;br&gt;90% Credible Interval\"),\n             prob_fav = html(\"Probability of&lt;br&gt;First Player Advantage\")) |&gt;\n  tab_spanner(label = md(\"**Homeworld Sizes**\"), columns = 1:2) |&gt;\n  cols_align(align = \"center\", columns = everything()) |&gt;\n  tab_header(title = \"First Player Advantage in Homeworlds Openings\") |&gt;\n  tab_options(container.height = \"500px\") |&gt;\n  opt_row_striping()\n\n\n\n\n\n  \n    \n      First Player Advantage in Homeworlds Openings\n    \n    \n    \n      \n        Homeworld Sizes\n      \n      First Player Advantage90% Credible Interval\n      Probability ofFirst Player Advantage\n    \n    \n      First Player\n      Second Player\n    \n  \n  \n    SM\nSM\n60-69%\n0.999999680\n    SM\nSL\n45-52%\n0.248413661\n    SM\nML\n47-52%\n0.387849078\n    SM\nSS\n25-90%\n0.687849323\n    SM\nMM\n32-51%\n0.061530777\n    SM\nLL\n40-62%\n0.556708384\n    SL\nSM\n41-49%\n0.013054745\n    SL\nSL\n53-67%\n0.991093430\n    SL\nML\n46-54%\n0.533008043\n    SL\nSS\n34-87%\n0.773895167\n    SL\nMM\n21-61%\n0.212754245\n    SL\nLL\n33-63%\n0.423357565\n    ML\nSM\n44-50%\n0.066035425\n    ML\nSL\n50-58%\n0.935076177\n    ML\nML\n55-64%\n0.999647706\n    ML\nSS\n26-58%\n0.202200438\n    ML\nMM\n26-55%\n0.132786582\n    ML\nLL\n31-58%\n0.249135577\n    SS\nSM\n8-36%\n0.001698437\n    SS\nSL\n14-53%\n0.081930335\n    SS\nML\n8-47%\n0.032714844\n    SS\nSS\n29-81%\n0.635929076\n    SS\nMM\n17-83%\n0.500000000\n    SS\nLL\n8-66%\n0.187500000\n    MM\nSM\n52-73%\n0.973623861\n    MM\nSL\n56-87%\n0.986698151\n    MM\nML\n57-77%\n0.996318685\n    MM\nSS\n34-87%\n0.773895167\n    MM\nMM\n40-69%\n0.704614834\n    MM\nLL\n34-83%\n0.746093750\n    LL\nSM\n60-81%\n0.998935554\n    LL\nSL\n38-60%\n0.448433249\n    LL\nML\n54-71%\n0.991765439\n    LL\nSS\n48-95%\n0.937298892\n    LL\nMM\n61-96%\n0.989278600\n    LL\nLL\n41-58%\n0.462032892"
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#visualizing-first-player-advantage",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#visualizing-first-player-advantage",
    "title": "Homeworlds Openings",
    "section": "Visualizing First Player Advantage",
    "text": "Visualizing First Player Advantage\n\nCredible IntervalsPosterior Probability of First Player Advantage\n\n\n\nvisualization_data |&gt;\n  # underlying plot\n  ggplot() +\n  # aesthetic mapping\n  aes(x = p1_hw, y = p2_hw, fill = p1_wr) +\n  # visual elements representing the data\n  ## add tiles coloured to indicate the observed first player win rate\n  geom_tile(width = 0.8, height = 0.8) +\n  ## add dots to indicate universe size\n  geom_point(aes(colour = universe_size), \n             position = position_nudge(x = 0.3, y = 0.3), \n             size = 3) +\n  ## trick to circle the universe size dots without a fill scale collision\n  geom_point(size = 3, \n             shape = 21, \n             fill = NA, \n             colour = \"black\", \n             position = position_nudge(x = 0.3, y = 0.3)) +\n  # scales\n  ## gradient scale for the tiles\n  scale_fill_viridis(option = \"cividis\") +\n  ## discrete scale for the universe size dots\n  scale_colour_viridis(option = \"cividis\", discrete = TRUE) +\n  scale_y_discrete(limits = rev) +\n  # labels\n  labs(title = \"Homeworlds First Player Advantage by Opening\",\n       x = \"First Player's Homeworld Sizes\",\n       y = \"Second Player's Homeworld Sizes\",\n       fill = \"Observed First Player\\nWin Rate\",\n       colour = \"Universe Size\") +\n  # theming\n  theme_jdonland() +\n#  theme(panel.background = element_rect(colour = \"#fffcf9\")) +\n  ## put the win rate legend under the universe size legend\n  guides(colour = guide_legend(order = 1)) -&gt;\n  plot\n\n# version labelled with credible intervals\nplot +\n  # add formatted credible intervals as text\n  geom_label(aes(label = formatted_ci, fontface = ci_fontface),\n             fill = \"#fffcf9\",\n             label.padding = unit(0.125, \"lines\")) +\n  # add subtitle\n  labs(subtitle = \"90% Bayesian credible intervals with uniform priors\")\n\n\n\n\n\n\n\n\n\n\n\n# version labelled with posterior probability of first player advantage\nplot + \n  # add posterior probability first player is favoured as text\n  geom_label(aes(label = formatted_prob_fav, fontface = prob_fav_fontface),\n            fill = \"#fffcf9\",\n            label.padding = unit(0.125, \"lines\")) +\n  # add subtitle\n  labs(subtitle = \"Posterior probability first player is favoured\")"
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#observations",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#observations",
    "title": "Homeworlds Openings",
    "section": "Observations",
    "text": "Observations\nIt seems that the first player should probably choose a large-large or medium-medium homeworld configuration. A homeworld with equally-sized stars is known as a “Gemini” configuration. When the first player chooses a Gemini opening, the second player loses the option of creating a large universe; only a small or a microverse is possible.\nThe second player should opt for a small universe rather than a microverse. (A large universe is not possible when the first player’s homeworld contains stars of equal size.)\nWhy isn’t small-small also advantageous for the first player? This is likely due to creating a bank configuration which will provide better lines of play to the second player in the early turns of the game."
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#confounding-variables",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#confounding-variables",
    "title": "Homeworlds Openings",
    "section": "Confounding Variables",
    "text": "Confounding Variables\nOne might reasonably object to the model described above. For one thing, it does not take into account the colours of the pieces selected. This is a relatively minor issue, however, since a certain combination of colours5 is almost universally chosen.5 Blue and yellow stars with a green ship.\nMore concerning is the potentially confounding effect of player skill. It’s very possible that some openings may be effective only in the hands of experts. It would be interesting to revisit this analysis with more granular and detailed data that included Elo ratings. You can help with this by learning Homeworlds!\nYou can buy a Homeworlds set from Looney Labs. The same pieces can also be used to play many other interesting games."
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#conclusions",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#conclusions",
    "title": "Homeworlds Openings",
    "section": "Conclusions",
    "text": "Conclusions\nIf you’re playing first, start with two large stars of blue and yellow, and a large green ship. If you’re playing second and your opponent has followed my advice, try small-large or large-large in the same colours."
  },
  {
    "objectID": "posts/homeworlds_openings/homeworlds_openings.html#references",
    "href": "posts/homeworlds_openings/homeworlds_openings.html#references",
    "title": "Homeworlds Openings",
    "section": "References",
    "text": "References\n\nHomeworlds | Looney Labs\nPlay Homeworlds Online | BoardGameArena\nLooney Labs’ Online Store\nBabamots’ Homeworlds Site\nAndy’s Page About Homeworlds\nHomeworlds | Board Game | BoardGameGeek"
  },
  {
    "objectID": "posts/hour_records/hour_records.html",
    "href": "posts/hour_records/hour_records.html",
    "title": "Cycling Hour Records",
    "section": "",
    "text": "One of the most prestigious achievements in professional cycling is the hour record: travel farther on a bicycle in one hour than anyone has before."
  },
  {
    "objectID": "posts/hour_records/hour_records.html#introduction",
    "href": "posts/hour_records/hour_records.html#introduction",
    "title": "Cycling Hour Records",
    "section": "",
    "text": "One of the most prestigious achievements in professional cycling is the hour record: travel farther on a bicycle in one hour than anyone has before."
  },
  {
    "objectID": "posts/hour_records/hour_records.html#history",
    "href": "posts/hour_records/hour_records.html#history",
    "title": "Cycling Hour Records",
    "section": "History",
    "text": "History\nHenri Desgrange, the bicycle racer and sports journalist who organized the Tour de France in its earliest years, set the first official hour record at 35.325 km on the Vélodrome Buffalo in Paris in 1893. Two months later, a Mlle1 de Saint-Saveur set the women’s record at 26.012 km on the same track.1 Her first name seems to be lost to history.\nEight decades later, Eddy Merckx would break the men’s record by riding 49.431 km. Merckx was by that time undisputedly the greatest cylist of all time, but despite taking advantage of the lower aerodynamic drag offered by Mexico City’s high altitude, he described it as the hardest ride of his life. Maria Cressari broke the women’s record with a 41.471 km ride on the same track a month later.\nThe 1980s and 90s were a period of controversy for the hour record. A series of improvements on the records of Merckx and Cressari were achieved, but many attributed these to more aerodynamic equipment and riding postures. The battle over the hour record between the unconventional cyclist Graeme Obree and the UCI2 was dramatized in the 2006 film The Flying Scotsman. The ultimate result was a decision split the hour record into two categories: one requiring the use of equipment similar to what was available to Merckx and Cressari in the early 70s, and another allowing basically anything that was arguably a bicycle. This change was retroactive, so the official men’s hour record was reverted to Merckx’s.2 Professional cycling’s regulatory body.\nWhile Cressari’s record had been improved upon five times by the early 2000s, the men’s record was just barely broken3 by Chris Boardman and then by Ondřej Sosenka. The former had the advantage of shoes which locked into the pedals and the latter was probably doping at the time he the broke record. There followed a decade of disinterest in the hour record.3 By a margin of only 10 m!\nThe UCI revived the hour record in 2014 by changing the rules again. Going forward, the hour record would use the same equipment as Olympic track cycling races. The result was a burst of new hour record attempts including several successes. The current record holders are Filippo Ganna and Ellen van Dijk with 56.792 km and 49.254 km respectively."
  },
  {
    "objectID": "posts/hour_records/hour_records.html#data",
    "href": "posts/hour_records/hour_records.html#data",
    "title": "Cycling Hour Records",
    "section": "Data",
    "text": "Data\nWe can classify successful record attempts into four types: historical records set before Merckx’s, classic records set under the restrictive equipment rules, absolute records set under the permissive rules, and unified records set since the unification of the equipment rules with track racing.\n\nread_csv(\"hour_records.csv\", col_types = \"ccffn\") |&gt;\n  mutate(date = mdy(date)) -&gt;\n  hour_records\n\nhour_records |&gt;\n  kable(caption = \"Cycling Hour Records\") |&gt;\n  scroll_box(height = \"5in\")\n\n\n\nCycling Hour Records\n\n\ndate\nname\ngender\ntype\ndistance\n\n\n\n\n1893-05-11\nHenri Desgrange\nMen's\nHistorical\n35.325\n\n\n1894-10-31\nJules Dubois\nMen's\nHistorical\n38.220\n\n\n1897-07-30\nOscar Van den Eynde\nMen's\nHistorical\n39.240\n\n\n1898-07-03\nWillie Hamilton\nMen's\nHistorical\n40.781\n\n\n1905-08-24\nLucien Petit-Breton\nMen's\nHistorical\n41.110\n\n\n1907-06-20\nMarcel Berthet\nMen's\nHistorical\n41.520\n\n\n1912-08-22\nOscar Egg\nMen's\nHistorical\n42.360\n\n\n1913-08-07\nMarcel Berthet\nMen's\nHistorical\n42.741\n\n\n1913-08-21\nOscar Egg\nMen's\nHistorical\n43.525\n\n\n1913-09-20\nMarcel Berthet\nMen's\nHistorical\n43.775\n\n\n1914-08-18\nOscar Egg\nMen's\nHistorical\n44.247\n\n\n1933-08-25\nJan van Hout\nMen's\nHistorical\n44.588\n\n\n1933-09-28\nMaurice Richard\nMen's\nHistorical\n44.777\n\n\n1935-10-31\nGiuseppe Olmo\nMen's\nHistorical\n45.090\n\n\n1936-10-14\nMaurice Richard\nMen's\nHistorical\n45.325\n\n\n1937-09-29\nFrans Slaats\nMen's\nHistorical\n45.485\n\n\n1937-11-03\nMaurice Archambaud\nMen's\nHistorical\n45.767\n\n\n1942-11-07\nFausto Coppi\nMen's\nHistorical\n45.798\n\n\n1956-06-29\nJacques Anquetil\nMen's\nHistorical\n46.159\n\n\n1956-09-19\nErcole Baldini\nMen's\nHistorical\n46.394\n\n\n1957-09-18\nRoger Rivière\nMen's\nHistorical\n46.923\n\n\n1959-09-23\nRoger Rivière\nMen's\nHistorical\n47.347\n\n\n1967-10-30\nFerdi Bracke\nMen's\nHistorical\n48.093\n\n\n1968-10-10\nOle Ritter\nMen's\nHistorical\n48.653\n\n\n1972-10-25\nEddy Merckx\nMen's\nClassic\n49.431\n\n\n1984-01-09\nFrancesco Moser\nMen's\nAbsolute\n50.808\n\n\n1984-01-23\nFrancesco Moser\nMen's\nAbsolute\n51.151\n\n\n1993-07-17\nGraeme Obree\nMen's\nAbsolute\n51.596\n\n\n1993-07-23\nChris Boardman\nMen's\nAbsolute\n52.270\n\n\n1994-04-27\nGraeme Obree\nMen's\nAbsolute\n52.713\n\n\n1994-09-02\nMiguel Indurain\nMen's\nAbsolute\n53.040\n\n\n1994-10-22\nTony Rominger\nMen's\nAbsolute\n53.832\n\n\n1994-11-05\nTony Rominger\nMen's\nAbsolute\n55.291\n\n\n1996-09-06\nChris Boardman\nMen's\nAbsolute\n56.375\n\n\n2000-10-27\nChris Boardman\nMen's\nClassic\n49.441\n\n\n2005-07-19\nOndřej Sosenka\nMen's\nClassic\n49.700\n\n\n2014-09-01\nJens Voigt\nMen's\nUnified\n51.110\n\n\n2014-10-30\nMatthias Brändle\nMen's\nUnified\n51.852\n\n\n2015-02-08\nRohan Dennis\nMen's\nUnified\n52.491\n\n\n2015-05-02\nAlex Dowsett\nMen's\nUnified\n52.937\n\n\n2015-06-07\nBradley Wiggins\nMen's\nUnified\n54.526\n\n\n2019-04-16\nVictor Campenaerts\nMen's\nUnified\n55.089\n\n\n2022-08-19\nDaniel Bigham\nMen's\nUnified\n55.548\n\n\n2022-10-08\nFilippo Ganna\nMen's\nUnified\n56.792\n\n\n1893-07-01\nMlle de Saint-Saveur\nWomen's\nHistorical\n26.012\n\n\n1893-08-01\nRenée Debatz\nWomen's\nHistorical\n28.019\n\n\n1893-10-01\nHélène Dutrieu\nWomen's\nHistorical\n28.780\n\n\n1897-10-01\nLouise Roger\nWomen's\nHistorical\n34.684\n\n\n1911-01-01\nAlfonsina Strada\nWomen's\nHistorical\n37.192\n\n\n1947-10-01\nÉlyane Bonneau\nWomen's\nHistorical\n37.560\n\n\n1948-11-01\nJeannine Lemaire\nWomen's\nHistorical\n37.720\n\n\n1949-07-01\nJeannine Lemaire\nWomen's\nHistorical\n38.283\n\n\n1952-01-01\nJeannine Lemaire\nWomen's\nHistorical\n39.735\n\n\n1955-07-07\nTamara Novikova\nWomen's\nHistorical\n38.473\n\n\n1957-09-18\nRenee Vissac\nWomen's\nHistorical\n38.569\n\n\n1958-09-25\nMillie Robinson\nWomen's\nHistorical\n39.719\n\n\n1958-11-09\nElsy Jacobs\nWomen's\nHistorical\n41.347\n\n\n1972-11-25\nMaria Cressari\nWomen's\nClassic\n41.471\n\n\n1978-09-16\nKeetie van Oosten\nWomen's\nClassic\n43.082\n\n\n2000-10-18\nAnna Wilson-Millward\nWomen's\nClassic\n43.501\n\n\n2000-11-05\nJeannie Longo-Ciprelli\nWomen's\nClassic\n44.767\n\n\n2000-12-07\nJeannie Longo-Ciprelli\nWomen's\nClassic\n45.094\n\n\n2003-10-01\nLeontien Zijlaard-van Moorsel\nWomen's\nClassic\n46.065\n\n\n1986-09-20\nJeannie Longo-Ciprelli\nWomen's\nAbsolute\n44.770\n\n\n1987-09-23\nJeannie Longo-Ciprelli\nWomen's\nAbsolute\n44.933\n\n\n1989-10-01\nJeannie Longo-Ciprelli\nWomen's\nAbsolute\n46.352\n\n\n1995-04-29\nCatherine Marsal\nWomen's\nAbsolute\n47.112\n\n\n1995-06-17\nYvonne McGregor\nWomen's\nAbsolute\n47.411\n\n\n1996-10-26\nJeannie Longo-Ciprelli\nWomen's\nAbsolute\n48.159\n\n\n2015-09-12\nMolly Shaffer Van Houweling\nWomen's\nUnified\n46.273\n\n\n2016-01-22\nBridie O'Donnell\nWomen's\nUnified\n46.882\n\n\n2016-02-27\nEvelyn Stevens\nWomen's\nUnified\n47.980\n\n\n2018-09-13\nVittoria Bussi\nWomen's\nUnified\n48.007\n\n\n2021-09-30\nJoscelin Lowden\nWomen's\nUnified\n48.405\n\n\n2022-05-23\nEllen van Dijk\nWomen's\nUnified\n49.254"
  },
  {
    "objectID": "posts/hour_records/hour_records.html#visualization",
    "href": "posts/hour_records/hour_records.html#visualization",
    "title": "Cycling Hour Records",
    "section": "Visualization",
    "text": "Visualization\nExpand the code for the second plot to see a useful trick that allows one to modify the data set underlying a ggplot object, in this case to filter out the historical records.\n\n1893-present1972-present\n\n\n\n\nCode\nhour_records |&gt;\n  ggplot() +\n  # aesthetic mapping\n  aes(x = date, y = distance, colour = type, fill = type, label = name) +\n  # visual elements representing the data\n  geom_line(colour = \"#b4b4b4\") +\n  geom_point(size = 2) +\n  # scales\n  scale_y_continuous(breaks = seq(25,60,5), minor_breaks = seq(25, 60, 1)) +\n  scale_x_date(breaks = as.Date(paste0(seq(1890, 2030, by = 5), \"-01-01\")),\n               labels = date_format(\"%Y\"),\n               guide = guide_axis(n.dodge = 2),\n               limits = as.Date(c(\"1890-01-01\", \"2025-01-01\")),\n               expand = c(0, 0)) +\n  scale_colour_viridis(discrete = TRUE) +\n  scale_fill_viridis(discrete = TRUE) +\n  # facets\n  facet_wrap(facets = vars(gender), \n             scales = \"free_y\", \n             nrow = 2, \n             strip.position=\"right\") +\n  # labels\n  labs(title = \"Cycling Hour Records\", \n       y = \"Distance (km)\", \n       x = \"Date\") +\n  # theming\n  theme_bw() +\n  theme(legend.position = \"none\",\n        legend.title = element_blank(),\n        panel.grid.minor.x = element_blank()) -&gt;\n  p\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# change the data embedded in the previous plot\np$data |&gt;\n  filter(type != \"Historical\") -&gt;\n  p$data\n\n# re-plot it with text labels and new scales\np +\n  geom_text_repel(size = 2, colour = \"black\") +\n  scale_x_date(breaks = as.Date(paste0(seq(1970, 2030, by = 5), \"-01-01\")),\n               labels = date_format(\"%Y\"),\n               limits = as.Date(c(\"1970-01-01\", \"2025-01-01\")),\n               expand = c(0, 0)) +\n  scale_y_continuous() +\n  scale_colour_viridis(discrete = TRUE, begin = 0.33) +\n  labs(title = \"Cycling Hour Records, 1972-present\")"
  },
  {
    "objectID": "posts/hour_records/hour_records.html#observations",
    "href": "posts/hour_records/hour_records.html#observations",
    "title": "Cycling Hour Records",
    "section": "Observations",
    "text": "Observations\nThese charts highlight how little progress was made on the men’s record during the classic era. We can also see that the unified records have overtaken the old “anything goes” absolute records. Finally, it’s interesting to note that the current women’s record is just slightly shy of Merckx’s."
  },
  {
    "objectID": "posts/hour_records/hour_records.html#references",
    "href": "posts/hour_records/hour_records.html#references",
    "title": "Cycling Hour Records",
    "section": "References",
    "text": "References\n\nHour record (Wikipedia)\nEddy Merckx and the Hour Record\nThe Retrogrouch: The Hour Record"
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html",
    "title": "(Machine) Learning about Runes",
    "section": "",
    "text": "Today we’ll learn about an ancient Germanic writing system, clean up some messy data, engineer a couple of simple features, and see if we can find a way to predict the age of some historical artifacts without having to pay for a fancy-schmancy archaeology degree. Along the way, we’ll get a little overview of the tidyverse and tidymodels approaches to data analysis and modeling in R.\n\n\nA rune is a letter from any of a handful of closely-related alphabets used by Germanic peoples primarily from around the 3rd to the 13th centuries CE. Like the Latin alphabet used to write the Germanic languages today, they derive from an ancient form of the Greek alphabet.1 These alphabets are sometimes called “futhark” (or “futhorc”, “fuþąrk”, etc.) after first few letters in their canonical order.21 Possibly via one of the alpine Old Italic alphabets such as Venetic.2 Curiously, the first three letters of these alphabets spell out a vulgar word for a woman’s genitals in Old Norse.\nThe main runic alphabets are3:3 The abecedaria, or “rune rows”, in this table are much simplified. In reality, there were many variations of each. The medieval runes in particular were augmented by diacritical dots in order to disambiguate sounds represented by the same rune in the younger futhark.\n\n\n\n\n\n\n\n\n\nName\nMain Language(s)\nEra (c. CE)\nLetters\n\n\n\n\nElder Futhark\nProto-Germanic, Proto-Norse\n1st–8th\nᚠ ᚢ ᚦ ᚨ ᚱ ᚲ ᚷ ᚹ ᚺ ᚾ ᛁ ᛃ ᛈ ᛇ ᛉ ᛊ ᛏ ᛒ ᛖ ᛗ ᛚ\n\n\nAnglo-Frisian Futhorc\nOld Frisian, Old English\n5th–11th\nᚠ ᚢ ᚦ ᚩ ᚱ ᚳ ᚷ ᚹ ᚻ ᚾ ᛁ ᛡ ᛇ ᛈ ᛉ ᛋ ᛏ ᛒ ᛖ ᛗ ᛚ ᛝ ᛟ ᛞ ᚪ ᚫ ᛠ ᚣ\n\n\nYounger Futhark\nOld Norse\n8th–12th\nᚠ ᚢ ᚦ ᚬ ᚱ ᚴ ᚼ ᚾ ᛁ ᛅ ᛦ ᛋ ᛏ ᛒ ᛘ ᛚ\n\n\nMedieval Runes\nOld Icelandic, Old Swedish\n12th–17th\nᚠ ᚢ ᚦ ᚮ ᚱ ᚴ ᚼ ᚿ ᛁ ᛆ ᛌ ᛐ ᛒ ᛘ ᛚ ᛦ\n\n\n\nMagical incantations were sometimes written in runes, but despite what such august institutions as the gift shop of the National Museum of Iceland would have you believe, there’s no evidence that each runic letter had a specific symbolic meaning.4 Neither is there much evidence that they were used in divination. These alphabets were used mainly for mundane purposes such as inscribing an object with its owner or creator’s name, and for memorializing the dead on gravestones. Nevertheless, in recent times these alphabets have become an element in various kinds of New Age mysticism, neo-pagan religions, and unsavoury political movements.4 The runes have mnemonic names, and occasionally a rune could stand in for its name, but even this is rare.\n\n\n\nRuneS-DB is a database of artifacts bearing runic writing compiled from a variety of sources as part of RuneS, an overarching project to study these writing systems. It’s available from the RuneS project’s website.\n\nThe rights to the data are held by the Göttingen Academy of Sciences and Humanities and are subject to the CC-BY- SA law. The RuneS research units Göttingen, Eichstätt-München and Kiel were involved in the generation of the data. RuneS-DB contains data from the Kiel Runenprojekt, the Danish database runer.ku.dk and the Samnordisk runtextdatabas/Rundata accessible under the Open Database License (ODbL). Please also note the additional information on other origin of the data provided under the label sources.\n\nRuneS provides a tool on their website to query the database, but it’s pretty clunky. Instead, we’ll use R to see if we can learn anything interesting.\n\n\n\nWe’ll use the tidyverse collection of packages for manipulating and visualizing our data and the tidymodels packages to quickly define, train, and compare a few machine learning models. These packages “share an underlying design philosophy, grammar, and data structures” rooted in the “tidy data” principles originally espoused by Hadley Wickham.\nThis document was lovingly hand-crafted in artisanal, farm-to-table, GMO-free Quarto."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#introduction",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#introduction",
    "title": "(Machine) Learning about Runes",
    "section": "",
    "text": "Today we’ll learn about an ancient Germanic writing system, clean up some messy data, engineer a couple of simple features, and see if we can find a way to predict the age of some historical artifacts without having to pay for a fancy-schmancy archaeology degree. Along the way, we’ll get a little overview of the tidyverse and tidymodels approaches to data analysis and modeling in R.\n\n\nA rune is a letter from any of a handful of closely-related alphabets used by Germanic peoples primarily from around the 3rd to the 13th centuries CE. Like the Latin alphabet used to write the Germanic languages today, they derive from an ancient form of the Greek alphabet.1 These alphabets are sometimes called “futhark” (or “futhorc”, “fuþąrk”, etc.) after first few letters in their canonical order.21 Possibly via one of the alpine Old Italic alphabets such as Venetic.2 Curiously, the first three letters of these alphabets spell out a vulgar word for a woman’s genitals in Old Norse.\nThe main runic alphabets are3:3 The abecedaria, or “rune rows”, in this table are much simplified. In reality, there were many variations of each. The medieval runes in particular were augmented by diacritical dots in order to disambiguate sounds represented by the same rune in the younger futhark.\n\n\n\n\n\n\n\n\n\nName\nMain Language(s)\nEra (c. CE)\nLetters\n\n\n\n\nElder Futhark\nProto-Germanic, Proto-Norse\n1st–8th\nᚠ ᚢ ᚦ ᚨ ᚱ ᚲ ᚷ ᚹ ᚺ ᚾ ᛁ ᛃ ᛈ ᛇ ᛉ ᛊ ᛏ ᛒ ᛖ ᛗ ᛚ\n\n\nAnglo-Frisian Futhorc\nOld Frisian, Old English\n5th–11th\nᚠ ᚢ ᚦ ᚩ ᚱ ᚳ ᚷ ᚹ ᚻ ᚾ ᛁ ᛡ ᛇ ᛈ ᛉ ᛋ ᛏ ᛒ ᛖ ᛗ ᛚ ᛝ ᛟ ᛞ ᚪ ᚫ ᛠ ᚣ\n\n\nYounger Futhark\nOld Norse\n8th–12th\nᚠ ᚢ ᚦ ᚬ ᚱ ᚴ ᚼ ᚾ ᛁ ᛅ ᛦ ᛋ ᛏ ᛒ ᛘ ᛚ\n\n\nMedieval Runes\nOld Icelandic, Old Swedish\n12th–17th\nᚠ ᚢ ᚦ ᚮ ᚱ ᚴ ᚼ ᚿ ᛁ ᛆ ᛌ ᛐ ᛒ ᛘ ᛚ ᛦ\n\n\n\nMagical incantations were sometimes written in runes, but despite what such august institutions as the gift shop of the National Museum of Iceland would have you believe, there’s no evidence that each runic letter had a specific symbolic meaning.4 Neither is there much evidence that they were used in divination. These alphabets were used mainly for mundane purposes such as inscribing an object with its owner or creator’s name, and for memorializing the dead on gravestones. Nevertheless, in recent times these alphabets have become an element in various kinds of New Age mysticism, neo-pagan religions, and unsavoury political movements.4 The runes have mnemonic names, and occasionally a rune could stand in for its name, but even this is rare.\n\n\n\nRuneS-DB is a database of artifacts bearing runic writing compiled from a variety of sources as part of RuneS, an overarching project to study these writing systems. It’s available from the RuneS project’s website.\n\nThe rights to the data are held by the Göttingen Academy of Sciences and Humanities and are subject to the CC-BY- SA law. The RuneS research units Göttingen, Eichstätt-München and Kiel were involved in the generation of the data. RuneS-DB contains data from the Kiel Runenprojekt, the Danish database runer.ku.dk and the Samnordisk runtextdatabas/Rundata accessible under the Open Database License (ODbL). Please also note the additional information on other origin of the data provided under the label sources.\n\nRuneS provides a tool on their website to query the database, but it’s pretty clunky. Instead, we’ll use R to see if we can learn anything interesting.\n\n\n\nWe’ll use the tidyverse collection of packages for manipulating and visualizing our data and the tidymodels packages to quickly define, train, and compare a few machine learning models. These packages “share an underlying design philosophy, grammar, and data structures” rooted in the “tidy data” principles originally espoused by Hadley Wickham.\nThis document was lovingly hand-crafted in artisanal, farm-to-table, GMO-free Quarto."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#reading-and-cleaning-the-data",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#reading-and-cleaning-the-data",
    "title": "(Machine) Learning about Runes",
    "section": "Reading and Cleaning the Data",
    "text": "Reading and Cleaning the Data\n\nReading the File\nThe file we obtain from the RuneS website is tab-separated, contains three lines of preamble, and uses a hyphen to indicate missing data. Providing this information to readr’s read_delim will produce a tibble (i.e., a fancy data frame, which is in turn a fancy matrix) that we can begin to manipulate.\n\n# read the file\nread_delim(file = \"runes_data.csv\",\n           delim = \";\",\n           skip = 3,\n           na = \"-\",\n           show_col_types = FALSE) -&gt;\n  # save to a dataframe\n  runes_data\n\n\n\nThe Vimose Comb\nOne artifact we’d expect to find in RuneS-DB is the Vimose comb, which bears what’s considered to be the oldest known datable5 runic inscription. dplyr’s filter function and stringr’s str_detect let us filter the data to show only rows where value in the column of inscription names matches “Vimose comb”.65 Insert joke about “datable” here.6 Just checking for rs_short_inscr_name == \"Vimose comb\" would fail because the values in that column have extra whitespace characters.\n\nrunes_data |&gt;\n  filter(str_detect(rs_short_inscr_name, pattern = \"Vimose comb\")) |&gt;\n  kbl() |&gt;\n  scroll_box(width = \"51.75%\")\n\n\n\n\n\nFindno\nrs_short_inscr_name\nrs_fundort\nrs_short_storage\nrs_extdat\nrs_short_dat_art\nrs_short_context\nrs_objklasse\nrs_objtyp\nrs_short_obj_complete\nrs_short_matklasse\nrs_mat\nrs_short_obj_state\nrs_runenreihe\nrs_short_museum\nrs_short_ins_complete\nrs_translat\nrs_translit\nrs_short_ins_state\nrs_short_markings\nrs_short_transkript\nrs_namen\nrs_fundjahr\nrs_short_gemeinde\nrs_short_bezirk\nrs_short_landschaft\nrs_short_land\nrs_short_invnr\nrs_short_kategorie\nrs_short_sigils\nrs_traeger\n\n\n\n\n20\nVimose comb\nVimose\nMuseum\n140-160\narch.\ndeposit find\ntool/implement\npersonal hygiene\nyes\nbone/horn\nantler\ngood\nolder fuþark\nNationalmuseet\nyes\nHarja.\nharja |\ngood\nnein\nNA\nVimose-kam\n1865\nAllese Sogn\nOdense Amt\nFyn\nDK\n22657\nrun.\nFyn 19\ncomb\n\n\n\n\n\n\n\nSo we can see that this object was found in Vimose, currently resides in a museum, has been dated to 140-160 CE, is a personal hygiene tool made of antler, says “harja” (ᚺᚨᚱᛃᚨ), etc.\n\n\nDiagnosing\nOne way to quantify the messiness of a data set is to calculate the count and proportion of missing and unique values in each column. dlookr’s diagnose does this.\n\nrunes_data |&gt; \n  diagnose() |&gt;\n  kbl() |&gt;\n  scroll_box(width = \"51.75%\")\n\n\n\n\n\nvariables\ntypes\nmissing_count\nmissing_percent\nunique_count\nunique_rate\n\n\n\n\nFindno\nnumeric\n0\n0.0000000\n8281\n1.0000000\n\n\nrs_short_inscr_name\ncharacter\n7\n0.0845309\n5457\n0.6589784\n\n\nrs_fundort\ncharacter\n34\n0.4105784\n3156\n0.3811134\n\n\nrs_short_storage\ncharacter\n0\n0.0000000\n1723\n0.2080667\n\n\nrs_extdat\ncharacter\n6\n0.0724550\n573\n0.0691945\n\n\nrs_short_dat_art\ncharacter\n1988\n24.0067625\n19\n0.0022944\n\n\nrs_short_context\ncharacter\n5240\n63.2773820\n21\n0.0025359\n\n\nrs_objklasse\ncharacter\n33\n0.3985026\n13\n0.0015699\n\n\nrs_objtyp\ncharacter\n1452\n17.5341142\n45\n0.0054341\n\n\nrs_short_obj_complete\ncharacter\n1873\n22.6180413\n4\n0.0004830\n\n\nrs_short_matklasse\ncharacter\n43\n0.5192610\n11\n0.0013283\n\n\nrs_mat\ncharacter\n3550\n42.8692187\n119\n0.0143702\n\n\nrs_short_obj_state\ncharacter\n1970\n23.7893974\n6\n0.0007246\n\n\nrs_runenreihe\ncharacter\n463\n5.5911122\n12\n0.0014491\n\n\nrs_short_museum\ncharacter\n4382\n52.9163145\n291\n0.0351407\n\n\nrs_short_ins_complete\ncharacter\n1259\n15.2034778\n4\n0.0004830\n\n\nrs_translat\ncharacter\n698\n8.4289337\n5592\n0.6752808\n\n\nrs_translit\ncharacter\n91\n1.0989011\n7482\n0.9035141\n\n\nrs_short_ins_state\ncharacter\n1371\n16.5559715\n6\n0.0007246\n\n\nrs_short_markings\ncharacter\n2676\n32.3149378\n3\n0.0003623\n\n\nrs_short_transkript\ncharacter\n1090\n13.1626615\n5536\n0.6685183\n\n\nrs_namen\ncharacter\n5662\n68.3733849\n1554\n0.1876585\n\n\nrs_fundjahr\ncharacter\n4895\n59.1112185\n700\n0.0845309\n\n\nrs_short_gemeinde\ncharacter\n1435\n17.3288250\n1682\n0.2031156\n\n\nrs_short_bezirk\ncharacter\n3287\n39.6932738\n406\n0.0490279\n\n\nrs_short_landschaft\ncharacter\n1384\n16.7129574\n124\n0.0149740\n\n\nrs_short_land\ncharacter\n48\n0.5796401\n28\n0.0033812\n\n\nrs_short_invnr\ncharacter\n4709\n56.8651129\n2524\n0.3047941\n\n\nrs_short_kategorie\ncharacter\n1227\n14.8170511\n15\n0.0018114\n\n\nrs_short_sigils\ncharacter\n125\n1.5094795\n8129\n0.9816447\n\n\nrs_traeger\ncharacter\n8\n0.0966067\n416\n0.0502355\n\n\n\n\n\n\n\nUnfortunately, RuneS-DB is a bit of a mess. Because it’s amalgamated from many sources, information is coded inconsistently. A mix of languages are used, including in the column names. Getting this data set clean enough to start visualizing it is easy enough using dplyr functions like rename(), separate(), and mutate().\nThe first step is to make the column names more informative and get rid of the weird “rs_” and “rs_short_” prefixes.\n\nrunes_data |&gt;\n  # rename the columns\n  rename(\n    find_number          = Findno,\n    inscription_name     = rs_short_inscr_name,\n    location             = rs_short_storage,\n    date                 = rs_extdat,\n    dating_method        = rs_short_dat_art,\n    context              = rs_short_context,\n    findspot             = rs_fundort,\n    object_class         = rs_objklasse,\n    object_type          = rs_objtyp,\n    object_complete      = rs_short_obj_complete,\n    material_class       = rs_short_matklasse,\n    material             = rs_mat,\n    object_state         = rs_short_obj_state,\n    writing_system       = rs_runenreihe,\n    museum               = rs_short_museum,\n    inscription_complete = rs_short_ins_complete,\n    translation          = rs_translat,\n    transliteration      = rs_translit,\n    inscription_state    = rs_short_ins_state,\n    markings             = rs_short_markings,\n    transcription        = rs_short_transkript,\n    names                = rs_namen,\n    find_year            = rs_fundjahr,\n    community            = rs_short_gemeinde,\n    district             = rs_short_bezirk,\n    region               = rs_short_landschaft,\n    country              = rs_short_land,\n    inventory_number     = rs_short_invnr,\n    category             = rs_short_kategorie,\n    shelf_marks          = rs_short_sigils,\n    carrier              = rs_traeger\n  ) -&gt;\n  # update the dataframe\n  runes_data\n\nNext we’ll separate the location column into two columns, one with the general category and the other with the rest of the information. We’ll also create separate columns for the lower and upper bounds of each date, then convert this into a midpoint and a range.\n\nrunes_data |&gt;\n  # separate location category and detail\n  separate(\n    location,\n    into = c(\"location_class\", \"location_detail\"),\n    sep = \":\",\n    extra = \"merge\") |&gt;\n  # separate the date column into lower and upper bounds\n  separate(date, into = c(\"date_lower\", \"date_upper\")) |&gt;\n  mutate(\n    # set dates to NA for undated objects\n    across(starts_with(\"date\"), ~ na_if(., \"0\")),\n    # treat the dates as numbers\n    across(starts_with(\"date\"), ~ as.numeric(.)),\n    # get the middle of each date range\n    date = (date_lower + date_upper) / 2,\n    # get the range of each date\n    date_range = date_upper - date_lower) |&gt;\n  # discard the date bounds\n  select(-c(date_upper, date_lower)) -&gt;\n  # update the dataframe again\n  runes_data\n\nNext let’s convert RuneS-DB’s somewhat idiosyncratic country codes into human-readable names. While we’re at it, we’ll collapse the rarest ones into a “Rest of World” category.\n\nrunes_data |&gt;\n  mutate(\n    country = recode(\n      country,\n      \"S\"  = \"Sweden\",\n      \"N\"  = \"Norway\",\n      \"DK\" = \"Denmark\",\n      \"IS\" = \"Iceland\",\n      \"GB\" = \"Great Britain\",\n      \"D\"  = \"Germany\",\n      \"KN\" = \"Greenland\",\n      .default = \"Rest of World\")) -&gt;\n  runes_data\n\nFinally some miscellaneous re-encoding.\n\nrunes_data |&gt;\n  mutate(\n    # treat the ID numbers as strings\n    find_number = as.character(find_number),\n    # eliminate excess whitespace\n    across(where(is.character), str_squish),\n    # translate the markings column into English\n    markings = recode(markings, \"ja\" = \"yes\", \"nein\" = \"no\"),\n    # replace cross symbol\n    across(where(is.character), ~ recode(., \"†\" = \"lost/destroyed\")),\n    # replace \"rune stick/rúnakefli\", shorten \"weapon/weapon accessories\"\n    object_class = recode(\n      iconv(object_class, to = 'ASCII//TRANSLIT'),\n      \"rune stick/runakefli\" = \"rune stick\",\n      \"weapon/weapon accessories\" = \"weapon/accessory\"),\n    # make \"Museum\" lowercase for consistency\n    location_class = recode(location_class, \"Museum\" = \"museum\"),\n    # combine parchment and paper into a single material class\n    material_class = recode(\n      material_class,\n      \"parchment; paper\" = \"parchment/paper\",\n      \"parchment\" = \"parchment/paper\",\n      \"paper\" = \"parchment/paper\"),\n    # give the writing systems friendlier names, combine medieval and post-Reformation runes as \"manuscript runes\"\n    writing_system = recode(\n      iconv(writing_system, to = 'ASCII//TRANSLIT'),\n      \"older fu?ark\" = \"elder futhark\",\n      \"younger fu?ark\" = \"younger futhark\",\n      \"fu?orc\" = \"anglo-frisian futhorc\",\n      \"post-Reformation runes\" = \"manuscript runes\",\n      \"medieval runes\" = \"manuscript runes\",\n      .default = \"mixed/unknown\")) -&gt;\n  runes_data\n\nThere’s more cleaning we could do, but this is enough for now."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#exploratory-data-analysis",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#exploratory-data-analysis",
    "title": "(Machine) Learning about Runes",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nrunes_data |&gt;\n  drop_na(object_class, material_class) |&gt;\n  group_by(object_class, material_class) |&gt;\n  summarise(count = n()) |&gt;\n  ggplot(aes(x = object_class, y = material_class, size = count)) +\n  geom_point(shape = 21, fill = \"#595959\") +\n  coord_flip() +\n  scale_size(range = c(0, 10)) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Runic Artifacts by Object and Material Class\",\n    x = \"Object Class\",\n    y = \"Material Class\")\n\n\n\n\n\n\n\n\nNearly half of the artifacts in RuneS-DB are stones7, which it turns out are always made of stone! Likewise, coins and bracteates8 are always made of metal, and rune sticks are always made of wood. Tools, edifices, “inventory” (mostly furniture), and “other objects” all seem to come in a variety of materials.7 Good old rock, nothin’ beats that!8 What’s a bracteate, you ask? Why, it’s “is”a flat, thin, single-sided gold medal worn as jewelry”.\nWhere is this stuff typically found?\n\nrunes_data |&gt;\n  # discard artifacts missing country information\n  drop_na(country) |&gt;\n  ggplot(aes(fct_infreq(country))) +\n  geom_bar() +\n  geom_hline(yintercept = seq(0, 4000, 1000), color = \"white\") +\n  coord_flip() +\n  labs(\n    title = \"Runic Objects by Country\",\n    x = \"Country of Discovery\",\n    y = \"Number of Objects\")\n\n\n\n\n\n\n\n\nRight. Scandinavia, mostly.\nOne last exploratory graphic. How old is this stuff?\n\nrunes_data |&gt;\n  # discard undated objects\n  drop_na(date) |&gt;\n  ggplot(aes(date)) +\n  # bin into centuries\n  geom_histogram(\n    fill = \"#595959\", \n    color = \"white\", \n    breaks = seq(1, 2022, by=100)) +\n  geom_hline(yintercept = seq(0, 3000, 1000), color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 2000, by = 100)) +\n  labs(\n    title = \"Number of Runic Objects by Century\",\n    x = \"Year of Manufacture\",\n    y = \"Number of Objects\")\n\n\n\n\n\n\n\n\nThis histogram has a huge spike at the 10th century. Upon cursory investigation, this seems to be because the data set contains many rune stones with dated to the rather wide range of “725-1100”, i.e. the Viking Age. Obviously this data set is not random sample of all runic objects ever produced; some things are more likely to survive and be cataloged than others. The smaller spike at the 13th century seems to be more organic."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#feature-engineering",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#feature-engineering",
    "title": "(Machine) Learning about Runes",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nSuppose you’re lost in the woods in northwestern Europe9 and stumble upon some runes. Is there some rule you can use to estimate when they were carved (or written)?9 What do you do if you’re lost in the woods in Iceland? Stand up.\nFirst, we need to consider what features of the object you’d be able to determine. I think these are all reasonable:\n\nobject class (stone, coin, etc.)\nmaterial class (stone, metal, wood, etc.)\ncountry\nwriting system (elder futhark, anglo-frisian futhorc, etc.)\nlength of the inscription\nwhether the inscription contains an abecedarium; something like “fuþarkgw…” or “abcdefgh…”\nwhether the inscription seems to contain any of a few common words or morphemes\n\nThe first four of these features are already present in our data frame. We’ll have to “engineer” the others.\nUnfortunately, the transliterations in RuneS-DB are very inconsistently encoded, but we can still get an approximate length of each inscription by converting the transliteration to ASCII10 and counting the number of resulting alphanumeric characters11. It’s too bad the database doesn’t simply use the runic characters included in Unicode, but perhaps that wouldn’t be sufficient to encode parts of the inscriptions which are unclear, damaged, combined into ligatures (so-called “bind runes”), etc.10 Note that different operating systems have different ideas about how to convert other encodings into ASCII.11 Plus question marks, to account for some of the non-ASCII letters.\n\nrunes_data |&gt;\n  mutate(\n    inscription_length =\n      transliteration |&gt;\n      # convert to ASCII\n      iconv(to = \"ASCII//TRANSLIT\") |&gt;\n      # count alphanumeric characters\n      str_count(\"[[:lower:]\\\\?]\")) -&gt;\n  runes_data\n\nWe can identify abecedaria by slightly re-encoding the “category” column.\n\nrunes_data |&gt;\n  mutate(\n    abecedarium = case_when(\n      str_detect(category, \"alphabet\") ~ \"abc\",\n      str_detect(category, \"row\") ~ \"futh\",\n      TRUE ~ \"no\")) -&gt;\n  runes_data\n\nAnother feature we can derive from the inscription is what kind of spacing is used between words.\n\nrunes_data |&gt;\n  mutate(\n    spaces = case_when(\n      str_detect(transliteration, \" [÷\\\\*\\\\+] \") ~ \"crosses/stars\",\n      str_detect(transliteration, \" × \") ~ \"crosses/stars\",\n      str_detect(transliteration, \" [ˈ\\\\.] \") ~ \"single dots\",\n      str_detect(transliteration, \" : \") ~ \"double dots\",\n      TRUE ~ \"none/other\")) -&gt;\n  runes_data\n\nFinally let’s encode the presence or absence of a few of the most common words/morphemes.\n\nrunes_data |&gt;\n  mutate(\n    sin_sun_syn = str_detect(transliteration, \"sin|sun|syn\"),\n    auk_uk_ok   = str_detect(transliteration, \"auk|uk|ok\"),\n    at          = str_detect(transliteration, \"at\"),\n#   fathur      = str_detect(transliteration, \"faþur\"),\n    stain_stin  = str_detect(transliteration, \"stain|stin|stӕin\"),\n    lit         = str_detect(transliteration, \"lit\"),\n    across(sin_sun_syn:lit, as.numeric)) -&gt;\n  runes_data\n\nIs this enough information to be able to predict the age of a runic inscription with any accuracy? Let’s try fitting a few different models using different approaches."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#model-fitting",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#model-fitting",
    "title": "(Machine) Learning about Runes",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nTest/Training Data Split\nFirst we’ll split the data into a training and a test set, then create cross-validation folds from training data to help estimate model performance.\n\nrunes_data |&gt;\n  # keep just the features we want to predict from\n  select(\n    object_class, \n    material_class, \n    country, \n    writing_system, \n    inscription_length,\n    abecedarium,\n    spaces,\n    sin_sun_syn:lit,\n    date) |&gt;\n  # discard objects with missing data\n  na.omit() -&gt;\n  runes_data\n\n# split into training (75%) and test (25%) sets, stratified by date\ninitial_split(\n    data = runes_data, \n    prop = 0.75, \n    strata = date) -&gt;\n  split\n\n# store a copy of each set\ntraining(split) -&gt; train\ntesting(split) -&gt; test\n\n# create 10 cross-validation folds\nvfold_cv(train) -&gt; folds\n\nThe tidymodels framework provides a unified interface to various model-specific packages, as well as convenient functions for defining, fitting, tuning, and comparing many combinations of data pre-processing recipes and model specifications.\n\nIn parsnip, the model type differentiates basic modeling approaches, such as random forests, logistic regression, linear support vector machines, etc.; the mode denotes in what kind of modeling context it will be used (most commonly, classification or regression); and the computational engine indicates how the model is fit, such as with a specific R package implementation or even methods outside of R like Keras or Stan.\n\n\n\nPre-Processing Recipe\nNext we’ll define two pre-processing recipes. In both cases we’ll normalize our numeric predictor. Some model types require categorical predictors to be dummy-encoded, while others can exhibit better performance with categorical predictors left as-is. We’ll try both ways.\n\n# create a pre-processing recipe\nrecipe(runes_data) |&gt;\n  update_role(date, new_role = \"outcome\") |&gt;\n  update_role(1:(ncol(runes_data) - 1), new_role = \"predictor\") |&gt;\n  # normalize the numeric feature\n  step_normalize(all_numeric_predictors()) |&gt;\n  # dummy encode the categorical features\n  step_dummy(all_nominal_predictors()) -&gt;\n  runes_recipe\n\n\n\nBaseline\nNow we can define our models. First let’s “fit” the null model, which consists of just always guessing the mean date value from the training set. It’s straightforward to simply calculate the appropriate RMSE estimate in this case, but for illustrative purposes we’ll use cross-validation anyway. It doesn’t matter which recipe we use since the null model ignores the predictors anyway.\n\nnull_model() |&gt;\n  set_engine(\"parsnip\") |&gt;\n  set_mode(\"regression\") -&gt;\n  null_spec\n\nworkflow() |&gt;\n  add_model(null_spec) |&gt;\n  add_recipe(runes_recipe) |&gt;\n  fit_resamples(resamples = folds, metrics = metric_set(rmse)) |&gt;\n  collect_metrics() |&gt;\n  pull(mean)\n\n[1] 311.4979\n\n\nThe null model’s prediction is, in a certain sense, off by more than three centuries on average. Surely we can do better than that.\n\n\nThe Bias-Variance Trade-off\n\n\nModel Specification\nWe’ll try six more kinds of model. Each comes with some hyperparameters which control the bias-variance tradeoff, the step size for gradient descent, etc. We can leave tune() as a placeholder for these values when creating the model specifications. When we fit the models, we’ll try 10 combinations of hyperparameter values for each model type, and keep only the best ones.\nA linear model assumes the outcome is a linear function of the predictors and finds the best coefficient to assign to each. Linear models are inflexible, and so tend to suffer from bias (unless the underlying relationship really is approximately linear), but they tend to have lower variance than more flexible model types, since their outputs are not too sensitive to small changes in their inputs. The penalty and mixture hyperparameters here control how much L1 (LASSO) and/or L2 (ridge) regularization to apply. Regularization penalizes more complex models in order to prevent overfitting.\n\nlinear_reg(\n  engine  = \"glmnet\",\n  penalty = tune(),\n  mixture = tune()) -&gt;\n  linear_spec\n\nA decision tree is essentially a flowchart, with each split corresponding to a rule in the form of an “if/then” condition on a predictor. The idea is to find the splits which best separate the outcome. Predictions are produced by taking the average outcome among the training data belonging to the relevant terminal node of the tree. tree_depth specifies the maximum depth of the tree; without some maximum, the training data could be completely interpolated (or “memorized”), an extreme form of overfitting. cost_complexity controls how well a split must separate its subset of the training data in order to be considered, and min_n controls how much training data must belong to a node in order to justify any further splitting.\n\ndecision_tree(engine     = \"partykit\",\n              tree_depth = tune(),\n              min_n      = tune()) -&gt;\n  decision_tree_spec\n\nNearest neighbours models predict that the value of the outcome for a test data point will be some kind of weighted average of that point’s nearest neighbours in the training data. neighbors controls the number of neighbours to consider and the other hyperparameters specify the precise notions of “weighted average” and “nearest” to use.\n\nnearest_neighbor(engine      = \"kknn\",\n                 neighbors   = tune(),\n                 weight_func = tune(),\n                 dist_power  = tune()) -&gt;\n  nearest_neighbours_spec\n\nBoosted tree ensembles are a very popular machine learning approach involving fitting many small decision trees, each of which is optimized to improve the predictions obtained by combining the preceding trees. This model type inherents the hyperparameters involved in fitting decision trees, plus additional hyperparameters specifying the number of trees to use, the proportion of training data and number of predictors to consider at each step during fitting, as well as how much weight to initially apply to each new tree.\n\nboost_tree(engine         = \"xgboost\",\n           trees          = 1000,\n           tree_depth     = tune(),\n           min_n          = tune(),\n           loss_reduction = tune(),\n           sample_size    = tune(),\n           mtry           = tune(),\n           learn_rate     = tune()) -&gt;\n  boosted_trees_spec\n\nSome approaches combine multiple other types of models. Cubist involves a tree ensemble with linear models fit on each tree node, a boosting-like procedure, and a final nearest-neighbours-based adjustment.\n\ncubist_rules(engine = \"Cubist\",\n             committees = tune(),\n             neighbors = tune()) -&gt;\n  cubist_spec\n\n\n\nModel Fitting\nIn tidymodels, a “workflow” is an object which bundles together a model specification together with any associated pre-processing recipes, hyperparameter values, and/or evaluation results. workflow_set and workflow_map allow us to tune all of our model specifications as a batch.\n\n# combine the model specifications in a list\nlist(linear        = linear_spec,\n     tree          = decision_tree_spec,\n     nn            = nearest_neighbours_spec,\n     boosted_trees = boosted_trees_spec,\n     cubist        = cubist_spec) -&gt;\n  runes_model_specs\n\n# set the prediction mode of each engine to \"regression\"\nrunes_model_specs |&gt;\n  map(~set_mode(., \"regression\")) -&gt;\n  runes_model_specs\n\n# combine pre-processing recipe and model specifications into a workflow set\nworkflow_set(preproc = list(recipe = runes_recipe),\n             models = runes_model_specs) -&gt;\n  runes_workflow_set\n\nrunes_workflow_set |&gt;\n  # for each model specification, try ten combinations of tuning parameters\n  # and estimate rmse using cross-validation\n  workflow_map(\"tune_grid\",\n               resamples = folds,\n               grid = 10,\n               metrics = metric_set(rmse)) -&gt;\n  # update the workflow set with the results\n  runes_workflow_set\n\n\n\nPerformance Comparison\nNow our workflow set contains ten fit models per model type for each of ten hyperparameter combinations. We can extract the best version of each model type and plot the cross-validated performance estimates.\n\n# plot the rmse estimate from the best iteration of each type of model\nrank_results(runes_workflow_set, select_best = TRUE) |&gt;\n  group_by(model) |&gt;\n  slice_max(mean) |&gt;\n  select(mean, std_err, model) |&gt;\n  rename(rmse_mean = mean, rmse_std_err = std_err) |&gt;\n  ggplot(aes(x = fct_reorder(model, -desc(rmse_mean)), y = rmse_mean)) +\n  geom_errorbar(aes(ymin = rmse_mean - rmse_std_err,\n                    ymax = rmse_mean + rmse_std_err),\n                width = 0.1,\n                size = 1.5,\n                color = \"#595959\") +\n  labs(x = \"Model Type\",\n       y = \"Estimated RMSE\",\n       title = \"Estimated RMSE for Best Model of Each Type\")\n\n\n\n\n\n\n\n\nIt appears that Cubist and boosted trees models work best for these data. Let’s finalize a Cubist model by re-fitting the best one on the entire training set, and seeing how well it predicts the age of artifacts in the test set.\n\nrunes_workflow_set |&gt;\n  # get the cubist workflow\n  extract_workflow(\"recipe_cubist\") |&gt;\n  # get the best cubist hyperparameters and apply them to the workflow\n  finalize_workflow(\n    runes_workflow_set |&gt;\n      extract_workflow_set_result(\"recipe_cubist\") |&gt;\n      select_best(metric = \"rmse\")) |&gt;\n  # fit on the entire training set\n  last_fit(split, metrics = metric_set(rmse)) -&gt;\n  final_cubist\n\n\n\nThe Performance-Explainability Trade-off\nUnfortunately, our finalized Cubist model is useless. Since we’re lost in the woods, we can’t actually compute a prediction involving a complicated collection of trees of linear models with hundreds or thousands of coefficients and weights in total.\nCan we find a simple decision tree with comparable performance by trying more hyperparameter combinations? Let’s set tree_depth to 3, find good values for the other hyperparameters, then finalize and evaluate the resulting decision tree model.\n\ndecision_tree_spec |&gt;\n  set_args(tree_depth = 3) |&gt;\n  set_mode(\"regression\") -&gt;\n  decision_tree_spec\n\nworkflow() |&gt;\n  add_model(decision_tree_spec) |&gt;\n  add_recipe(runes_recipe) |&gt;\n  # try 100 combinations of cost_complexity and min_n\n  tune_grid(resamples = folds, metrics = metric_set(rmse), grid = 100) |&gt;\n  # keep the best ones\n  select_best(metric = \"rmse\") |&gt;\n  # plug them back into the model specification\n  finalize_workflow(\n    workflow() |&gt; \n      add_model(decision_tree_spec) |&gt;\n      add_recipe(runes_recipe),\n    parameters = _) |&gt;\n  # fit on the entire training set\n  last_fit(split, metrics = metric_set(rmse)) -&gt;\n  small_tree\n\nNow we have two finalized models: a Cubist model and a small decision tree. The most obvious way to compare them would be to inspect their performance, but we can also use the vip package to extract the number of features used by each model, giving us a way to compare their relative complexity as well.\n\ndata.frame(\n  model    = c(\"Cubist\", \"Small Decision Tree\"),\n  rmse     = c(final_cubist |&gt; collect_metrics() |&gt; pull(.estimate), \n               small_tree   |&gt; collect_metrics() |&gt; pull(.estimate)),\n  features = c(final_cubist |&gt; extract_fit_engine() |&gt; vi() |&gt; filter(Importance &gt; 0) |&gt; nrow(),\n               small_tree   |&gt; extract_fit_engine() |&gt; vi() |&gt; filter(Importance &gt; 0) |&gt; nrow())) |&gt;\n  kbl() |&gt;\n  scroll_box(width = \"51.75%\")\n\n\n\n\n\nmodel\nrmse\nfeatures\n\n\n\n\nCubist\n109.7088\n38\n\n\nSmall Decision Tree\n131.1698\n7\n\n\n\n\n\n\n\nThe small decision tree’s predictions are about 16% worse than the best Cubist model we could find. On the other hand, it’s much simpler. While the Cubist model uses essentially all of the information we provided to it, since we constrained our decision tree to three levels, it can use only a maximum of seven features.12 Although the more complicated Cubist performs well, it’s difficult to explain exactly why, or what the role of each feature is in generating predictions. This illustrates the performance-explainability trade-off.12 None of which are among the ones we engineered, as it happens.\nOur small tree model is simple enough to write on an index card and keep with us when venturing out into the forests of rural Scandinavia. Here it is:\n\nsmall_tree |&gt;\n  extract_fit_engine() |&gt;\n  as.simpleparty() |&gt;\n  plot(\n    ip_args = list(pval = FALSE, id = FALSE),\n    tp_args = list(\n      id = FALSE, \n      FUN = function(node){round(node$prediction[1])}))"
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#conclusion",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#conclusion",
    "title": "(Machine) Learning about Runes",
    "section": "Conclusion",
    "text": "Conclusion\nIf you can foresee yourself desperately needing to know the approximate age of a runic inscription, I recommend you write down the decision tree above and keep it in your pocket. That, or always bring a licensed and qualified runologist13 with you.13 The real job title of perhaps an entire dozen people!"
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#references",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#references",
    "title": "(Machine) Learning about Runes",
    "section": "References",
    "text": "References\n\nRunes\n\nRunes and their Origin: Denmark and Elsewhere (Moltke, 1985)\nRunes (Findell, 2014)\nFuthark Journal\n\n\n\nStatistical Inference and Machine Learning\n\nElements of Statistical Learning (Friedman, et al., 2001)\n\n\n\nData Science in R\n\nTidy Data (Wickham, 2014)\nR for Data Science (Grolemund & Wickham, 2016)\nTidy Modeling with R (Kuhn & Silge, forthcoming)"
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html",
    "href": "posts/wordle_scores/wordle_scores.html",
    "title": "Wordle Scores",
    "section": "",
    "text": "Unless you spent 2022 on the moon, you’ve heard of Wordle, but just in case you haven’t here’s the story. You get up to six guesses to identify a secret five-letter word. The game tells you whether each of the letters of your guesses appears in the word and, if they do, whether they’re in the correct place.\n\n\n\nA typical game of Wordle in progress."
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html#introduction",
    "href": "posts/wordle_scores/wordle_scores.html#introduction",
    "title": "Wordle Scores",
    "section": "",
    "text": "Unless you spent 2022 on the moon, you’ve heard of Wordle, but just in case you haven’t here’s the story. You get up to six guesses to identify a secret five-letter word. The game tells you whether each of the letters of your guesses appears in the word and, if they do, whether they’re in the correct place.\n\n\n\nA typical game of Wordle in progress."
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html#data",
    "href": "posts/wordle_scores/wordle_scores.html#data",
    "title": "Wordle Scores",
    "section": "Data",
    "text": "Data\n\nMy Scores\n\nread_csv(\"wordle_scores.csv\", col_types = \"nn-\") |&gt;\n  mutate(source = \"me\", puzzle = row_number()) -&gt;\n  wordle_scores\n\nwordle_scores |&gt;\n  select(-source, -puzzle) |&gt;\n  st(title = \"My Wordle Scores: Summary Statistics\")\n\n\nMy Wordle Scores: Summary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nscore\n282\n4.1\n1.1\n2\n3\n5\n7\n\n\n\n\n\n\n\n\n\nScores from Twitter\nI found two data sets of Wordle-related tweets on Kaggle: this one and this other one.\nWe can combine them, taking care to remove duplicates. There’s surely some bias here, since people are more likely to share their scores on social media when they do well.\n\nread_csv(\"wordle_tweets_1.zip\", col_types = \"nc--c\") |&gt;\n  rename(puzzle = wordle_id) -&gt;\n  twitter_scores_1\n\nread_csv(\"wordle_tweets_2.zip\", col_types = \"ncc\") |&gt;\n  rename(puzzle = WordleID, tweet_id = ID, tweet_text = Text) -&gt;\n  twitter_scores_2\n\ntwitter_scores_1 |&gt;\n  rbind(twitter_scores_2) |&gt;\n  mutate(score = str_extract(tweet_text, \"Wordle [0-9]{3} ([1-6X])/6\", 1),\n         score = as.numeric(case_match(score, \"X\" ~ \"7\", .default = score)),\n         source = \"twitter\",\n         .keep = \"unused\") |&gt;\n  drop_na() |&gt;\n  distinct(tweet_id, .keep_all = TRUE) |&gt;\n  select(-tweet_id) -&gt;\n  twitter_scores\n\nrm(twitter_scores_1, twitter_scores_2)\n\ntwitter_scores |&gt; \n  select(-source, -puzzle) |&gt;\n  st(title = \"Wordle Scores from Twitter: Summary Statistics\")\n\n\nWordle Scores from Twitter: Summary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nscore\n3090345\n4.1\n1.2\n1\n3\n5\n7\n\n\n\n\n\n\n\nWe should be careful when thinking about mean scores, since we’ve coded the “X” representing a failed puzzle as a 7. If we filter those out, we get the average number of guesses per solved puzzle.\n\nwordle_scores |&gt;\n  full_join(twitter_scores) |&gt;\n  filter(score &lt; 7) |&gt;\n  group_by(source) |&gt;\n  summarize(filtered_mean = mean(score)) |&gt;\n  gt()\n\n\n\n\n\n  \n    \n    \n      source\n      filtered_mean\n    \n  \n  \n    me\n4.021583\n    twitter\n4.085486\n  \n  \n  \n\n\n\n\nLooks like I’m slightly better on average than the people who tweeted their scores, not accounting for any failed puzzles."
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html#visualization",
    "href": "posts/wordle_scores/wordle_scores.html#visualization",
    "title": "Wordle Scores",
    "section": "Visualization",
    "text": "Visualization\nFirst let’s make one those those trend-and-distribution charts that I love so much. See the post about my Jeopardy! Coryat scores for another one.\nFor a more detailed comparison against the scores from Twitter, we’ll look at the cumulative distributions.\n\nLine Chart and HistogramCumulative Distributions\n\n\n\n\nCode\n# main plot\n(wordle_scores |&gt;\n  ggplot() +\n  # aesthetic mapping\n  aes(x = puzzle, y = score) +\n  # visual elements representing the data\n  geom_line(colour = \"#b4b4b4\") +\n  geom_smooth(se = FALSE, colour = \"black\") +\n  geom_point(colour = \"#dc2828\") +\n  # scales\n  scale_y_continuous(limits = c(7.25, 0.75), \n                     breaks = 1:7, \n                     labels = c(1:6, \"X\"), \n                     trans = \"reverse\") +\n  scale_x_continuous(expand = c(0, 0), breaks = NULL) +\n  # labels\n  labs(title = \"My Wordle Scores\",\n       subtitle = \"Trend and distribution\",\n       x = \"\", \n       y = \"Score\") +\n  # theming\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())) |&gt;\n  # add the marginal histogram\n  ggMarginal(type = \"histogram\", \n             margins = \"y\", \n             fill = \"#b4b4b4\",\n             yparams = list(bins = 7, center = 0, binwidth = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# combine the scores from twitter with my own\nwordle_scores |&gt;\n  full_join(twitter_scores) |&gt;\n  # count the occurrences of each possible combination of source and score\n  group_by(source, score) |&gt;\n  count() |&gt;\n  ungroup() |&gt;\n  complete(source, score, fill = list(n = 0)) |&gt;\n  # get the cumulative percentiles for each source\n  arrange(source, score) |&gt;\n  group_by(source) |&gt;\n  mutate(percentile = cumsum(n)/sum(n)) |&gt;\n  # add some helper columns for evil secondary axis trickery later\n  mutate(axis = case_when(percentile == 1 ~ \"n\",\n                          source == \"me\" ~ \"l\",\n                          TRUE ~ \"r\")) |&gt;\n  mutate(\n    r_label_colour  = case_match(axis, \"r\" ~ \"grey30\"),\n    l_label_colour  = case_match(axis, \"l\"  ~ \"grey30\"),\n    r_tick_linetype = case_match(axis, \"r\" ~ \"solid\", .default = \"blank\"),\n    l_tick_linetype = case_match(axis, \"l\"  ~ \"solid\", .default = \"blank\")) -&gt; \n  # need to save this dataframe so we can refer to it within the ggplot call\n  temp\n\ntemp |&gt;\n  ggplot() +\n  aes(x = score, y = percentile, fill = source) +\n  # visual elements representing the data\n  geom_line(linetype = \"dotted\") +\n  geom_point(size = 3, shape = 21, colour = \"black\") +\n  # scales\n  scale_x_continuous(breaks = 1:7, \n                     labels = c(1:6, \"X\"),\n                     expand = c(0, 0)) +\n  ## evil secondary axis trickery part one\n  scale_y_continuous(breaks = temp$percentile,\n                     labels = scales::label_percent(),\n                     limits = c(0, 1),\n                     expand = c(0, 0),\n                     sec.axis = dup_axis()) +\n  scale_fill_manual(values = c(\"#dc2828\", \"white\")) +\n  # labels\n  labs(y = \"\", \n       x = \"Score\", \n       title = \"My Wordle Scores vs Twitter\", \n       subtitle = \"Cumulative distributions\",\n       fill = \"Source\") +\n  # theming\n  theme_bw() +\n  ## evil secondary axis trickery part two\n  theme(panel.grid.minor   = element_blank(),\n        axis.text.y.right  = element_text(colour = temp$r_label_colour),\n        axis.text.y.left   = element_text(colour = temp$l_label_colour),\n        axis.ticks.y.right = element_line(linetype = temp$r_tick_linetype),\n        axis.ticks.y.left  = element_line(linetype = temp$l_tick_linetype))"
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html#observations",
    "href": "posts/wordle_scores/wordle_scores.html#observations",
    "title": "Wordle Scores",
    "section": "Observations",
    "text": "Observations\nI get fewer puzzles in two or fewer guesses than those who posted their scores to Twitter, but I do better at the harder words. I suspect this is the result of the bias speculated about above; probably there are many people who only tweeted because they got the puzzle in two guesses."
  },
  {
    "objectID": "posts/wordle_scores/wordle_scores.html#references-further-reading",
    "href": "posts/wordle_scores/wordle_scores.html#references-further-reading",
    "title": "Wordle Scores",
    "section": "References & Further Reading",
    "text": "References & Further Reading\n\nWordle-solving state of the art: all optimality results so far"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Jesse Onland",
    "section": "",
    "text": "Cycling Hour Records\n\n\n\n\n\nTime trials and tribulations.\n\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n\n\nHomeworlds Openings\n\n\n\n\n\nA sojourn on star situations.\n\n\n\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\nWordle Scores\n\n\n\n\n\nA wealth of word whimsy.\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\nBooks No One Else Has\n\n\n\n\n\nAn examination of extraordinary editions.\n\n\n\n\n\n\nFeb 4, 2023\n\n\n\n\n\n\n\n\nCrossword Times\n\n\nWith a word on density plots\n\n\nA consideration of cruciverbal complexity.\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n\n\nCoryat Scores\n\n\n\n\n\nThe joy of Jeopardy! judgement.\n\n\n\n\n\n\nOct 17, 2022\n\n\n\n\n\n\n\n\n(Machine) Learning about Runes\n\n\n\n\n\nAlphabetic algorithm alchemy.\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n\n\nCities with Nice Weather\n\n\n\n\n\nA meteorological meditation.\n\n\n\n\n\n\nMay 12, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#modelling-solve-times",
    "href": "posts/crossword_times/crossword_times.html#modelling-solve-times",
    "title": "Crossword Times",
    "section": "Modelling Solve Times",
    "text": "Modelling Solve Times\nIt’s plausible to think that crossword solve times would be distributed log-normally. As the name suggests, a log-normally distributed variable has a normally distributed logarithm. The relevant Wikipedia article lists a variety of phenomena which seem to follow a log-normal distribution, including the length of chess games, the incubation period of diseases, and incomes (except those of the very rich).\n\nFitting a Bayesian Log-Normal Regression\nIf we imagine that the distribution of solves times within each combination of publisher and day of the week is log-normal where the underlying normal distribution has some particular mean and standard deviation, we can provide that model specification to brms, accept some default priors, and have it estimate posterior distributions of those means and standard deviations based on the observed solve times.\n\ncrossword_times |&gt;\n  brm(formula = minutes ~ (1 + day | publisher),\n      data = _,\n      family = lognormal(),\n      warmup = 1000,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = 0.95)) -&gt;\n  solve_time_model"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#data",
    "href": "posts/crossword_times/crossword_times.html#data",
    "title": "Crossword Times",
    "section": "Data",
    "text": "Data\nI’m accustomed to thinking of the week as beginning with Sunday, but for the purposes of crossword difficulty analysis, Monday is better. We’ll convert the solve times from 00:00-style to minutes.\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |&gt;\n  transmute(Publisher,\n            day = factor(wday(Date, week_start = 1)),\n            minutes = period_to_seconds(ms(Time))/60) |&gt;\n  rename(publisher = Publisher) -&gt;\n  crossword_times\n\n\nTableSummary\n\n\n\n\nCode\ncrossword_times |&gt;\n  mutate(minutes = round(minutes)) |&gt;\n  kable(caption = \"My Crossword Solve Times\") |&gt;\n  scroll_box(height = \"5in\")\n\n\n\n\nMy Crossword Solve Times\n\n\npublisher\nday\nminutes\n\n\n\n\nNew York Times\n2\n11\n\n\nLos Angeles Times\n2\n6\n\n\nLos Angeles Times\n3\n9\n\n\nNew York Times\n3\n22\n\n\nNew York Times\n4\n14\n\n\nLos Angeles Times\n4\n16\n\n\nLos Angeles Times\n5\n19\n\n\nNew York Times\n5\n22\n\n\nNew York Times\n1\n9\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n10\n\n\nNew York Times\n3\n12\n\n\nLos Angeles Times\n3\n7\n\n\nNew York Times\n4\n18\n\n\nLos Angeles Times\n4\n21\n\n\nLos Angeles Times\n5\n8\n\n\nNew York Times\n7\n29\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n6\n17\n\n\nNew York Times\n6\n37\n\n\nLos Angeles Times\n6\n14\n\n\nLos Angeles Times\n2\n10\n\n\nNew Yorker\n2\n13\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n2\n5\n\n\nNew Yorker\n2\n16\n\n\nNew Yorker\n1\n19\n\n\nNew Yorker\n4\n8\n\n\nNew Yorker\n5\n8\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n4\n5\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n12\n\n\nNew York Times\n7\n36\n\n\nNew Yorker\n3\n7\n\n\nNew Yorker\n1\n33\n\n\nLos Angeles Times\n6\n15\n\n\nNew York Times\n4\n38\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n26\n\n\nNew Yorker\n5\n22\n\n\nLos Angeles Times\n5\n17\n\n\nLos Angeles Times\n6\n17\n\n\nNew York Times\n6\n18\n\n\nNew York Times\n7\n61\n\n\nNew Yorker\n1\n65\n\n\nLos Angeles Times\n7\n38\n\n\nNew Yorker\n3\n8\n\n\nNew Yorker\n2\n17\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n1\n21\n\n\nLos Angeles Times\n7\n23\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n15\n\n\nNew Yorker\n1\n22\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n27\n\n\nLos Angeles Times\n4\n18\n\n\nNew York Times\n4\n14\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n48\n\n\nLos Angeles Times\n5\n22\n\n\nLos Angeles Times\n6\n18\n\n\nNew York Times\n7\n41\n\n\nLos Angeles Times\n7\n27\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n7\n\n\nNew York Times\n3\n7\n\n\nNew Yorker\n1\n39\n\n\nNew Yorker\n2\n12\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n7\n\n\nNew Yorker\n3\n13\n\n\nLos Angeles Times\n4\n18\n\n\nNew York Times\n4\n19\n\n\nNew Yorker\n4\n6\n\n\nNew Yorker\n5\n10\n\n\nLos Angeles Times\n5\n18\n\n\nNew York Times\n5\n13\n\n\nLos Angeles Times\n6\n32\n\n\nNew York Times\n6\n35\n\n\nNew York Times\n7\n47\n\n\nLos Angeles Times\n7\n24\n\n\nLos Angeles Times\n1\n9\n\n\nNew York Times\n1\n8\n\n\nNew Yorker\n1\n37\n\n\nLos Angeles Times\n2\n12\n\n\nNew York Times\n2\n7\n\n\nNew York Times\n5\n55\n\n\nNew Yorker\n2\n16\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n13\n\n\nNew Yorker\n3\n14\n\n\nLos Angeles Times\n4\n13\n\n\nNew Yorker\n4\n9\n\n\nNew York Times\n4\n22\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n5\n12\n\n\nLos Angeles Times\n3\n8\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n3\n18\n\n\nLos Angeles Times\n4\n11\n\n\nNew York Times\n6\n22\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n5\n17\n\n\nNew York Times\n3\n14\n\n\nLos Angeles Times\n3\n10\n\n\nNew York Times\n4\n17\n\n\nLos Angeles Times\n4\n17\n\n\nNew Yorker\n4\n8\n\n\nNew Yorker\n3\n21\n\n\nLos Angeles Times\n5\n29\n\n\nNew York Times\n5\n29\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n10\n\n\nNew Yorker\n1\n50\n\n\nNew Yorker\n2\n14\n\n\nNew York Times\n2\n6\n\n\nLos Angeles Times\n3\n10\n\n\nNew Yorker\n3\n12\n\n\nNew York Times\n3\n12\n\n\nWall Street Journal\n3\n19\n\n\nNew York Times\n4\n24\n\n\nWall Street Journal\n4\n19\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n7\n\n\nLos Angeles Times\n5\n24\n\n\nNew Yorker\n5\n10\n\n\nWall Street Journal\n6\n47\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n20\n\n\nNew Yorker\n2\n16\n\n\nNew York Times\n2\n10\n\n\nLos Angeles Times\n2\n7\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n7\n\n\nWall Street Journal\n3\n11\n\n\nNew Yorker\n3\n11\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n4\n20\n\n\nWall Street Journal\n4\n15\n\n\nNew York Times\n5\n19\n\n\nNew Yorker\n5\n7\n\n\nWall Street Journal\n5\n13\n\n\nNew York Times\n6\n56\n\n\nLos Angeles Times\n6\n21\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n5\n\n\nWall Street Journal\n6\n34\n\n\nNew Yorker\n1\n39\n\n\nLos Angeles Times\n2\n7\n\n\nNew Yorker\n2\n21\n\n\nNew York Times\n3\n10\n\n\nNew Yorker\n3\n8\n\n\nLos Angeles Times\n4\n8\n\n\nNew York Times\n4\n23\n\n\nNew Yorker\n4\n8\n\n\nLos Angeles Times\n6\n22\n\n\nNew York Times\n6\n34\n\n\nNew York Times\n5\n39\n\n\nNew Yorker\n5\n28\n\n\nLos Angeles Times\n7\n41\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n37\n\n\nNew York Times\n1\n8\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n3\n14\n\n\nWall Street Journal\n2\n11\n\n\nNew York Times\n2\n9\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n18\n\n\nNew York Times\n3\n13\n\n\nLos Angeles Times\n3\n11\n\n\nNew Yorker\n3\n7\n\n\nNew York Times\n4\n21\n\n\nLos Angeles Times\n4\n10\n\n\nNew Yorker\n4\n6\n\n\nLos Angeles Times\n5\n12\n\n\nNew York Times\n5\n21\n\n\nNew Yorker\n5\n11\n\n\nLos Angeles Times\n6\n45\n\n\nNew York Times\n6\n41\n\n\nLos Angeles Times\n7\n27\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n11\n\n\nNew York Times\n3\n16\n\n\nLos Angeles Times\n3\n10\n\n\nNew Yorker\n3\n12\n\n\nWall Street Journal\n3\n16\n\n\nNew Yorker\n5\n11\n\n\nNew Yorker\n4\n5\n\n\nNew Yorker\n2\n31\n\n\nWall Street Journal\n1\n9\n\n\nLos Angeles Times\n4\n36\n\n\nNew York Times\n5\n18\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n10\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n6\n\n\nNew York Times\n3\n19\n\n\nNew York Times\n4\n28\n\n\nNew Yorker\n4\n5\n\n\nLos Angeles Times\n4\n6\n\n\nNew Yorker\n5\n15\n\n\nWall Street Journal\n5\n19\n\n\nNew York Times\n5\n16\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n9\n\n\nNew York Times\n3\n8\n\n\nWall Street Journal\n1\n5\n\n\nNew York Times\n6\n31\n\n\nWall Street Journal\n2\n10\n\n\nLos Angeles Times\n4\n11\n\n\nNew York Times\n4\n26\n\n\nNew Yorker\n4\n4\n\n\nLos Angeles Times\n5\n13\n\n\nNew York Times\n5\n39\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nLos Angeles Times\n2\n7\n\n\nWall Street Journal\n2\n10\n\n\nWall Street Journal\n6\n34\n\n\nWall Street Journal\n6\n43\n\n\nNew York Times\n2\n7\n\n\nNew York Times\n3\n12\n\n\nLos Angeles Times\n3\n20\n\n\nNew Yorker\n3\n10\n\n\nWall Street Journal\n1\n10\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n26\n\n\nLos Angeles Times\n5\n9\n\n\nWall Street Journal\n5\n28\n\n\nNew Yorker\n5\n16\n\n\nNew York Times\n7\n23\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n5\n\n\nWall Street Journal\n1\n7\n\n\nLos Angeles Times\n2\n7\n\n\nNew Yorker\n2\n22\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n14\n\n\nLos Angeles Times\n3\n11\n\n\nNew Yorker\n3\n14\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n4\n28\n\n\nWall Street Journal\n4\n19\n\n\nLos Angeles Times\n5\n18\n\n\nNew York Times\n5\n20\n\n\nNew Yorker\n5\n16\n\n\nWall Street Journal\n5\n15\n\n\nLos Angeles Times\n6\n33\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n3\n11\n\n\nLos Angeles Times\n3\n11\n\n\nNew York Times\n5\n11\n\n\nLos Angeles Times\n5\n12\n\n\nNew Yorker\n4\n9\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n5\n10\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n2\n27\n\n\nNew Yorker\n1\n39\n\n\nNew York Times\n2\n8\n\n\nNew York Times\n3\n13\n\n\nNew Yorker\n3\n5\n\n\nLos Angeles Times\n4\n12\n\n\nNew Yorker\n4\n6\n\n\nWall Street Journal\n2\n11\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n4\n12\n\n\nWall Street Journal\n3\n9\n\n\nWall Street Journal\n5\n14\n\n\nNew Yorker\n5\n11\n\n\nNew York Times\n6\n66\n\n\nNew York Times\n7\n44\n\n\nNew York Times\n1\n6\n\n\nNew York Times\n2\n10\n\n\nLos Angeles Times\n3\n13\n\n\nNew York Times\n4\n40\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n9\n\n\nNew York Times\n5\n25\n\n\nNew York Times\n2\n7\n\n\nNew Yorker\n5\n16\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n2\n8\n\n\nLos Angeles Times\n2\n6\n\n\nNew Yorker\n1\n58\n\n\nNew Yorker\n2\n14\n\n\nNew York Times\n3\n8\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n4\n20\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n5\n22\n\n\nLos Angeles Times\n5\n18\n\n\nNew Yorker\n5\n16\n\n\nLos Angeles Times\n6\n41\n\n\nNew York Times\n7\n35\n\n\nNew York Times\n1\n5\n\n\nNew Yorker\n1\n25\n\n\nLos Angeles Times\n1\n6\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n13\n\n\nWall Street Journal\n1\n6\n\n\nWall Street Journal\n5\n9\n\n\nNew York Times\n3\n18\n\n\nLos Angeles Times\n3\n8\n\n\nNew Yorker\n3\n7\n\n\nWall Street Journal\n6\n23\n\n\nWall Street Journal\n6\n28\n\n\nNew York Times\n4\n28\n\n\nLos Angeles Times\n4\n7\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n21\n\n\nLos Angeles Times\n5\n10\n\n\nNew Yorker\n5\n26\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n11\n\n\nLos Angeles Times\n6\n24\n\n\nWall Street Journal\n1\n7\n\n\nNew Yorker\n2\n37\n\n\nNew York Times\n2\n6\n\n\nLos Angeles Times\n2\n8\n\n\nWall Street Journal\n2\n13\n\n\nLos Angeles Times\n3\n12\n\n\nNew York Times\n3\n9\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n19\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n6\n\n\nNew Yorker\n5\n21\n\n\nNew York Times\n1\n9\n\n\nNew York Times\n7\n41\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n25\n\n\nWall Street Journal\n3\n22\n\n\nLos Angeles Times\n4\n11\n\n\nNew Yorker\n4\n6\n\n\nLos Angeles Times\n5\n22\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n6\n\n\nWall Street Journal\n1\n5\n\n\nNew Yorker\n5\n8\n\n\nNew York Times\n3\n7\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n2\n6\n\n\nNew Yorker\n2\n16\n\n\nWall Street Journal\n2\n11\n\n\nNew York Times\n3\n10\n\n\nLos Angeles Times\n3\n6\n\n\nNew York Times\n4\n14\n\n\nLos Angeles Times\n4\n12\n\n\nNew Yorker\n4\n4\n\n\nLos Angeles Times\n5\n13\n\n\nNew Yorker\n5\n6\n\n\nNew York Times\n6\n29\n\n\nWall Street Journal\n6\n38\n\n\nLos Angeles Times\n1\n9\n\n\nNew York Times\n1\n7\n\n\nNew Yorker\n4\n7\n\n\nLos Angeles Times\n4\n14\n\n\nNew Yorker\n5\n23\n\n\nNew York Times\n1\n5\n\n\nNew York Times\n2\n10\n\n\nNew York Times\n3\n11\n\n\nNew Yorker\n5\n14\n\n\nNew York Times\n3\n5\n\n\nNew Yorker\n3\n10\n\n\nNew Yorker\n4\n6\n\n\nNew York Times\n1\n4\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n2\n5\n\n\nNew Yorker\n3\n10\n\n\nNew York Times\n4\n11\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n13\n\n\nNew Yorker\n5\n14\n\n\nNew York Times\n6\n17\n\n\nNew York Times\n1\n7\n\n\nLos Angeles Times\n1\n8\n\n\nNew York Times\n2\n9\n\n\nNew York Times\n4\n16\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n8\n\n\nNew York Times\n4\n15\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n7\n25\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n7\n\n\nWall Street Journal\n2\n8\n\n\nNew York Times\n3\n8\n\n\nLos Angeles Times\n3\n7\n\n\nNew Yorker\n5\n10\n\n\nNew Yorker\n4\n4\n\n\nNew York Times\n5\n19\n\n\nNew York Times\n1\n4\n\n\nLos Angeles Times\n1\n7\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n6\n\n\nNew Yorker\n2\n15\n\n\nNew York Times\n3\n10\n\n\nWall Street Journal\n3\n19\n\n\nNew York Times\n4\n30\n\n\nLos Angeles Times\n4\n13\n\n\nWall Street Journal\n4\n27\n\n\nNew York Times\n5\n12\n\n\nLos Angeles Times\n5\n7\n\n\nNew Yorker\n5\n15\n\n\nNew York Times\n6\n11\n\n\nWall Street Journal\n5\n18\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n5\n\n\nWall Street Journal\n1\n9\n\n\nNew Yorker\n1\n31\n\n\nNew York Times\n2\n5\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n2\n19\n\n\nNew York Times\n3\n12\n\n\nNew Yorker\n3\n14\n\n\nNew York Times\n4\n23\n\n\nNew Yorker\n4\n6\n\n\nWall Street Journal\n4\n20\n\n\nNew York Times\n5\n19\n\n\nNew Yorker\n3\n8\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n1\n6\n\n\nNew York Times\n2\n6\n\n\nNew York Times\n3\n14\n\n\nNew York Times\n1\n6\n\n\nLos Angeles Times\n2\n8\n\n\nNew York Times\n2\n8\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n3\n12\n\n\nNew Yorker\n3\n6\n\n\nNew York Times\n4\n16\n\n\nNew Yorker\n4\n5\n\n\nNew York Times\n5\n9\n\n\nNew Yorker\n5\n8\n\n\nNew York Times\n6\n38\n\n\nLos Angeles Times\n6\n15\n\n\nLos Angeles Times\n5\n13\n\n\nNew York Times\n7\n26\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n8\n\n\nNew Yorker\n1\n24\n\n\nNew York Times\n2\n12\n\n\nLos Angeles Times\n2\n9\n\n\nNew Yorker\n2\n9\n\n\nNew York Times\n3\n8\n\n\nLos Angeles Times\n3\n8\n\n\nNew York Times\n4\n17\n\n\nLos Angeles Times\n4\n9\n\n\nNew Yorker\n4\n4\n\n\nNew Yorker\n5\n12\n\n\nNew York Times\n7\n51\n\n\nLos Angeles Times\n7\n25\n\n\nNew Yorker\n1\n30\n\n\nNew York Times\n1\n4\n\n\nNew York Times\n7\n68\n\n\nNew York Times\n3\n9\n\n\nLos Angeles Times\n3\n5\n\n\nNew Yorker\n5\n8\n\n\nWall Street Journal\n6\n23\n\n\nNew York Times\n6\n29\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n1\n21\n\n\nWall Street Journal\n1\n4\n\n\nWall Street Journal\n5\n16\n\n\nWall Street Journal\n6\n33\n\n\nNew York Times\n2\n8\n\n\nNew Yorker\n2\n9\n\n\nWall Street Journal\n2\n6\n\n\nLos Angeles Times\n2\n7\n\n\nNew York Times\n3\n17\n\n\nLos Angeles Times\n3\n7\n\n\nWall Street Journal\n3\n12\n\n\nNew Yorker\n4\n4\n\n\nLos Angeles Times\n5\n9\n\n\nNew Yorker\n5\n9\n\n\nLos Angeles Times\n6\n13\n\n\nNew York Times\n6\n33\n\n\nWall Street Journal\n6\n32\n\n\nNew York Times\n1\n4\n\n\nLos Angeles Times\n1\n7\n\n\nNew Yorker\n1\n24\n\n\nNew Yorker\n2\n14\n\n\nNew York Times\n2\n5\n\n\nLos Angeles Times\n2\n9\n\n\nWall Street Journal\n2\n6\n\n\nNew York Times\n3\n7\n\n\nLos Angeles Times\n3\n7\n\n\nNew Yorker\n3\n8\n\n\nWall Street Journal\n3\n14\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n20\n\n\nLos Angeles Times\n4\n6\n\n\nNew York Times\n4\n30\n\n\nNew York Times\n5\n14\n\n\nLos Angeles Times\n5\n11\n\n\nNew Yorker\n5\n8\n\n\nNew York Times\n6\n31\n\n\nLos Angeles Times\n6\n23\n\n\nWall Street Journal\n6\n33\n\n\nLos Angeles Times\n7\n27\n\n\nNew York Times\n7\n35\n\n\nLos Angeles Times\n1\n5\n\n\nNew York Times\n1\n4\n\n\nNew Yorker\n1\n27\n\n\nWall Street Journal\n1\n6\n\n\nNew York Times\n2\n8\n\n\nLos Angeles Times\n2\n5\n\n\nWall Street Journal\n2\n12\n\n\nNew Yorker\n2\n13\n\n\nNew York Times\n3\n10\n\n\nLos Angeles Times\n3\n6\n\n\nNew Yorker\n3\n10\n\n\nWall Street Journal\n3\n10\n\n\nNew York Times\n4\n19\n\n\nLos Angeles Times\n4\n17\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n15\n\n\nNew York Times\n5\n20\n\n\nLos Angeles Times\n5\n14\n\n\nNew Yorker\n5\n18\n\n\nNew York Times\n6\n32\n\n\nLos Angeles Times\n6\n22\n\n\nWall Street Journal\n6\n16\n\n\nNew York Times\n7\n38\n\n\nLos Angeles Times\n7\n28\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n6\n\n\nNew Yorker\n1\n18\n\n\nWall Street Journal\n1\n6\n\n\nNew York Times\n2\n7\n\n\nLos Angeles Times\n2\n6\n\n\nNew Yorker\n2\n11\n\n\nWall Street Journal\n2\n7\n\n\nNew York Times\n3\n10\n\n\nLos Angeles Times\n3\n14\n\n\nWall Street Journal\n3\n11\n\n\nNew Yorker\n3\n9\n\n\nNew York Times\n4\n14\n\n\nLos Angeles Times\n4\n9\n\n\nNew Yorker\n4\n5\n\n\nWall Street Journal\n4\n13\n\n\nNew York Times\n5\n20\n\n\nLos Angeles Times\n5\n12\n\n\nNew Yorker\n5\n10\n\n\nNew York Times\n5\n24\n\n\nLos Angeles Times\n6\n18\n\n\nWall Street Journal\n6\n27\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n1\n4\n\n\nWall Street Journal\n1\n8\n\n\nNew Yorker\n1\n21\n\n\nNew York Times\n2\n7\n\n\nWall Street Journal\n2\n6\n\n\nLos Angeles Times\n2\n8\n\n\nNew Yorker\n2\n12\n\n\nNew York Times\n3\n14\n\n\nLos Angeles Times\n3\n10\n\n\nWall Street Journal\n3\n9\n\n\nNew Yorker\n3\n15\n\n\nNew York Times\n4\n15\n\n\nLos Angeles Times\n4\n10\n\n\nWall Street Journal\n4\n17\n\n\nNew Yorker\n5\n11\n\n\nLos Angeles Times\n5\n15\n\n\nNew York Times\n6\n26\n\n\nNew York Times\n7\n39\n\n\nLos Angeles Times\n7\n20\n\n\nLos Angeles Times\n6\n16\n\n\nNew York Times\n1\n5\n\n\nLos Angeles Times\n2\n7\n\n\nNew York Times\n2\n9\n\n\nNew Yorker\n2\n35\n\n\nLos Angeles Times\n3\n6\n\n\nNew Yorker\n3\n8\n\n\nNew Yorker\n4\n7\n\n\nNew Yorker\n5\n7\n\n\nLos Angeles Times\n6\n33\n\n\nNew York Times\n5\n20\n\n\n\n\n\n\n\n\n\n\n\nCode\ncrossword_times |&gt;\n  group_by(publisher, day) |&gt;\n  summarize(n = n(), mean = mean(minutes), sd = sd(minutes)) |&gt;\n  mutate(across(c(\"mean\", \"sd\"), \\(x) round(x))) |&gt;\n  pivot_wider(names_from = publisher, \n              values_from = c(\"n\", \"mean\", \"sd\"),\n              names_sort = TRUE) |&gt;\n  relocate(c(1, 2, 6, 10, 3, 7, 11, 4, 8, 12, 5, 9, 13)) |&gt;\n  gt() |&gt;\n  tab_header(title = \"My Crossword Solve Times: Summary\") |&gt;\n  tab_spanner(label = \"Los Angeles Times\", columns = 2:4) |&gt;\n  tab_spanner(label = \"New York Times\", columns = 5:7) |&gt;\n  tab_spanner(label = \"New Yorker\", columns = 8:10) |&gt;\n  tab_spanner(label = \"Wall Street Journal\", columns = 11:13) |&gt;\n  cols_label(starts_with(\"n\") ~ \"n\", \n             starts_with(\"mean\") ~ \"mean\", \n             starts_with(\"sd\") ~ \"sd\")\n\n\n\n\n\n\n  \n    \n      My Crossword Solve Times: Summary\n    \n    \n    \n      day\n      \n        Los Angeles Times\n      \n      \n        New York Times\n      \n      \n        New Yorker\n      \n      \n        Wall Street Journal\n      \n    \n    \n      n\n      mean\n      sd\n      n\n      mean\n      sd\n      n\n      mean\n      sd\n      n\n      mean\n      sd\n    \n  \n  \n    1\n36\n6\n1\n43\n6\n1\n22\n32\n13\n14\n7\n2\n    2\n33\n8\n2\n39\n8\n2\n26\n17\n7\n12\n9\n2\n    3\n29\n9\n3\n40\n12\n4\n29\n11\n3\n12\n14\n4\n    4\n27\n13\n6\n31\n21\n7\n41\n6\n1\n12\n17\n5\n    5\n23\n15\n6\n28\n23\n11\n36\n13\n6\n8\n16\n5\n    6\n18\n24\n10\n19\n32\n13\nNA\nNA\nNA\n13\n32\n8\n    7\n10\n28\n7\n16\n40\n12\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#modelling",
    "href": "posts/crossword_times/crossword_times.html#modelling",
    "title": "Crossword Times",
    "section": "Modelling",
    "text": "Modelling\nIt’s plausible to think that crossword solve times would be distributed log-normally. As the name suggests, a log-normally distributed variable has a normally distributed logarithm. The relevant Wikipedia article lists a variety of phenomena which seem to follow a log-normal distribution, including the length of chess games, the incubation period of diseases, and incomes (except those of the very rich).\n\nFitting a Bayesian Log-Normal Regression\nIf we imagine that the distribution of solves times within each combination of publisher and day of the week is log-normal where the underlying normal distribution has some particular mean and standard deviation, we can provide that model specification to brms, accept some default priors, and have it estimate posterior distributions of those means and standard deviations based on the observed solve times.\n\ncrossword_times |&gt;\n  brm(formula = minutes ~ (1 + day | publisher),\n      data = _,\n      family = lognormal(),\n      warmup = 1000,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = 0.95)) -&gt;\n  solve_time_model"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#visualization",
    "href": "posts/crossword_times/crossword_times.html#visualization",
    "title": "Crossword Times",
    "section": "Visualization",
    "text": "Visualization\nIt’s a matter of some debate among data visualization practitioners about when to use density plots, such as violin plots, instead of histograms. Or indeed whether density plots should ever be used. Or even whether histograms should ever be used!\nI disagree with several of the points Angela Collier makes in her video “violin plots should not exist”, but one that I find compelling is that drawing density plots usually involves what amounts to fitting an unjustified model.\nIn most situations, ggplot uses locally estimated scatterplot smoothing (LOESS) by default, which involves fitting a separate polynomial regression model on a weighted neighbourhood around each data point and evaluating it there. It (usually) makes nice looking violin plots, but you wouldn’t expect it to reflect that “actual” theoretical distribution of the data.\nIt seems to me that this sort of thing is a symptom of a general desire to avoid having to actually specify models by pretending that there’s some bright-line distinction between descriptive statistics and statistical inference.\nSince we were willing to actually specify a model, we can make density plots that show something meaningful: the posterior predictive distributions corresponding to our model. We’ll use add_predicted_draws() from tidybayes to generate draws from those models and use these to draw violins that reflect the data we expect to observe in the future based on those we have observed so far. (We also enlist the help of data_grid from modelr.)\n\nViolin PlotLine Graphs\n\n\n\n\nCode\ncrossword_times |&gt;\n  data_grid(publisher, day) |&gt;\n  add_predicted_draws(solve_time_model) |&gt;\n  # filter out predictions for groups that do not actually occur\n  filter(!(publisher == \"New Yorker\" & day %in% c(6, 7)),\n         !(publisher == \"Wall Street Journal\" & day == 7)) |&gt;\n  ggplot() +\n  # plot structure\n  facet_grid(cols = vars(publisher)) +\n  # aesthetic mapping for the violins\n  aes(y = .prediction, x = day) +\n  # visual elements representing the posterior predictive model\n  stat_eye(colour = NA, fill = \"grey\") +\n  # visual elements representing the observed data\n  geom_sina(data = crossword_times,\n            aes(y = minutes, fill = publisher),\n            pch = 21, \n            alpha = 0.5) +\n  # scales\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  scale_y_continuous(breaks = seq(0, 70, 5), \n                     limits = c(0, 70), \n                     expand = c(0, 0)) +\n  scale_fill_viridis(discrete = TRUE) +\n  # labels\n  labs(title = \"Crossword Solve Times\",\n       subtitle = \"With posterior predictive log-normal distributions\",\n       x = \"Day of the Week\",\n       y = \"Solve Time (Minutes)\") +\n  guides(fill = \"none\") +\n  # theming\n  theme_bw() +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        strip.text = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nc(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\") |&gt;\n  setNames(1:7) |&gt;\n  as_labeller() -&gt;\n  day_labeller\n\ncrossword_times |&gt;\n  # group by publisher and day, add per-group row numbers\n  group_by(publisher, day) |&gt;\n  mutate(id = row_number()) |&gt;\n  ggplot() +\n  # plot structure\n  facet_grid2(rows = vars(publisher), \n              cols = vars(day),\n              labeller = labeller(day = day_labeller),\n              scales = \"free_x\",\n              independent = \"x\") +\n  # aesthetic mapping\n  aes(x = id, y = minutes, colour = publisher) +\n  geom_point() +\n  # visual elements representing the data\n  geom_line() +\n  # scales\n  scale_y_continuous(n.breaks = 12, minor_breaks = 0:65) +\n  scale_colour_viridis(discrete = TRUE) +\n  # labels\n  labs(title = \"Solve Time Trend by Publisher and Day of Week\",\n       y = \"Solve Time (Minutes)\") +\n  # theming\n  theme_bw() +\n  theme(legend.position = \"none\", \n        panel.spacing.y = unit(0.02, \"npc\"),\n        axis.text.y = element_text(size = rel(0.65)),\n        axis.text.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        legend.margin = margin(0, 0, 0, 0),\n        strip.text = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeom_violin vs Log-Normal Densities\nWhat’s the difference between plotting LOESS-based densities and posterior predictive distributions? I think this image illustrates the shortcomings of the former approach well:\n\n\n\ngeom_violin (black) vs our posterior predictive distributions (grey)."
  }
]