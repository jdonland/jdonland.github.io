[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesse Onland",
    "section": "",
    "text": "I‚Äôm a data scientist at Samos in Kitchener, Ontario, Canada.\nI‚Äôm interested in inference broadly construed. That includes symbolic logic, mathematical proof, statistical inference, machine learning, and other topics related to knowing what we ought to conclude from the facts available to us.\nMy motto is, ‚ÄúIf mathematics were cheesecake, I wouldn‚Äôt be a piano tuner.‚Äù Ask me about it some time!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Here are some things on the Web that I‚Äôve found interesting, useful, or impactful.\n\nüé® Art\n\nThis Man Has Spent 30 Years Mapping the Imaginary Land of Ukrania\nRagna R√≥bertsd√≥ttir\nSelected Works by M. C. Escher\nSelected Piet Mondrian Paintings\nHenry Darger\nGerd Arntz Web Archive\n\n\n\nüèôÔ∏è Cities\n\n50 Reasons Why Everyone Should Want More Walkable Streets\nWhat I Mean When I Say ‚ÄòBan Cars‚Äô\nThe People Who Hate People\nStroads are Ugly, Expensive, and Dangerous (and they‚Äôre everywhere)\nWhat‚Äôs a Gadgetbahn?\nShifting gears: why US cities are falling out of love with the parking lot\n\n\n\nüõç Consumer Goods\n\nHow To Judge Quality In Clothing\nShaving Is Too Expensive. Also the world is out to get you\nThe Kitchen Starter Kit: Essential Tools for Every Cook\nYour stuff is actually worse now\n\n\n\nüèõÔ∏è Culture & Politics\n\nCanada is fake\nIs Culture Snobbery The New Resistance?\nA decade of sore winners\nWhat Moneyball-for-Everything Has Done to American Culture\nWe Live In The Age of The Bullshitter\n\n\n\nüìä Data Science\n\nData Science: Reality Doesn‚Äôt Meet Expectations\nThe weirdest paradox in statistics (and machine learning)\nReview of Probability Theory: The Logic of Science\nPhilosophy of Statistics\nThe Good Research Code Handbook\nThinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data\nCommon statistical tests are linear models (or: how to teach stats)\nGoodbye, Data Science\nProbability that a number is prime\nWhy Business Data Science Irritates Me\nSimpson‚Äôs paradox all the way down\nOn Moving from Statistics to Machine Learning, the Final Stage of Grief\n\n\n\nüé≤ Games\n\nhangman is a weird game\n\n\n\nüó£Ô∏è Language\n\nUtopian for Beginners\nThe Interpreter\nNo B√©arla\nWhat Consonant Clusters are Possible?\nWhy is English so weirdly different from other languages?\nWhy language might be the optimal self-regulating system\n\n\n\nüìö Learning\n\nHow To Understand Things\nHow to Self-Study Math\nEffectively self-studying over the¬†Internet\n\n\n\nüíµ Money\n\nThere‚Äôs Nothing to Do Except Gamble\nThe Singular Pursuit of Comrade Bezos\n\n\n\nüéµ Music\n\nBlood on the Frets\nWhy 12 notes to the Octave?\nA Guide to Imaginational Anthem‚Äôs 10 Volumes of Stirring American Primitive Guitar\n‚ÄòA Real Little Taste Of Heaven‚Äô: Visionary Guitarist Jack Rose In 11 Songs\n\n\n\nü§î Philosophy\n\nWhose Idea Is It, Anyway?\nIt‚Äôs all bullsh*t! On the relationship between pseudoscience and pseudophilosophy\nThe Normative Status of Logic\nUse and Mention\nPhilosophy of Probability\nPhilosophy of Statistics\n\n\n\nüìñ Poetry & Short Fiction\n\nMMAcevedo\nSecond Person, Present Tense\nWant Song\nAutopsy\nMiscreant Insects\nTwo-Headed Calf\n\n\n\n‚è∞ Productivity\n\nProductivity 101: An Introduction to The Pomodoro Technique\nHow a Lazy Bitch like me learned to be Productive\nThe Case Against Collaboration\n\n\n\nüß† Psychology\n\nWhy aren‚Äôt smart people happier?\n\n\n\nüëî Style\n\nHow To Develop Good Taste, Pt. 1\nHow To Develop Good Taste, Pt. 2\nHow To Develop Good Taste, Pt. 3\nArticles of Interest - American Ivy\nThe Most Stylish Man Alive\n\n\n\nüíæ Technology\n\nWhy Nothing Works Anymore\nSocialism‚Äôs DIY Computer\nHow to Take Back the Internet by Choosing the Internet Less Traveled\nThe Old Internet Shows Signs of Quietly Coming Back\nWhy I Still Use RSS\n12ft Ladder\nTinyWow\nWhy We Should Have Markdown Rendered Websites"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Jesse Onland",
    "section": "",
    "text": "Crossword Times\n\n\n\n\n\nA consideration of cruciverbal complexity.\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCoryat Scores\n\n\n\n\n\nThe joy of Jeopardy! judgement.\n\n\n\n\n\n\nOct 17, 2022\n\n\n\n\n\n\n  \n\n\n\n\n(Machine) Learning about Runes\n\n\n\n\n\nAlphabetic algorithm alchemy.\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCities with Nice Weather\n\n\n\n\n\nA meteorological meditation.\n\n\n\n\n\n\nMay 12, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html",
    "href": "posts/city_temperatures/city_temperatures.html",
    "title": "Cities with Nice Weather",
    "section": "",
    "text": "I live near Toronto. It‚Äôs springtime, and currently about 30 ¬∞C. In my opinion, Toronto is too hot in the summer and too cold in the winter. I‚Äôd like to know which cities have the least deviation from a tolerable average temperature."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#tools",
    "href": "posts/city_temperatures/city_temperatures.html#tools",
    "title": "Cities with Nice Weather",
    "section": "Tools",
    "text": "Tools\nThis page was created using Quarto. I‚Äôm using the tidyverse for data wrangling, ggplot2 and ggExtra to plot."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#data",
    "href": "posts/city_temperatures/city_temperatures.html#data",
    "title": "Cities with Nice Weather",
    "section": "Data",
    "text": "Data\nFirst, I created a CSV file comprising all the information in the Wikpedia article List of cities by average temperature.\n\ncity_temps <-\n  read_csv(\"temps.csv\", show_col_types = FALSE)\n\nhead(city_temps)\n\n# A tibble: 6 √ó 15\n  Country City         Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct\n  <chr>   <chr>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Algeria Algiers     11.2  11.9  12.8  14.7  17.7  21.3  24.6  25.2  23.2  19.4\n2 Algeria Tamanrass‚Ä¶  12.8  15    18.1  22.2  26.1  28.9  28.7  28.2  26.5  22.4\n3 Algeria Reggane     16    18.2  23.1  27.9  32.2  36.4  39.8  38.4  35.5  29.2\n4 Angola  Luanda      26.7  28.5  28.6  28.2  27    23.9  22.1  22.1  23.5  25.2\n5 Benin   Cotonou     27.3  28.5  28.9  28.6  27.8  26.5  25.8  25.6  26    26.7\n6 Benin   Parakou     26.5  28.7  29.6  29    27.5  26.1  25.1  24.7  25    26.1\n# ‚Ä¶ with 3 more variables: Nov <dbl>, Dec <dbl>, Year <dbl>\n\n\nEach row corresponds to a distinct city. There are two text columns containing each city‚Äôs name and country, twelve numeric columns indicating the ‚Äúaverages of the daily highs and lows‚Äù1 for each month, and one additional numeric column containing the same figure for the entire year. The units are degrees Celsius. 455 cities are included in the data.\nWe‚Äôll define the ‚Äúdeviation‚Äù mentioned above as the difference between the value recorded for the coldest and hottest months, and the ‚Äúaverage‚Äù as the value recorded for the whole year overall.\nWe‚Äôll ignore any other weather characteristics like humidity, rain, wind, diurnal temperature difference, etc.2\n\ncity_temps <-\n  city_temps |>\n  rename_with(tolower) |>\n  rowwise() |>\n  transmute(city,\n            avg = year,\n            range = max(c_across(jan:dec)) - min(c_across(jan:dec)))\n\nhead(city_temps)\n\n# A tibble: 6 √ó 3\n# Rowwise: \n  city          avg range\n  <chr>       <dbl> <dbl>\n1 Algiers      17.4 14   \n2 Tamanrasset  21.7 16.1 \n3 Reggane      28.3 23.8 \n4 Luanda       25.8  6.5 \n5 Cotonou      27.2  3.30\n6 Parakou      26.8  4.9"
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#summary-statistics",
    "href": "posts/city_temperatures/city_temperatures.html#summary-statistics",
    "title": "Cities with Nice Weather",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nNow we can investigate the distribution of each of our two variables.\nHere are the default summaries:\n\ncity_temps |> \n  select(avg, range) |>\n  summarize()\n\n          N    Mean   SD     Min    Q1 Median    Q3  Max\n1   avg 455   18.00 8.12   -14.4 12.45   18.6 25.65 30.5\n2 range 455   13.75 9.53     0.7  5.65   12.1 21.00 58.1\n\n\nWhich cities correspond to the extremes for each variable?\n\ncity_temps |>\n  filter(\n    avg   %in% (city_temps |> pull(avg)   |> range())  || \n    range %in% (city_temps |> pull(range) |> range())) |>\n  arrange(avg)\n\n# A tibble: 4 √ó 3\n# Rowwise: \n  city         avg  range\n  <chr>      <dbl>  <dbl>\n1 Gjoa Haven -14.4 42    \n2 Yakutsk     -8.8 58.1  \n3 Honiara     26.5  0.700\n4 Assab       30.5  8.7  \n\n\nLet‚Äôs see the values for Toronto as a baseline, and save them for later:\n\ncity_temps |>\n  filter(city == \"Toronto\")\n\n# A tibble: 1 √ó 3\n# Rowwise: \n  city      avg range\n  <chr>   <dbl> <dbl>\n1 Toronto   9.4    26\n\ntoronto_avg <-\n  city_temps |>\n  filter(city == \"Toronto\") |>\n  pull(avg)\n\ntoronto_range <-\n  city_temps |>\n  filter(city == \"Toronto\") |>\n  pull(range)\n\nBy global standards, Toronto is cool on average, but in keeping with my subjective perception, the deviation from that average over the year is quite large."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#plots",
    "href": "posts/city_temperatures/city_temperatures.html#plots",
    "title": "Cities with Nice Weather",
    "section": "Plots",
    "text": "Plots\nLet‚Äôs look at a scatter plot with marginal histograms:\n\nplot <-\n  city_temps |>\n  ggplot(aes(x = avg, y = range)) +\n  geom_point(alpha = 0.33) +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  geom_hline(yintercept = toronto_range,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  labs(title = \"Average Temperature vs Range by City\",\n       x = \"Average Temperature (¬∞C)\",\n       y = \"Difference Between Hottest and Coldest Months (¬∞C)\") \n\nplot <-\n  plot |>\n  ggMarginal(type = \"histogram\", fill = \"transparent\", size = 10)\n\nplot\n\n\n\n\nHere Toronto is indicated by the dashed lines.\nWe can see there‚Äôs a negative association between a city‚Äôs average temperature and the range of temperatures experienced there. In particular, there‚Äôs a big cluster of very hot cities which have little difference between their hottest and coldest months.\nTen tropical cities fall into both the hottest decile and the least varying decile:\n\ncity_temps |> \n  filter(range < quantile(city_temps$range, 0.1),\n         avg   > quantile(city_temps$avg,   0.9)) |>\n  select(city)\n\n# A tibble: 10 √ó 1\n# Rowwise: \n   city         \n   <chr>        \n 1 Lodwar       \n 2 Palembang    \n 3 Pontianak    \n 4 Kuala Lumpur \n 5 Mal√©         \n 6 Lanka Colombo\n 7 Oranjestad   \n 8 Willemstad   \n 9 Panama City  \n10 Barranquilla \n\n\nWhile these cities see very little temperature variation throughout the year, they are much too hot."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#zooming-in",
    "href": "posts/city_temperatures/city_temperatures.html#zooming-in",
    "title": "Cities with Nice Weather",
    "section": "Zooming In",
    "text": "Zooming In\nThe area of this plot I‚Äôm most interested in is the vertical slice around Toronto. Let‚Äôs see the same plot, including only the cities within one degree of Toronto‚Äôs average temperature.3 We‚Äôll exclude the marginal histograms but add labels to the cities.\n\ncity_temps |>\n  filter(abs(avg - toronto_avg)<=1) |>\n  ggplot(aes(x = avg, y = range, label = city)) +\n  geom_point(alpha = 0.33) +\n  geom_text(size = 4, nudge_x = 0.01, hjust = \"left\") +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  geom_hline(yintercept = toronto_range,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  labs(title = \"Average Temperature vs Range by City (Detail 1)\",\n       x = \"Average Temperature (¬∞C)\",\n       y = \"Difference Between Hottest and Coldest Months (¬∞C)\")\n\n\n\n\nSo it seems that La Paz, Edinburgh, or Dublin might be good options.\nBut which cities are the best? These would be the ones with the smallest range for a given maximum average. Let‚Äôs find them."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#finding-the-cities-with-the-nicest-weather",
    "href": "posts/city_temperatures/city_temperatures.html#finding-the-cities-with-the-nicest-weather",
    "title": "Cities with Nice Weather",
    "section": "Finding the Cities with the Nicest Weather",
    "text": "Finding the Cities with the Nicest Weather\nWe want to know, for each maximum average temperature, the city that has the minimum range of temperatures. These are the cities that form the ‚Äúbottom-left edge‚Äù of our first plot.\nNine cities fit this criterion:\n\ncity_temps |>\n  arrange(avg) |>\n  cbind(city_temps |> arrange(avg) |> pull(range) |> cummin()) |>\n  rename(running_min = 4) |>\n  filter(range == running_min) |>\n  select(city)\n\n        city\n1 Gjoa Haven\n2     Dikson\n3       Nuuk\n4  Reykjav√≠k\n5    Stanley\n6     La Paz\n7      Cusco\n8     Bogot√°\n9    Honiara\n\n\nOf these, the first two have temperatures which are more variable than Toronto, so we can remove them from consideration.\nLet‚Äôs plot the final seven candidates:\n\ncity_temps |>\n  arrange(avg) |>\n  cbind(city_temps |> arrange(avg) |> pull(range) |> cummin()) |>\n  rename(running_min = 4) |>\n  filter(range == running_min) |>\n  select(-running_min) |>\n  filter(range <= toronto_range) |>\n  ggplot(aes(x = avg, y = range, label = city)) +\n  geom_point(alpha = 0.33) +\n  geom_text(size = 4, nudge_x = 0.5, hjust = \"left\") +\n  geom_vline(xintercept = toronto_avg,\n             linetype = \"dashed\",\n             alpha = 0.33) +\n  scale_x_continuous(expand = expansion(mult = 0.15)) +\n    labs(title = \"Average Temperature vs Range by City (Detail 2)\",\n         x = \"Average Temperature (¬∞C)\",\n         y = \"Difference Between Hottest and Coldest Months (¬∞C)\")\n\n\n\n\nAgain we see that La Paz has a similar overall average temperature to Toronto, but much less annual variability. Cusco and Bogot√° are warmer but even less variable.\nReykjav√≠k and Stanley are colder than Toronto, and while they represent a smaller decrease in variability compared to La Paz, Cusco, and Bogot√°, they have the benefit (for me) of being 98%+ English-speaking.\nNuuk and Honiara are right out."
  },
  {
    "objectID": "posts/city_temperatures/city_temperatures.html#next-steps",
    "href": "posts/city_temperatures/city_temperatures.html#next-steps",
    "title": "Cities with Nice Weather",
    "section": "Next Steps",
    "text": "Next Steps\nIt would be interesting to use detailed time series for each city and a utility function on temperatures (perhaps including wind chill and humidex) to determine which cities are truly mean-variance optimal.\nOf course, one should probably not choose a place to live based solely on the weather."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html",
    "title": "(Machine) Learning about Runes",
    "section": "",
    "text": "Today we‚Äôll learn about an ancient Germanic writing system, clean up some messy data, engineer a couple of simple features, and see if we can find a way to predict the age of some historical artifacts without having to pay for a fancy-schmancy archaeology degree. Along the way, we‚Äôll get a little overview of the tidyverse and tidymodels approaches to data analysis and modeling in R.\n\n\nA rune is a letter from any of a handful of closely-related alphabets used by Germanic peoples primarily from around the 3rd to the 13th centuries CE. Like the Latin alphabet used to write the Germanic languages today, they derive from an ancient form of the Greek alphabet.1 These alphabets are sometimes called ‚Äúfuthark‚Äù (or ‚Äúfuthorc‚Äù, ‚Äúfu√æƒÖrk‚Äù, etc.) after first few letters in their canonical order.2\nThe main runic alphabets are3:\n\n\n\n\n\n\n\n\n\nName\nMain Language(s)\nEra (c.¬†CE)\nLetters\n\n\n\n\nElder Futhark\nProto-Germanic, Proto-Norse\n1st‚Äì8th\n·ö† ·ö¢ ·ö¶ ·ö® ·ö± ·ö≤ ·ö∑ ·öπ ·ö∫ ·öæ ·õÅ ·õÉ ·õà ·õá ·õâ ·õä ·õè ·õí ·õñ ·õó ·õö\n\n\nAnglo-Frisian Futhorc\nOld Frisian, Old English\n5th‚Äì11th\n·ö† ·ö¢ ·ö¶ ·ö© ·ö± ·ö≥ ·ö∑ ·öπ ·öª ·öæ ·õÅ ·õ° ·õá ·õà ·õâ ·õã ·õè ·õí ·õñ ·õó ·õö ·õù ·õü ·õû ·ö™ ·ö´ ·õ† ·ö£\n\n\nYounger Futhark\nOld Norse\n8th‚Äì12th\n·ö† ·ö¢ ·ö¶ ·ö¨ ·ö± ·ö¥ ·öº ·öæ ·õÅ ·õÖ ·õ¶ ·õã ·õè ·õí ·õò ·õö\n\n\nMedieval Runes\nOld Icelandic, Old Swedish\n12th‚Äì17th\n·ö† ·ö¢ ·ö¶ ·öÆ ·ö± ·ö¥ ·öº ·öø ·õÅ ·õÜ ·õå ·õê ·õí ·õò ·õö ·õ¶\n\n\n\nMagical incantations were sometimes written in runes, but despite what such august institutions as the gift shop of the National Museum of Iceland would have you believe, there‚Äôs no evidence that each runic letter had a specific symbolic meaning.4 Neither is there much evidence that they were used in divination. These alphabets were used mainly for mundane purposes such as inscribing an object with its owner or creator‚Äôs name, and for memorializing the dead on gravestones. Nevertheless, in recent times these alphabets have become an element in various kinds of New Age mysticism, neo-pagan religions, and unsavoury political movements.\n\n\n\nRuneS-DB is a database of artifacts bearing runic writing compiled from a variety of sources as part of RuneS, an overarching project to study these writing systems. It‚Äôs available from the RuneS project‚Äôs website.\n\nThe rights to the data are held by the G√∂ttingen Academy of Sciences and Humanities and are subject to the CC-BY- SA law. The RuneS research units G√∂ttingen, Eichst√§tt-M√ºnchen and Kiel were involved in the generation of the data. RuneS-DB contains data from the Kiel Runenprojekt, the Danish database runer.ku.dk and the Samnordisk runtextdatabas/Rundata accessible under the Open Database License (ODbL). Please also note the additional information on other origin of the data provided under the label sources.\n\nRuneS provides a tool on their website to query the database, but it‚Äôs pretty clunky. Instead, we‚Äôll use R to see if we can learn anything interesting.\n\n\n\nWe‚Äôll use the tidyverse collection of packages for manipulating and visualizing our data and the tidymodels packages to quickly define, train, and compare a few machine learning models. These packages ‚Äúshare an underlying design philosophy, grammar, and data structures‚Äù rooted in the ‚Äútidy data‚Äù principles originally espoused by Hadley Wickham.\nThis document was lovingly hand-crafted in artisanal, farm-to-table, GMO-free Quarto."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#reading-and-cleaning-the-data",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#reading-and-cleaning-the-data",
    "title": "(Machine) Learning about Runes",
    "section": "Reading and Cleaning the Data",
    "text": "Reading and Cleaning the Data\n\nReading the File\nThe file we obtain from the RuneS website is tab-separated, contains three lines of preamble, and uses a hyphen to indicate missing data. Providing this information to readr‚Äôs read_delim will produce a tibble (i.e., a fancy data frame, which is in turn a fancy matrix) that we can begin to manipulate.\n\n# read the file\nread_delim(file = \"runes_data.csv\",\n           delim = \";\",\n           skip = 3,\n           na = \"-\",\n           show_col_types = FALSE) ->\n  # save to a dataframe\n  runes_data\n\n\n\nThe Vimose Comb\nOne artifact we‚Äôd expect to find in RuneS-DB is the Vimose comb, which bears what‚Äôs considered to be the oldest known datable5 runic inscription. dplyr‚Äôs filter function and stringr‚Äôs str_detect let us filter the data to show only rows where value in the column of inscription names matches ‚ÄúVimose comb‚Äù.6\n\nrunes_data |>\n  filter(str_detect(rs_short_inscr_name, pattern = \"Vimose comb\")) |>\n  kbl() |>\n  scroll_box(width = \"51.75%\")\n\n\n\n \n  \n    Findno \n    rs_short_inscr_name \n    rs_fundort \n    rs_short_storage \n    rs_extdat \n    rs_short_dat_art \n    rs_short_context \n    rs_objklasse \n    rs_objtyp \n    rs_short_obj_complete \n    rs_short_matklasse \n    rs_mat \n    rs_short_obj_state \n    rs_runenreihe \n    rs_short_museum \n    rs_short_ins_complete \n    rs_translat \n    rs_translit \n    rs_short_ins_state \n    rs_short_markings \n    rs_short_transkript \n    rs_namen \n    rs_fundjahr \n    rs_short_gemeinde \n    rs_short_bezirk \n    rs_short_landschaft \n    rs_short_land \n    rs_short_invnr \n    rs_short_kategorie \n    rs_short_sigils \n    rs_traeger \n  \n \n\n  \n    20 \n    Vimose comb \n    Vimose \n    Museum \n    140-160 \n    arch. \n    deposit find \n    tool/implement \n    personal hygiene \n    yes \n    bone/horn \n    antler \n    good \n    older fu√æark \n    Nationalmuseet \n    yes \n    Harja. \n    harja | \n    good \n    nein \n    NA \n    Vimose-kam \n    1865 \n    Allese Sogn \n    Odense Amt \n    Fyn \n    DK \n    22657 \n    run. \n    Fyn 19 \n    comb \n  \n\n\n\n\n\nSo we can see that this object was found in Vimose, currently resides in a museum, has been dated to 140-160 CE, is a personal hygiene tool made of antler, says ‚Äúharja‚Äù (·ö∫·ö®·ö±·õÉ·ö®), etc.\n\n\nDiagnosing\nOne way to quantify the messiness of a data set is to calculate the count and proportion of missing and unique values in each column. dlookr‚Äôs diagnose does this.\n\nrunes_data |> \n  diagnose() |>\n  kbl() |>\n  scroll_box(width = \"51.75%\")\n\n\n\n \n  \n    variables \n    types \n    missing_count \n    missing_percent \n    unique_count \n    unique_rate \n  \n \n\n  \n    Findno \n    numeric \n    0 \n    0.0000000 \n    8281 \n    1.0000000 \n  \n  \n    rs_short_inscr_name \n    character \n    7 \n    0.0845309 \n    5457 \n    0.6589784 \n  \n  \n    rs_fundort \n    character \n    34 \n    0.4105784 \n    3156 \n    0.3811134 \n  \n  \n    rs_short_storage \n    character \n    0 \n    0.0000000 \n    1723 \n    0.2080667 \n  \n  \n    rs_extdat \n    character \n    6 \n    0.0724550 \n    573 \n    0.0691945 \n  \n  \n    rs_short_dat_art \n    character \n    1988 \n    24.0067625 \n    19 \n    0.0022944 \n  \n  \n    rs_short_context \n    character \n    5240 \n    63.2773820 \n    21 \n    0.0025359 \n  \n  \n    rs_objklasse \n    character \n    33 \n    0.3985026 \n    13 \n    0.0015699 \n  \n  \n    rs_objtyp \n    character \n    1452 \n    17.5341142 \n    45 \n    0.0054341 \n  \n  \n    rs_short_obj_complete \n    character \n    1873 \n    22.6180413 \n    4 \n    0.0004830 \n  \n  \n    rs_short_matklasse \n    character \n    43 \n    0.5192610 \n    11 \n    0.0013283 \n  \n  \n    rs_mat \n    character \n    3550 \n    42.8692187 \n    119 \n    0.0143702 \n  \n  \n    rs_short_obj_state \n    character \n    1970 \n    23.7893974 \n    6 \n    0.0007246 \n  \n  \n    rs_runenreihe \n    character \n    463 \n    5.5911122 \n    12 \n    0.0014491 \n  \n  \n    rs_short_museum \n    character \n    4382 \n    52.9163145 \n    291 \n    0.0351407 \n  \n  \n    rs_short_ins_complete \n    character \n    1259 \n    15.2034778 \n    4 \n    0.0004830 \n  \n  \n    rs_translat \n    character \n    698 \n    8.4289337 \n    5592 \n    0.6752808 \n  \n  \n    rs_translit \n    character \n    91 \n    1.0989011 \n    7482 \n    0.9035141 \n  \n  \n    rs_short_ins_state \n    character \n    1371 \n    16.5559715 \n    6 \n    0.0007246 \n  \n  \n    rs_short_markings \n    character \n    2676 \n    32.3149378 \n    3 \n    0.0003623 \n  \n  \n    rs_short_transkript \n    character \n    1090 \n    13.1626615 \n    5536 \n    0.6685183 \n  \n  \n    rs_namen \n    character \n    5662 \n    68.3733849 \n    1554 \n    0.1876585 \n  \n  \n    rs_fundjahr \n    character \n    4895 \n    59.1112185 \n    700 \n    0.0845309 \n  \n  \n    rs_short_gemeinde \n    character \n    1435 \n    17.3288250 \n    1682 \n    0.2031156 \n  \n  \n    rs_short_bezirk \n    character \n    3287 \n    39.6932738 \n    406 \n    0.0490279 \n  \n  \n    rs_short_landschaft \n    character \n    1384 \n    16.7129574 \n    124 \n    0.0149740 \n  \n  \n    rs_short_land \n    character \n    48 \n    0.5796401 \n    28 \n    0.0033812 \n  \n  \n    rs_short_invnr \n    character \n    4709 \n    56.8651129 \n    2524 \n    0.3047941 \n  \n  \n    rs_short_kategorie \n    character \n    1227 \n    14.8170511 \n    15 \n    0.0018114 \n  \n  \n    rs_short_sigils \n    character \n    125 \n    1.5094795 \n    8129 \n    0.9816447 \n  \n  \n    rs_traeger \n    character \n    8 \n    0.0966067 \n    416 \n    0.0502355 \n  \n\n\n\n\n\nUnfortunately, RuneS-DB is a bit of a mess. Because it‚Äôs amalgamated from many sources, information is coded inconsistently. A mix of languages are used, including in the column names. Getting this data set clean enough to start visualizing it is easy enough using dplyr functions like rename(), separate(), and mutate().\nThe first step is to make the column names more informative and get rid of the weird ‚Äúrs_‚Äù and ‚Äúrs_short_‚Äù prefixes.\n\nrunes_data |>\n  # rename the columns\n  rename(\n    find_number          = Findno,\n    inscription_name     = rs_short_inscr_name,\n    location             = rs_short_storage,\n    date                 = rs_extdat,\n    dating_method        = rs_short_dat_art,\n    context              = rs_short_context,\n    findspot             = rs_fundort,\n    object_class         = rs_objklasse,\n    object_type          = rs_objtyp,\n    object_complete      = rs_short_obj_complete,\n    material_class       = rs_short_matklasse,\n    material             = rs_mat,\n    object_state         = rs_short_obj_state,\n    writing_system       = rs_runenreihe,\n    museum               = rs_short_museum,\n    inscription_complete = rs_short_ins_complete,\n    translation          = rs_translat,\n    transliteration      = rs_translit,\n    inscription_state    = rs_short_ins_state,\n    markings             = rs_short_markings,\n    transcription        = rs_short_transkript,\n    names                = rs_namen,\n    find_year            = rs_fundjahr,\n    community            = rs_short_gemeinde,\n    district             = rs_short_bezirk,\n    region               = rs_short_landschaft,\n    country              = rs_short_land,\n    inventory_number     = rs_short_invnr,\n    category             = rs_short_kategorie,\n    shelf_marks          = rs_short_sigils,\n    carrier              = rs_traeger\n  ) ->\n  # update the dataframe\n  runes_data\n\nNext we‚Äôll separate the location column into two columns, one with the general category and the other with the rest of the information. We‚Äôll also create separate columns for the lower and upper bounds of each date, then convert this into a midpoint and a range.\n\nrunes_data |>\n  # separate location category and detail\n  separate(\n    location,\n    into = c(\"location_class\", \"location_detail\"),\n    sep = \":\",\n    extra = \"merge\") |>\n  # separate the date column into lower and upper bounds\n  separate(date, into = c(\"date_lower\", \"date_upper\")) |>\n  mutate(\n    # set dates to NA for undated objects\n    across(starts_with(\"date\"), ~ na_if(., \"0\")),\n    # treat the dates as numbers\n    across(starts_with(\"date\"), ~ as.numeric(.)),\n    # get the middle of each date range\n    date = (date_lower + date_upper) / 2,\n    # get the range of each date\n    date_range = date_upper - date_lower) |>\n  # discard the date bounds\n  select(-c(date_upper, date_lower)) ->\n  # update the dataframe again\n  runes_data\n\nNext let‚Äôs convert RuneS-DB‚Äôs somewhat idiosyncratic country codes into human-readable names. While we‚Äôre at it, we‚Äôll collapse the rarest ones into a ‚ÄúRest of World‚Äù category.\n\nrunes_data |>\n  mutate(\n    country = recode(\n      country,\n      \"S\"  = \"Sweden\",\n      \"N\"  = \"Norway\",\n      \"DK\" = \"Denmark\",\n      \"IS\" = \"Iceland\",\n      \"GB\" = \"Great Britain\",\n      \"D\"  = \"Germany\",\n      \"KN\" = \"Greenland\",\n      .default = \"Rest of World\")) ->\n  runes_data\n\nFinally some miscellaneous re-encoding.\n\nrunes_data |>\n  mutate(\n    # treat the ID numbers as strings\n    find_number = as.character(find_number),\n    # eliminate excess whitespace\n    across(where(is.character), str_squish),\n    # translate the markings column into English\n    markings = recode(markings, \"ja\" = \"yes\", \"nein\" = \"no\"),\n    # replace cross symbol\n    across(where(is.character), ~ recode(., \"‚Ä†\" = \"lost/destroyed\")),\n    # replace \"rune stick/r√∫nakefli\", shorten \"weapon/weapon accessories\"\n    object_class = recode(\n      iconv(object_class, to = 'ASCII//TRANSLIT'),\n      \"rune stick/runakefli\" = \"rune stick\",\n      \"weapon/weapon accessories\" = \"weapon/accessory\"),\n    # make \"Museum\" lowercase for consistency\n    location_class = recode(location_class, \"Museum\" = \"museum\"),\n    # combine parchment and paper into a single material class\n    material_class = recode(\n      material_class,\n      \"parchment; paper\" = \"parchment/paper\",\n      \"parchment\" = \"parchment/paper\",\n      \"paper\" = \"parchment/paper\"),\n    # give the writing systems friendlier names, combine medieval and post-Reformation runes as \"manuscript runes\"\n    writing_system = recode(\n      iconv(writing_system, to = 'ASCII//TRANSLIT'),\n      \"older fu?ark\" = \"elder futhark\",\n      \"younger fu?ark\" = \"younger futhark\",\n      \"fu?orc\" = \"anglo-frisian futhorc\",\n      \"post-Reformation runes\" = \"manuscript runes\",\n      \"medieval runes\" = \"manuscript runes\",\n      .default = \"mixed/unknown\")) ->\n  runes_data\n\nThere‚Äôs more cleaning we could do, but this is enough for now."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#exploratory-data-analysis",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#exploratory-data-analysis",
    "title": "(Machine) Learning about Runes",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nrunes_data |>\n  drop_na(object_class, material_class) |>\n  group_by(object_class, material_class) |>\n  summarise(count = n()) |>\n  ggplot(aes(x = object_class, y = material_class, size = count)) +\n  geom_point(shape = 21, fill = \"#595959\") +\n  coord_flip() +\n  scale_size(range = c(0, 10)) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Runic Artifacts by Object and Material Class\",\n    x = \"Object Class\",\n    y = \"Material Class\")\n\n\n\n\n\n\n\n\nNearly half of the artifacts in RuneS-DB are stones7, which it turns out are always made of stone! Likewise, coins and bracteates8 are always made of metal, and rune sticks are always made of wood. Tools, edifices, ‚Äúinventory‚Äù (mostly furniture), and ‚Äúother objects‚Äù all seem to come in a variety of materials.\nWhere is this stuff typically found?\n\nrunes_data |>\n  # discard artifacts missing country information\n  drop_na(country) |>\n  ggplot(aes(fct_infreq(country))) +\n  geom_bar() +\n  geom_hline(yintercept = seq(0, 4000, 1000), color = \"white\") +\n  coord_flip() +\n  labs(\n    title = \"Runic Objects by Country\",\n    x = \"Country of Discovery\",\n    y = \"Number of Objects\")\n\n\n\n\n\n\n\n\nRight. Scandinavia, mostly.\nOne last exploratory graphic. How old is this stuff?\n\nrunes_data |>\n  # discard undated objects\n  drop_na(date) |>\n  ggplot(aes(date)) +\n  # bin into centuries\n  geom_histogram(\n    fill = \"#595959\", \n    color = \"white\", \n    breaks = seq(1, 2022, by=100)) +\n  geom_hline(yintercept = seq(0, 3000, 1000), color = \"white\") +\n  scale_x_continuous(breaks = seq(0, 2000, by = 100)) +\n  labs(\n    title = \"Number of Runic Objects by Century\",\n    x = \"Year of Manufacture\",\n    y = \"Number of Objects\")\n\n\n\n\n\n\n\n\nThis histogram has a huge spike at the 10th century. Upon cursory investigation, this seems to be because the data set contains many rune stones with dated to the rather wide range of ‚Äú725-1100‚Äù, i.e.¬†the Viking Age. Obviously this data set is not random sample of all runic objects ever produced; some things are more likely to survive and be cataloged than others. The smaller spike at the 13th century seems to be more organic."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#feature-engineering",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#feature-engineering",
    "title": "(Machine) Learning about Runes",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nSuppose you‚Äôre lost in the woods in northwestern Europe9 and stumble upon some runes. Is there some rule you can use to estimate when they were carved (or written)?\nFirst, we need to consider what features of the object you‚Äôd be able to determine. I think these are all reasonable:\n\nobject class (stone, coin, etc.)\nmaterial class (stone, metal, wood, etc.)\ncountry\nwriting system (elder futhark, anglo-frisian futhorc, etc.)\nlength of the inscription\nwhether the inscription contains an abecedarium; something like ‚Äúfu√æarkgw‚Ä¶‚Äù or ‚Äúabcdefgh‚Ä¶‚Äù\nwhether the inscription seems to contain any of a few common words or morphemes\n\nThe first four of these features are already present in our data frame. We‚Äôll have to ‚Äúengineer‚Äù the others.\nUnfortunately, the transliterations in RuneS-DB are very inconsistently encoded, but we can still get an approximate length of each inscription by converting the transliteration to ASCII10 and counting the number of resulting alphanumeric characters11. It‚Äôs too bad the database doesn‚Äôt simply use the runic characters included in Unicode, but perhaps that wouldn‚Äôt be sufficient to encode parts of the inscriptions which are unclear, damaged, combined into ligatures (so-called ‚Äúbind runes‚Äù), etc.\n\nrunes_data |>\n  mutate(\n    inscription_length =\n      transliteration |>\n      # convert to ASCII\n      iconv(to = \"ASCII//TRANSLIT\") |>\n      # count alphanumeric characters\n      str_count(\"[[:lower:]\\\\?]\")) ->\n  runes_data\n\nWe can identify abecedaria by slightly re-encoding the ‚Äúcategory‚Äù column.\n\nrunes_data |>\n  mutate(\n    abecedarium = case_when(\n      str_detect(category, \"alphabet\") ~ \"abc\",\n      str_detect(category, \"row\") ~ \"futh\",\n      TRUE ~ \"no\")) ->\n  runes_data\n\nAnother feature we can derive from the inscription is what kind of spacing is used between words.\n\nrunes_data |>\n  mutate(\n    spaces = case_when(\n      str_detect(transliteration, \" [√∑\\\\*\\\\+] \") ~ \"crosses/stars\",\n      str_detect(transliteration, \" √ó \") ~ \"crosses/stars\",\n      str_detect(transliteration, \" [Àà\\\\.] \") ~ \"single dots\",\n      str_detect(transliteration, \" : \") ~ \"double dots\",\n      TRUE ~ \"none/other\")) ->\n  runes_data\n\nFinally let‚Äôs encode the presence or absence of a few of the most common words/morphemes.\n\nrunes_data |>\n  mutate(\n    sin_sun_syn = str_detect(transliteration, \"sin|sun|syn\"),\n    auk_uk_ok   = str_detect(transliteration, \"auk|uk|ok\"),\n    at          = str_detect(transliteration, \"at\"),\n#   fathur      = str_detect(transliteration, \"fa√æur\"),\n    stain_stin  = str_detect(transliteration, \"stain|stin|st”ïin\"),\n    lit         = str_detect(transliteration, \"lit\"),\n    across(sin_sun_syn:lit, as.numeric)) ->\n  runes_data\n\nIs this enough information to be able to predict the age of a runic inscription with any accuracy? Let‚Äôs try fitting a few different models using different approaches."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#model-fitting",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#model-fitting",
    "title": "(Machine) Learning about Runes",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nTest/Training Data Split\nFirst we‚Äôll split the data into a training and a test set, then create cross-validation folds from training data to help estimate model performance.\n\nrunes_data |>\n  # keep just the features we want to predict from\n  select(\n    object_class, \n    material_class, \n    country, \n    writing_system, \n    inscription_length,\n    abecedarium,\n    spaces,\n    sin_sun_syn:lit,\n    date) |>\n  # discard objects with missing data\n  na.omit() ->\n  runes_data\n\n# split into training (75%) and test (25%) sets, stratified by date\ninitial_split(\n    data = runes_data, \n    prop = 0.75, \n    strata = date) ->\n  split\n\n# store a copy of each set\ntraining(split) -> train\ntesting(split) -> test\n\n# create 10 cross-validation folds\nvfold_cv(train) -> folds\n\nThe tidymodels framework provides a unified interface to various model-specific packages, as well as convenient functions for defining, fitting, tuning, and comparing many combinations of data pre-processing recipes and model specifications.\n\nIn parsnip, the model type differentiates basic modeling approaches, such as random forests, logistic regression, linear support vector machines, etc.; the mode denotes in what kind of modeling context it will be used (most commonly, classification or regression); and the computational engine indicates how the model is fit, such as with a specific R package implementation or even methods outside of R like Keras or Stan.\n\n\n\nPre-Processing Recipe\nNext we‚Äôll define two pre-processing recipes. In both cases we‚Äôll normalize our numeric predictor. Some model types require categorical predictors to be dummy-encoded, while others can exhibit better performance with categorical predictors left as-is. We‚Äôll try both ways.\n\n# create a pre-processing recipe\nrecipe(runes_data) |>\n  update_role(date, new_role = \"outcome\") |>\n  update_role(1:(ncol(runes_data) - 1), new_role = \"predictor\") |>\n  # normalize the numeric feature\n  step_normalize(all_numeric_predictors()) |>\n  # dummy encode the categorical features\n  step_dummy(all_nominal_predictors()) ->\n  runes_recipe\n\n\n\nBaseline\nNow we can define our models. First let‚Äôs ‚Äúfit‚Äù the null model, which consists of just always guessing the mean date value from the training set. It‚Äôs straightforward to simply calculate the appropriate RMSE estimate in this case, but for illustrative purposes we‚Äôll use cross-validation anyway. It doesn‚Äôt matter which recipe we use since the null model ignores the predictors anyway.\n\nnull_model() |>\n  set_engine(\"parsnip\") |>\n  set_mode(\"regression\") ->\n  null_spec\n\nworkflow() |>\n  add_model(null_spec) |>\n  add_recipe(runes_recipe) |>\n  fit_resamples(resamples = folds, metrics = metric_set(rmse)) |>\n  collect_metrics() |>\n  pull(mean)\n\n[1] 311.4979\n\n\nThe null model‚Äôs prediction is, in a certain sense, off by more than three centuries on average. Surely we can do better than that.\n\n\nThe Bias-Variance Trade-off\n\n\nModel Specification\nWe‚Äôll try six more kinds of model. Each comes with some hyperparameters which control the bias-variance tradeoff, the step size for gradient descent, etc. We can leave tune() as a placeholder for these values when creating the model specifications. When we fit the models, we‚Äôll try 10 combinations of hyperparameter values for each model type, and keep only the best ones.\nA linear model assumes the outcome is a linear function of the predictors and finds the best coefficient to assign to each. Linear models are inflexible, and so tend to suffer from bias (unless the underlying relationship really is approximately linear), but they tend to have lower variance than more flexible model types, since their outputs are not too sensitive to small changes in their inputs. The penalty and mixture hyperparameters here control how much L1 (LASSO) and/or L2 (ridge) regularization to apply. Regularization penalizes more complex models in order to prevent overfitting.\n\nlinear_reg(\n  engine  = \"glmnet\",\n  penalty = tune(),\n  mixture = tune()) ->\n  linear_spec\n\nA decision tree is essentially a flowchart, with each split corresponding to a rule in the form of an ‚Äúif/then‚Äù condition on a predictor. The idea is to find the splits which best separate the outcome. Predictions are produced by taking the average outcome among the training data belonging to the relevant terminal node of the tree. tree_depth specifies the maximum depth of the tree; without some maximum, the training data could be completely interpolated (or ‚Äúmemorized‚Äù), an extreme form of overfitting. cost_complexity controls how well a split must separate its subset of the training data in order to be considered, and min_n controls how much training data must belong to a node in order to justify any further splitting.\n\ndecision_tree(engine     = \"partykit\",\n              tree_depth = tune(),\n              min_n      = tune()) ->\n  decision_tree_spec\n\nNearest neighbours models predict that the value of the outcome for a test data point will be some kind of weighted average of that point‚Äôs nearest neighbours in the training data. neighbors controls the number of neighbours to consider and the other hyperparameters specify the precise notions of ‚Äúweighted average‚Äù and ‚Äúnearest‚Äù to use.\n\nnearest_neighbor(engine      = \"kknn\",\n                 neighbors   = tune(),\n                 weight_func = tune(),\n                 dist_power  = tune()) ->\n  nearest_neighbours_spec\n\nBoosted tree ensembles are a very popular machine learning approach involving fitting many small decision trees, each of which is optimized to improve the predictions obtained by combining the preceding trees. This model type inherents the hyperparameters involved in fitting decision trees, plus additional hyperparameters specifying the number of trees to use, the proportion of training data and number of predictors to consider at each step during fitting, as well as how much weight to initially apply to each new tree.\n\nboost_tree(engine         = \"xgboost\",\n           trees          = 1000,\n           tree_depth     = tune(),\n           min_n          = tune(),\n           loss_reduction = tune(),\n           sample_size    = tune(),\n           mtry           = tune(),\n           learn_rate     = tune()) ->\n  boosted_trees_spec\n\nSome approaches combine multiple other types of models. Cubist involves a tree ensemble with linear models fit on each tree node, a boosting-like procedure, and a final nearest-neighbours-based adjustment.\n\ncubist_rules(engine = \"Cubist\",\n             committees = tune(),\n             neighbors = tune()) ->\n  cubist_spec\n\n\n\nModel Fitting\nIn tidymodels, a ‚Äúworkflow‚Äù is an object which bundles together a model specification together with any associated pre-processing recipes, hyperparameter values, and/or evaluation results. workflow_set and workflow_map allow us to tune all of our model specifications as a batch.\n\n# combine the model specifications in a list\nlist(linear        = linear_spec,\n     tree          = decision_tree_spec,\n     nn            = nearest_neighbours_spec,\n     boosted_trees = boosted_trees_spec,\n     cubist        = cubist_spec) ->\n  runes_model_specs\n\n# set the prediction mode of each engine to \"regression\"\nrunes_model_specs |>\n  map(~set_mode(., \"regression\")) ->\n  runes_model_specs\n\n# combine pre-processing recipe and model specifications into a workflow set\nworkflow_set(preproc = list(recipe = runes_recipe),\n             models = runes_model_specs) ->\n  runes_workflow_set\n\nrunes_workflow_set |>\n  # for each model specification, try ten combinations of tuning parameters\n  # and estimate rmse using cross-validation\n  workflow_map(\"tune_grid\",\n               resamples = folds,\n               grid = 10,\n               metrics = metric_set(rmse)) ->\n  # update the workflow set with the results\n  runes_workflow_set\n\n\n\nPerformance Comparison\nNow our workflow set contains ten fit models per model type for each of ten hyperparameter combinations. We can extract the best version of each model type and plot the cross-validated performance estimates.\n\n# plot the rmse estimate from the best iteration of each type of model\nrank_results(runes_workflow_set, select_best = TRUE) |>\n  group_by(model) |>\n  slice_max(mean) |>\n  select(mean, std_err, model) |>\n  rename(rmse_mean = mean, rmse_std_err = std_err) |>\n  ggplot(aes(x = fct_reorder(model, -desc(rmse_mean)), y = rmse_mean)) +\n  geom_errorbar(aes(ymin = rmse_mean - rmse_std_err,\n                    ymax = rmse_mean + rmse_std_err),\n                width = 0.1,\n                size = 1.5,\n                color = \"#595959\") +\n  labs(x = \"Model Type\",\n       y = \"Estimated RMSE\",\n       title = \"Estimated RMSE for Best Model of Each Type\")\n\n\n\n\n\n\n\n\nIt appears that Cubist and boosted trees models work best for these data. Let‚Äôs finalize a Cubist model by re-fitting the best one on the entire training set, and seeing how well it predicts the age of artifacts in the test set.\n\nrunes_workflow_set |>\n  # get the cubist workflow\n  extract_workflow(\"recipe_cubist\") |>\n  # get the best cubist hyperparameters and apply them to the workflow\n  finalize_workflow(\n    runes_workflow_set |>\n      extract_workflow_set_result(\"recipe_cubist\") |>\n      select_best(metric = \"rmse\")) |>\n  # fit on the entire training set\n  last_fit(split, metrics = metric_set(rmse)) ->\n  final_cubist\n\n\n\nThe Performance-Explainability Trade-off\nUnfortunately, our finalized Cubist model is useless. Since we‚Äôre lost in the woods, we can‚Äôt actually compute a prediction involving a complicated collection of trees of linear models with hundreds or thousands of coefficients and weights in total.\nCan we find a simple decision tree with comparable performance by trying more hyperparameter combinations? Let‚Äôs set tree_depth to 3, find good values for the other hyperparameters, then finalize and evaluate the resulting decision tree model.\n\ndecision_tree_spec |>\n  set_args(tree_depth = 3) |>\n  set_mode(\"regression\") ->\n  decision_tree_spec\n\nworkflow() |>\n  add_model(decision_tree_spec) |>\n  add_recipe(runes_recipe) |>\n  # try 100 combinations of cost_complexity and min_n\n  tune_grid(resamples = folds, metrics = metric_set(rmse), grid = 100) |>\n  # keep the best ones\n  select_best(metric = \"rmse\") |>\n  # plug them back into the model specification\n  finalize_workflow(\n    workflow() |> \n      add_model(decision_tree_spec) |>\n      add_recipe(runes_recipe),\n    parameters = _) |>\n  # fit on the entire training set\n  last_fit(split, metrics = metric_set(rmse)) ->\n  small_tree\n\nNow we have two finalized models: a Cubist model and a small decision tree. The most obvious way to compare them would be to inspect their performance, but we can also use the vip package to extract the number of features used by each model, giving us a way to compare their relative complexity as well.\n\ndata.frame(\n  model    = c(\"Cubist\", \"Small Decision Tree\"),\n  rmse     = c(final_cubist |> collect_metrics() |> pull(.estimate), \n               small_tree   |> collect_metrics() |> pull(.estimate)),\n  features = c(final_cubist |> extract_fit_engine() |> vi() |> filter(Importance > 0) |> nrow(),\n               small_tree   |> extract_fit_engine() |> vi() |> filter(Importance > 0) |> nrow())) |>\n  kbl() |>\n  scroll_box(width = \"51.75%\")\n\n\n\n \n  \n    model \n    rmse \n    features \n  \n \n\n  \n    Cubist \n    109.7088 \n    38 \n  \n  \n    Small Decision Tree \n    131.1698 \n    7 \n  \n\n\n\n\n\nThe small decision tree‚Äôs predictions are about 16% worse than the best Cubist model we could find. On the other hand, it‚Äôs much simpler. While the Cubist model uses essentially all of the information we provided to it, since we constrained our decision tree to three levels, it can use only a maximum of seven features.12 Although the more complicated Cubist performs well, it‚Äôs difficult to explain exactly why, or what the role of each feature is in generating predictions. This illustrates the performance-explainability trade-off.\nOur small tree model is simple enough to write on an index card and keep with us when venturing out into the forests of rural Scandinavia. Here it is:\n\nsmall_tree |>\n  extract_fit_engine() |>\n  as.simpleparty() |>\n  plot(\n    ip_args = list(pval = FALSE, id = FALSE),\n    tp_args = list(\n      id = FALSE, \n      FUN = function(node){round(node$prediction[1])}))"
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#conclusion",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#conclusion",
    "title": "(Machine) Learning about Runes",
    "section": "Conclusion",
    "text": "Conclusion\nIf you can foresee yourself desperately needing to know the approximate age of a runic inscription, I recommend you write down the decision tree above and keep it in your pocket. That, or always bring a licensed and qualified runologist13 with you."
  },
  {
    "objectID": "posts/machine_learning_about_runes/machine_learning_about_runes.html#references",
    "href": "posts/machine_learning_about_runes/machine_learning_about_runes.html#references",
    "title": "(Machine) Learning about Runes",
    "section": "References",
    "text": "References\n\nRunes\n\nRunes and their Origin: Denmark and Elsewhere (Moltke, 1985)\nRunes (Findell, 2014)\nFuthark Journal\n\n\n\nStatistical Inference and Machine Learning\n\nElements of Statistical Learning (Friedman, et al., 2001)\n\n\n\nData Science in R\n\nTidy Data (Wickham, 2014)\nR for Data Science (Grolemund & Wickham, 2016)\nTidy Modeling with R (Kuhn & Silge, forthcoming)"
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html",
    "href": "posts/coryat_scores/coryat_scores.html",
    "title": "Coryat Scores",
    "section": "",
    "text": "The Coryat score is a way of measuring one‚Äôs performance when playing along with Jeopardy! at home. It is named after musician, philosopher of physics, and two-day Jeopardy! champion Karl Coryat.\nA player‚Äôs Coryat score is the total value of clues answered correctly, minus that of those answered incorrectly, counting correctly-answered Daily Doubles according to their board position and ignoring Final Jeopardy! and any incorrectly-answered Daily Doubles.\nThus the Coryat score is a measure of one‚Äôs knowledge of the trivia material used on the show, ignoring other strategic elements like wagering."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#histogram",
    "href": "posts/coryat_scores/coryat_scores.html#histogram",
    "title": "Coryat Scores",
    "section": "Histogram",
    "text": "Histogram\n\ncoryat_scores |>\n  ggplot(aes(x = score)) +\n  geom_histogram(fill = \"#8D2AB5\") +\n  geom_hline(yintercept = 1:15, col = \"white\") +\n  geom_vline(xintercept = seq(500,29500,1000), col = \"white\") +\n  scale_x_continuous(labels=scales::label_dollar()) +\n  theme_bw() +\n  labs(title = \"Distribution of Coryat Scores\", \n       x = \"Score\", \n       y = \"Count of Games\")"
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#line-chart",
    "href": "posts/coryat_scores/coryat_scores.html#line-chart",
    "title": "Coryat Scores",
    "section": "Line Chart",
    "text": "Line Chart\n\ncoryat_scores |>\n  ggplot(aes(x = game, y = score)) +\n  geom_point(color = \"#8D2AB5\") +\n  geom_line(color = \"#8D2AB5\", linetype = \"dotted\") +\n  ylim(0, 30000) +\n  scale_y_continuous(labels=scales::label_dollar()) +\n  theme_bw() +\n  labs(title = \"Coryat Score Trend\", x = \"Time\", y = \"Score\")"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html",
    "href": "posts/crossword_times/crossword_times.html",
    "title": "Crossword Times",
    "section": "",
    "text": "Down For Across is a website which allows you to solve user-uploaded crosswords solo or with friends.\nThe crosswords published in major American newspapers often differ in difficulty according to the day of the week. For example, the New York Times‚Äô guide explains that their Monday puzzles are the easiest and their Saturday puzzles the hardest. Meanwhile, the Sunday puzzles have easy clues but a large grid, and the Thursday puzzles have some unique gimmick. By contrast, the New Yorker‚Äôs crosswords decrease in difficulty over the week.\nI‚Äôve started keeping track of my solo completion times for the New York Times, Los Angeles Times, and New Yorker crosswords."
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#boxplot",
    "href": "posts/crossword_times/crossword_times.html#boxplot",
    "title": "Crossword Times",
    "section": "Boxplot",
    "text": "Boxplot\nlubridate provides convenient date and time parsing functions like ms. weekdays(as.Date('1970-01-01') + 4:10) is a nice trick to produce a character vector with the names of the week, beginning with Monday.12\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |>\n  ggplot(aes(x = factor(weekdays(Date), \n                        weekdays(as.Date('1970-01-01') + 4:10)),\n             y = period_to_seconds(ms(Time))/60, \n             fill = Publisher)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  facet_wrap(~Publisher) +\n  guides(fill = FALSE) + \n  labs(title = \"Crossword Completion Times\", \n           x = \"Day of the Week\",\n           y = \"Completion Time (Minutes, Log Scale)\")\n\n\n\n\n\nTab 1Tab 2"
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#average-score",
    "href": "posts/coryat_scores/coryat_scores.html#average-score",
    "title": "Coryat Scores",
    "section": "Average Score",
    "text": "Average Score\nAn average score of around $25,000 is considered appropriate for prospective contestants.\n\ncoryat_scores |>\n  pull(score) |>\n  mean() |>\n  round()\n\n[1] 19200\n\n\nClearly, I have some studying to do before I consider trying to compete on the show."
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#visualizing-scores",
    "href": "posts/coryat_scores/coryat_scores.html#visualizing-scores",
    "title": "Coryat Scores",
    "section": "Visualizing Scores",
    "text": "Visualizing Scores\nWe can create a histogram showing the distribution of my scores and a line chart showing the evolution of my scores over time using ggplot2.\n\nHistogram\n\ncoryat_scores |>\n  ggplot(aes(x = score)) +\n  geom_histogram(fill = \"#8D2AB5\") +\n  geom_hline(yintercept = 1:15, col = \"white\") +\n  geom_vline(xintercept = seq(500, 29500, 1000), col = \"white\") +\n  scale_x_continuous(labels=scales::label_dollar()) +\n  theme_bw() +\n  labs(title = \"Distribution of Coryat Scores\", \n       x = \"Score\", \n       y = \"Count of Games\")\n\n\n\n\n\n\n\n\n\n\nLine Chart\n\ncoryat_scores |>\n  ggplot(aes(x = game, y = score)) +\n  geom_point(color = \"#8D2AB5\") +\n  geom_line(color = \"#8D2AB5\", linetype = \"dotted\") +\n  ylim(0, 30000) +\n  scale_y_continuous(labels=scales::label_dollar()) +\n  theme_bw() +\n  labs(title = \"Coryat Score Trend\", x = \"Time\", y = \"Score\")"
  },
  {
    "objectID": "posts/coryat_scores/coryat_scores.html#references",
    "href": "posts/coryat_scores/coryat_scores.html#references",
    "title": "Coryat Scores",
    "section": "References",
    "text": "References\n\nJ! Scorer\nJ! Archive\nJ!ometry"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#references",
    "href": "posts/crossword_times/crossword_times.html#references",
    "title": "Crossword Times",
    "section": "References",
    "text": "References\n\nHow to Solve The New York Times Crossword"
  },
  {
    "objectID": "posts/crossword_times/crossword_times.html#visualizing-solve-times",
    "href": "posts/crossword_times/crossword_times.html#visualizing-solve-times",
    "title": "Crossword Times",
    "section": "Visualizing Solve Times",
    "text": "Visualizing Solve Times\nThere are a lot of ways to visualize a grouped collection of distributions. A box plot boils down each group to a five number summary. At the other extreme, one can simply plot a cloud of points. In between are binning methods like histograms and dot plots. We‚Äôll build a box plot, a dot plot, and a chart that overlays both.\nlubridate provides convenient date and time parsing functions like ms. weekdays(as.Date('1970-01-01') + 4:10) is a nice trick to produce a character vector with the names of the week, beginning with Monday.12\nIn the combined plot, we need to suppress the outliers in the box plot component in order to avoid double-plotting them.\n\nBox PlotDot PlotBoth\n\n\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |>\n  ggplot(aes(x = factor(weekdays(Date), \n                        weekdays(as.Date('1970-01-01') + 4:10)),\n             y = period_to_seconds(ms(Time))/60, \n             fill = Publisher)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  facet_wrap(~Publisher) +\n  guides(fill = FALSE) + \n  labs(title = \"Crossword Completion Times\", \n           x = \"Day of the Week\",\n           y = \"Completion Time (Minutes)\")\n\n\n\n\n\n\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |>\n  ggplot(aes(x = factor(weekdays(Date), \n                        weekdays(as.Date('1970-01-01') + 4:10)),\n             y = period_to_seconds(ms(Time))/60, \n             fill = Publisher)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.6) +\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  facet_wrap(~Publisher) +\n  guides(fill = FALSE) + \n  labs(title = \"Crossword Completion Times\", \n           x = \"Day of the Week\",\n           y = \"Completion Time (Minutes)\")\n\n\n\n\n\n\n\nread_csv(\"crossword_times.csv\", col_types = \"ccD\") |>\n  ggplot(aes(x = factor(weekdays(Date), \n                        weekdays(as.Date('1970-01-01') + 4:10)),\n             y = period_to_seconds(ms(Time))/60, \n             fill = Publisher)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.5) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.6) +\n  scale_x_discrete(labels = c(\"M\", \"T\", \"W\", \"T\", \"F\", \"S\", \"S\")) +\n  facet_wrap(~Publisher) +\n  guides(fill = FALSE) + \n  labs(title = \"Crossword Completion Times\", \n           x = \"Day of the Week\",\n           y = \"Completion Time (Minutes)\")"
  }
]