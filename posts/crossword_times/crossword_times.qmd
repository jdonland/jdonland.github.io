---
title: "Crosswords, Violin Plots, and Dubious Distinctions in Data Analysis"
date: "2023-01-02"
date-modified: now
date-format: long
image: violin_plot_thumbnail.png
description: "So-called \"descriptive\" analysis."
---

<!-- expand references -->

```{r}
#| label: libraries
#| include: false

library(here)
library(tidyverse)
library(sysfonts)
library(showtext)
library(gt)
library(viridis)
library(brms)
library(tidybayes)
library(modelr)

library(ggdist) #stat_eye

sysfonts::font_add_google(name = "Arimo", family = "arimo")
showtext::showtext_auto()
```

```{r}
#| label: theme
#| include: false

source(here("theme_swd.R"))
```

## Introduction

Data analysis comprises a broad range of practices undertaken to extract meaning, discover insights, or inform decisions on the basis of data. Within this set of practices, it's common to draw a distinction between *descriptive* and *inferential* analysis.

In this project we'll analyze how long it takes to solve a crossword puzzle, learn why descriptive and inferential data analysis are actually inextricable from one another, and resolve a raging controversy about violin plots.

## Data

The crosswords published in major American newspapers differ in difficulty according to the day of the week. For example, the [New York Times' guide](https://www.nytimes.com/guides/crosswords/how-to-solve-a-crossword-puzzle) explains that their Monday puzzles are the easiest and their Saturday puzzles the hardest. Meanwhile, the Sunday puzzles have easy clues but a large grid, and the Thursday puzzles have some unique gimmick. By contrast, the New Yorker's Monday puzzle is their most challenging, with the difficulty mostly decreasing over the course of the week.

I track how long it takes me to solve the New York Times, Los Angeles Times, New Yorker, and Wall Street Journal crosswords in a CSV file.

```{r}
#| label: data-preparation
#| message: false
#| code-fold: false

c("New York Times", 
  "New Yorker", 
  "Wall Street Journal", 
  "Los Angeles Times") ->
  publishers

read_csv("crossword_times.csv", col_types = "ccD") |>
  filter(publisher %in% publishers) |>
  mutate(publisher,
         day = factor(wday(date, week_start = 1, label = TRUE, abbr = FALSE)),
         minutes = period_to_seconds(ms(time))/60,
         .keep = "none") ->
  crossword_times
```

## Descriptive Analysis Doesn't Exist

At this point we're ready to do "descriptive" data analysis. We can count observations, calculate so called "five-number" summaries, and so on.

```{r}
#| label: summary-statistics
#| fig-width: 10
#| fig-height: 5

crossword_times |>
  group_by(publisher, day) |>
  reframe(
    Count = n(),
    Mean = mean(minutes),
    value = fivenum(minutes),
    name = c("Minimum", 
             "1ˢᵗ Quartile", 
             "Median",
             "3ʳᵈ Quartile",
             "Maximum")) |>
  pivot_wider() |>
  mutate(across(where(is.numeric), \(x) round(x, 1))) |>
  gt(rowname_col = "day", groupname_col = "publisher") |>
  tab_header(title = "Crossword Solve Times: Descriptive Statistics") |>
  opt_align_table_header(align = "center") |>
  tab_options(table_body.hlines.color = "transparent") |>
  tab_style(
    style = list(cell_text(color = "#555655", align = "center")), 
    locations = list(cells_column_labels(), cells_stub(), cells_stubhead())) |>
  tab_source_note(source_note = "Source: Self-collected data") |>
  opt_vertical_padding(scale = 0.25)
```

It's difficult to discern any story from a table like this. Let's make a plot:

```{r}
#| label: box-plot
#| warning: false
#| message: false
#| fig-width: 10
#| fig-height: 8

crossword_times |>
  ggplot() +
  # plot structure
  facet_grid(cols = vars(publisher)) +
  # aesthetic mapping for the violins
  aes(y = minutes, x = day, fill = publisher) +
  # visual elements representing the observed data
  geom_boxplot(alpha = 0.5) +
  # scales
  scale_x_discrete(labels = c("M", "T", "W", "T", "F", "S", "S")) +
  scale_y_continuous(breaks = seq(0, 70, 5), 
                     limits = c(0, 70), 
                     expand = c(0, 0)) +
  scale_fill_viridis(discrete = TRUE) +
  # labels
  labs(title = "Crossword Solve Times",
       subtitle = "Box and whisker plot",
       x = "Day of the Week",
       y = "Solve Time (Minutes)") +
  guides(fill = "none") +
  # theming
  theme_swd()
```

Now the relative difficulty of each group of puzzles stands out. So what's the problem?

For one thing, notice that `ggplot2`'s `geom_boxplot()` function has identified some of the data points as outliers and plotted them separately. Is that appropriate for this data set? As the Wikipedia article on outliers points out:

> There is no rigid mathematical definition of what constitutes an outlier; determining whether or not an observation is an outlier is ultimately a subjective exercise.

`geom_boxplot()` treats anything more than 1.5 times the interquartile range from the box as an outlier, but whether an observation should be considered an outlier obviously depends on the sampling distribution. For example, an observation of x = 2 is a fairly remarkable if X ~ Normal(0, 1) but not if X ~ Cauchy(0, 1), since the Cauchy distribution has fatter tails. And by using the same rule at both extremes of the sample, we implicitly assume the sampling distribution is symmetric.^[Notice that nearly all the outliers in our box plot are at the high end.] ^[See [this article](https://nightingaledvs.com/ive-stopped-using-box-plots-should-you/) for more on issues with box plots in general.]

Grouping the data by publisher and day of the week suggests that we believe the observations within each group are independent and identically distributed, even if we don't explicitly say so. When we report a mean, we suggest that the sampling distribution has a finite mean.

Perhaps the idea that there's a real distinction between descriptive and inferential data analysis ultimately stems from the "everything is normally distributed" fallacy.

## Histograms, Density Plots, and Violins

It's a matter of some debate among data visualization practitioners about when to use density plots, such as violin plots, instead of histograms. Or indeed whether density plots should ever be used. Or even whether _histograms_ should ever be used!

Scientist and YouTuber Angela Collier doesn't like violin plots:

<p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/_0QMKFzW9fw?si=Zbd0fmecfDTwNFFC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>

One of the points she makes is that drawing density plots usually involves what amounts to fitting an unjustified model.

In most situations, `ggplot` uses locally estimated scatterplot smoothing (LOESS) by default, which involves fitting a separate polynomial regression model on a weighted neighbourhood around each data point and evaluating it there. This often results in serviceable violin plots, but you wouldn't necessarily expect it to reflect the "actual" theoretical distribution of the data.

Designing charts is another situation in which we cannot avoid making modelling assumptions just because we insist we're only doing "descriptive" analysis. As soon as you click "add trendline" in Excel, you're fitting a model. Accepting this and being willing to specify a model allows us to create better data visualizations.

Let's apply a simple Bayesian model of our crossword data and use it to design a violin plot in which the densities we present are actually justified.

## Modelling

It's plausible to think that crossword solve times would be distributed log-normally. As the name suggests, a log-normally distributed variable has a normally distributed logarithm. The relevant Wikipedia [article](https://en.wikipedia.org/wiki/Log-normal_distribution) lists a variety of phenomena which seem to follow a log-normal distribution, including the length of chess games, the incubation period of diseases, and incomes (except those of the very rich).

### Fitting a Bayesian Log-Normal Regression

If we imagine that the distribution of solves times within each combination of publisher and day of the week is log-normal where the underlying normal distribution has some particular mean and standard deviation, we can provide that model specification to `brms`, accept some default priors, and have it estimate posterior distributions of those means and standard deviations based on the observed solve times.

```{r}
#| label: model
#| eval: false
#| output: false
#| code-fold: false

crossword_times |>
  brm(formula = minutes ~ (1 + day | publisher),
      data = _,
      family = lognormal(),
      warmup = 1000,
      iter = 2000,
      chains = 4,
      control = list(adapt_delta = 0.95)) ->
  solve_time_model
```

```{r}
#| include: false

if (exists("solve_time_model")) {
  save(solve_time_model, file = "solve_time_model.RData")
}

if (file.exists("solve_time_model.RData")) {load("solve_time_model.RData")}
```

## Visualization

Since we were willing to actually specify a model, we can now make a density plot that shows something meaningful: the [posterior predictive distributions](http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-668/predictive.pdf) corresponding to our model. We'll use `add_predicted_draws()` from `tidybayes` to generate draws from those models and use these to draw violins that reflect the data we expect to observe in the future based on those we have observed so far. (We also enlist the help of `data_grid()` from `modelr`.)

```{r}
#| label: violin-plot
#| warning: false
#| message: false
#| code-fold: true
#| fig-width: 10
#| fig-height: 8

crossword_times |>
  data_grid(publisher, day) |>
  add_predicted_draws(solve_time_model) |>
  # filter out predictions for groups that do not actually occur
  filter(!(publisher == "New Yorker" & as.integer(day) %in% c(6, 7)),
         !(publisher == "Wall Street Journal" & as.integer(day) == 7)) |>
  ggplot() +
  # plot structure
  facet_grid(cols = vars(publisher)) +
  # aesthetic mapping for the violins
  aes(y = .prediction, x = day, fill = publisher) +
  # visual elements representing the posterior predictive model
  stat_eye(color = NA, alpha = 0.5) +
  scale_x_discrete(labels = c("M", "T", "W", "T", "F", "S", "S")) +
  scale_y_continuous(breaks = seq(0, 70, 5), 
                     limits = c(0, 70), 
                     expand = c(0, 0)) +
  scale_fill_viridis(discrete = TRUE) +
  # labels
  labs(title = "Crossword Solve Times",
       subtitle = "Posterior predictive log-normal distributions",
       x = "Day of the Week",
       y = "Solve Time (Minutes)") +
  guides(fill = "none") +
  # theming
  theme_swd()
```

### geom_violin vs Log-Normal Densities

What's the difference between plotting LOESS-based densities and posterior predictive distributions? I think this image illustrates the shortcomings of the former approach well:

![geom_violin (black) vs our posterior predictive distributions (grey).](loess_vs_model.png){fig-align="center" width="500"}

## References

-   [How to Solve the New York Times Crossword](https://www.nytimes.com/guides/crosswords/how-to-solve-a-crossword-puzzle)
-   [Anna Shechtman, the New Queen of Crosswords](https://www.theguardian.com/global/2019/may/12/anna-shechtman-the-new-queen-of-crosswords)
-   [Violin Plots Should Not Exist](https://www.youtube.com/watch?v=_0QMKFzW9fw)
-   [I’ve Stopped Using Box Plots. Should You?](https://nightingaledvs.com/ive-stopped-using-box-plots-should-you/)