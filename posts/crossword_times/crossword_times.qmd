---
title: "Crossword Times"
subtitle: "With a word on density plots"
date: "2023-01-02"
date-modified: now
date-format: long
description: "A consideration of cruciverbal complexity."
filters:
   - lightbox
lightbox: auto
---

```{r}
#| label: libraries
#| include: false

library(tidyverse)
library(gt)
library(vtable)
library(brms)
library(modelr)
library(tidybayes)
library(ggdist)
library(viridis)
```

## Introduction

[Down For Across](https://downforacross.com/) is a website which allows you to solve user-uploaded crosswords solo or with friends.

The crosswords published in major American newspapers often differ in difficulty according to the day of the week. For example, the [New York Times' guide](https://www.nytimes.com/guides/crosswords/how-to-solve-a-crossword-puzzle) explains that their Monday puzzles are the easiest and their Saturday puzzles the hardest. Meanwhile, the Sunday puzzles have easy clues but a large grid, and the Thursday puzzles have some unique gimmick. By contrast, the New Yorker's crosswords decrease in difficulty over the week.

I've started keeping track of my solo completion times for the New York Times, Los Angeles Times, New Yorker, and Wall Street Journal crosswords.

## Reading the Data

I'm accustomed to thinking of the week as beginning with Sunday, but for the purposes of crossword difficulty analysis, Monday is better. We'll convert the solve times from `00:00`-style to minutes.

```{r}
#| label: data-preparation
#| message: false

read_csv("crossword_times.csv", col_types = "ccD") |>
  transmute(Publisher,
            day = factor(wday(Date, week_start = 1)),
            minutes = period_to_seconds(ms(Time))/60) |>
  rename(publisher = Publisher) ->
  crossword_times
```

::: panel-tabset
### Table
```{r}
#| label: table
#| message: false
#| code-fold: true

crossword_times |>
  mutate(minutes = round(minutes)) |>
  kable(caption = "My Crossword Solve Times") |>
  scroll_box(height = "5in")
```

### Summary
```{r}
#| label: summary
#| message: false
#| code-fold: true

crossword_times |>
  group_by(publisher, day) |>
  summarize(n = n(), mean = mean(minutes), sd = sd(minutes)) |>
  mutate(across(c("mean", "sd"), \(x) round(x))) |>
  pivot_wider(names_from = publisher, 
              values_from = c("n", "mean", "sd"),
              names_sort = TRUE) |>
  relocate(c(1, 2, 6, 10, 3, 7, 11, 4, 8, 12, 5, 9, 13)) |>
  gt() |>
  tab_header(title = "My Crossword Solve Times: Summary") |>
  tab_spanner(label = "Los Angeles Times", columns = 2:4) |>
  tab_spanner(label = "New York Times", columns = 5:7) |>
  tab_spanner(label = "New Yorker", columns = 8:10) |>
  tab_spanner(label = "Wall Street Journal", columns = 11:13) |>
  cols_label(starts_with("n") ~ "n", 
             starts_with("mean") ~ "mean", 
             starts_with("sd") ~ "sd")
```
:::

## Modelling Solve Times

It's plausible to think that crossword solve times would be distributed log-normally. As the name suggests, a log-normally distributed variable has a normally distributed logarithm. The relevant Wikipedia [article](https://en.wikipedia.org/wiki/Log-normal_distribution) lists a variety of phenomena which seem to follow a log-normal distribution, including the length of chess games, the incubation period of diseases, and incomes (except those of the very rich).

### Fitting a Bayesian Log-Normal Regression

If we imagine that the distribution of solves times within each combination of publisher and day of the week is log-normal where the underlying normal distribution has some particular mean and standard deviation, we can provide that model specification to `brms`, accept some default priors, and have it estimate posterior distributions of those means and standard deviations based on the observed solve times.

```{r}
#| label: model
#| eval: false
#| output: false

crossword_times |>
  brm(formula = minutes ~ (1 + day | publisher),
      data = _,
      family = lognormal(),
      warmup = 1000,
      iter = 2000,
      chains = 4,
      control = list(adapt_delta = 0.95)) ->
  solve_time_model
```

```{r}
#| include: false
if (exists("solve_time_model")) {
  save(solve_time_model, file = "solve_time_model.RData")
}

if (file.exists("solve_time_model.RData")) {load("solve_time_model.RData")}
```

## Visualizing Solve Times

It's a matter of some debate among data visualization practitioners about when to use density plots, such as violin plots, instead of histograms. Or indeed whether density plots should ever be used. Or even whether histograms should ever be used!

I disagree with several of the points Angela Collier makes in her video ["violin plots should not exist"](https://www.youtube.com/watch?v=_0QMKFzW9fw), but one that I find compelling is that drawing density plots requires making modelling assumptions which may not accurately reflect the actual process from which the data result.

In most situations, `ggplot` uses locally estimated scatterplot smoothing (LOESS) by default, which involves evaluating a fitting a separate polynomial regression model on a weighted neighbourhood around each data point and evaluating it there. It makes nice looking violin plots, but you wouldn't expect it to reflect that "actual" theoretical distribution of the data. In particular you run into edge cases like a non-zero violin width outside the range in which the data can actually fall. LOESS doesn't know that it can't take -1.5 minutes for the first Northern cardinal to appear at your bird feeder after you fill it.

It seems to me that this sort of thing is a symptom of a general desire to avoid having to actually specify models by pretending that there's some bright-line distinction between descriptive statistics and statistical inference.

Since we were willing to actually specify a model, we can make density plots that show something meaningful: the [posterior predictive distributions](http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-668/predictive.pdf) corresponding to our model. We'll use `add_predicted_draws()` from `tidybayes` to generate draws from those models and use these to draw violins that reflect the data we expect to observe in the future based on those we have observed so far. (We also enlist the help of `data_grid` from `modelr`.)

### Violin Plot

```{r}
#| label: violin-plots
#| warning: false
#| message: false
#| code-fold: true

crossword_times |>
  data_grid(publisher, day) |>
  add_predicted_draws(solve_time_model) |>
  # filter out predictions for groups that do not actually occur
  filter(!(publisher == "New Yorker" & day %in% c(6, 7)),
         !(publisher == "Wall Street Journal" & day == 7)) |>
  ggplot() +
  # plot structure
  facet_grid(cols = vars(publisher)) +
  # aesthetic mapping for the violins
  aes(y = .prediction, x = day) +
  # visual elements representing the posterior predictive model
  stat_eye(colour = NA, fill = "grey") +
  # visual elements representing the observed data
  geom_jitter(width = 0.25, 
              data = crossword_times, 
              aes(y = minutes, fill = publisher),
              alpha = 0.5,
              pch = 21) +
  # scales
  scale_x_discrete(labels = c("M", "T", "W", "T", "F", "S", "S")) +
  scale_y_continuous(breaks = seq(0, 70, 5), 
                     limits = c(0, 70), 
                     expand = c(0, 0)) +
  scale_fill_viridis(discrete = TRUE) +
  # labels
  labs(title = "Crossword Solve Times",
       subtitle = "With posterior predictive log-normal distributions",
       x = "Day of the Week",
       y = "Solve Time (Minutes)") +
  guides(fill = "none") +
  # theming
  theme_bw() +
  theme(panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(),
        strip.text = element_text(hjust = 0.5))
```

### Violins vs Log-Normals

What's the difference between plotting LOESS-based densities and posterior predictive distributions? I think this image illustrates the shortcomings of the former approach well:

![geom_violin (black) vs our posterior predictive distributions (grey).](loess_vs_model.png){fig-align="center" width="500"}

```{r}
#| label: cumulative-distributions
#| warning: false
#| message: false
#| code-fold: true
#| eval: false
#| include: false

c(`1`="M", `2`="T", `3`="W", `4`="T", `5`="F", `6`="S", `7`="S") |>
  as_labeller() ->
  day_labeller

crossword_times |>
  # group by publisher and day, sort by solve time, transform to percentiles
  group_by(publisher, day) |>
  arrange(minutes) |>
  summarize(percentile = cumsum(minutes)/sum(minutes), minutes = minutes) |>
  ggplot() +
  # plot structure
  facet_grid(rows = vars(day), labeller = day_labeller) +
  # aesthetic mapping
  aes(x = minutes, y = percentile, colour = publisher) +
  # visual elements representing the data
  geom_line(size = 2, alpha = 0.75) +
  # scales
  scale_y_continuous(labels = scales::label_percent(),
                     limits = c(0, NA),
                     expand = c(0 ,0),
                     sec.axis = sec_axis(~ ., 
                                         labels = NULL, 
                                         breaks = NULL, 
                                         name = "Day of the Week\n")) +
  scale_x_continuous(n.breaks = 12, minor_breaks = 0:65) +
  scale_colour_viridis(discrete = TRUE) +
  # labels
  labs(title = "Crossword Solve Times", 
       subtitle = "Cumulative distribution by publisher and day of the week",
       x = "Time (Minutes)", 
       y = "",
       colour = NULL) +
  # theming
  theme_bw() +
  theme(legend.position = "top", 
        panel.spacing.y = unit(0.02, "npc"),
        axis.text.y = element_text(size = rel(0.8)),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_line(linetype = 3),
        legend.margin = margin(0, 0, 0, 0),
        strip.text = element_text(hjust = 0.5))
```

## References

-   [How to Solve The New York Times Crossword](https://www.nytimes.com/guides/crosswords/how-to-solve-a-crossword-puzzle)
-   [Anna Shechtman, the new queen of crosswords](https://www.theguardian.com/global/2019/may/12/anna-shechtman-the-new-queen-of-crosswords)
